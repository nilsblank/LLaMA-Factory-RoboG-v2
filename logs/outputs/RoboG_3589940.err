GpuFreq=control_disabled
[W1022 01:20:40.867163024 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 01:20:40.867214663 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 01:20:40.867267052 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 01:20:40.867407804 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:43,946 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-22 01:20:44,166 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-22 01:20:44,913 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-22 01:20:45,156 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-22 01:20:45,160 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-22 01:20:45,401 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-22 01:20:45,619 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-22 01:20:45,976 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-22 01:20:45,980 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-10-22 01:20:46,839 >> loading configuration file processor_config.json from cache at None
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 01:20:47.807867472 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W1022 01:20:47.807928105 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 01:20:47.827444403 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-10-22 01:20:47,241 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

Converting format of dataset (num_proc=128):   0%|          | 0/95244 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 7/95244 [00:00<36:34, 43.40 examples/s]Converting format of dataset (num_proc=128):   0%|          | 355/95244 [00:00<00:57, 1654.52 examples/s]Converting format of dataset (num_proc=128):   1%|          | 732/95244 [00:00<00:37, 2495.23 examples/s]Converting format of dataset (num_proc=128):   1%|          | 1016/95244 [00:00<00:37, 2534.51 examples/s]Converting format of dataset (num_proc=128):   1%|▏         | 1291/95244 [00:00<00:44, 2109.45 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1701/95244 [00:00<00:35, 2656.07 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 2067/95244 [00:00<00:31, 2940.37 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 2533/95244 [00:00<00:27, 3433.07 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 3132/95244 [00:01<00:22, 4174.74 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 3756/95244 [00:01<00:19, 4777.51 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 4332/95244 [00:01<00:17, 5062.02 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 4855/95244 [00:01<00:17, 5105.82 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 5411/95244 [00:01<00:17, 5235.73 examples/s]Converting format of dataset (num_proc=128):   6%|▋         | 6025/95244 [00:01<00:16, 5504.99 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 6582/95244 [00:01<00:17, 4996.40 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 7094/95244 [00:01<00:18, 4805.09 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 7664/95244 [00:01<00:17, 5045.84 examples/s]Converting format of dataset (num_proc=128):   9%|▊         | 8180/95244 [00:01<00:17, 5065.68 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 8694/95244 [00:02<00:17, 5086.03 examples/s]Converting format of dataset (num_proc=128):  10%|▉         | 9207/95244 [00:02<00:17, 4904.37 examples/s]Converting format of dataset (num_proc=128):  10%|█         | 9702/95244 [00:02<00:18, 4529.98 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 10192/95244 [00:02<00:18, 4610.64 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 10659/95244 [00:02<00:19, 4372.35 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 11102/95244 [00:02<00:19, 4291.18 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 11535/95244 [00:02<00:20, 4183.61 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 11959/95244 [00:02<00:20, 4112.73 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 12373/95244 [00:02<00:20, 4052.63 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 12827/95244 [00:03<00:19, 4183.83 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 13277/95244 [00:03<00:19, 4267.77 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 13709/95244 [00:03<00:19, 4203.91 examples/s]Converting format of dataset (num_proc=128):  15%|█▍        | 14131/95244 [00:03<00:19, 4106.43 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 14551/95244 [00:03<00:19, 4130.16 examples/s]Converting format of dataset (num_proc=128):  16%|█▌        | 14969/95244 [00:03<00:20, 3916.76 examples/s]Converting format of dataset (num_proc=128):  16%|█▋        | 15498/95244 [00:03<00:18, 4302.70 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 15936/95244 [00:03<00:18, 4207.36 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 16521/95244 [00:03<00:16, 4672.71 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 17001/95244 [00:04<00:16, 4674.08 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 17472/95244 [00:04<00:17, 4530.04 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 17928/95244 [00:04<00:17, 4345.43 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 18373/95244 [00:04<00:17, 4364.22 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 18814/95244 [00:04<00:17, 4279.44 examples/s]Converting format of dataset (num_proc=128):  20%|██        | 19244/95244 [00:04<00:17, 4258.41 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 19676/95244 [00:04<00:17, 4244.35 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 20139/95244 [00:04<00:17, 4330.68 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 20577/95244 [00:04<00:17, 4344.55 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 21012/95244 [00:04<00:17, 4319.46 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 21503/95244 [00:05<00:16, 4471.95 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 21952/95244 [00:05<00:16, 4428.48 examples/s]Converting format of dataset (num_proc=128):  24%|██▎       | 22396/95244 [00:05<00:17, 4277.44 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 22825/95244 [00:05<00:16, 4263.07 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 23329/95244 [00:05<00:16, 4480.15 examples/s]Converting format of dataset (num_proc=128):  25%|██▌       | 23862/95244 [00:05<00:15, 4727.84 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 24455/95244 [00:05<00:14, 5048.08 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 24996/95244 [00:05<00:13, 5145.91 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 25513/95244 [00:05<00:13, 5002.69 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 26017/95244 [00:06<00:14, 4857.47 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 26587/95244 [00:06<00:13, 5096.76 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 27100/95244 [00:06<00:13, 5076.73 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 27625/95244 [00:06<00:14, 4780.36 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 28111/95244 [00:06<00:13, 4800.14 examples/s]Converting format of dataset (num_proc=128):  30%|███       | 28606/95244 [00:06<00:13, 4842.81 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 29094/95244 [00:06<00:14, 4640.36 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 29564/95244 [00:06<00:15, 4364.99 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 30007/95244 [00:06<00:16, 4029.40 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 30591/95244 [00:07<00:14, 4484.02 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 31121/95244 [00:07<00:13, 4690.69 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 31685/95244 [00:07<00:13, 4850.44 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 32261/95244 [00:07<00:12, 5099.52 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 32778/95244 [00:07<00:12, 5001.62 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 33283/95244 [00:07<00:12, 4913.11 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 33779/95244 [00:07<00:12, 4854.79 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 34522/95244 [00:07<00:10, 5589.57 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 35087/95244 [00:07<00:11, 5257.27 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 35622/95244 [00:08<00:12, 4966.61 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 36127/95244 [00:08<00:12, 4771.53 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 36610/95244 [00:08<00:13, 4461.51 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 37091/95244 [00:08<00:12, 4552.20 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 37583/95244 [00:08<00:12, 4648.76 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 38062/95244 [00:08<00:12, 4684.00 examples/s]Converting format of dataset (num_proc=128):  40%|████      | 38534/95244 [00:08<00:12, 4398.70 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 38987/95244 [00:08<00:12, 4433.60 examples/s]Converting format of dataset (num_proc=128):  41%|████▏     | 39488/95244 [00:08<00:12, 4596.79 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 39967/95244 [00:08<00:12, 4432.27 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 40586/95244 [00:09<00:11, 4918.47 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 41103/95244 [00:09<00:11, 4894.11 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 41788/95244 [00:09<00:09, 5437.20 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 42337/95244 [00:09<00:10, 5171.02 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 42860/95244 [00:09<00:11, 4750.03 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 43345/95244 [00:09<00:11, 4563.30 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 43926/95244 [00:09<00:10, 4860.64 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 44421/95244 [00:09<00:10, 4781.72 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 44908/95244 [00:10<00:11, 4329.39 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 45507/95244 [00:10<00:10, 4766.56 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 46030/95244 [00:10<00:10, 4837.31 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 46560/95244 [00:10<00:09, 4964.51 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 47099/95244 [00:10<00:09, 5081.99 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 47614/95244 [00:10<00:09, 4814.50 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 48122/95244 [00:10<00:09, 4885.19 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 48707/95244 [00:10<00:09, 5129.97 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 49225/95244 [00:10<00:09, 5091.32 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 49874/95244 [00:10<00:08, 5488.67 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 50427/95244 [00:11<00:08, 5492.83 examples/s]Converting format of dataset (num_proc=128):  54%|█████▎    | 51019/95244 [00:11<00:07, 5616.26 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 51583/95244 [00:11<00:07, 5477.90 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 52140/95244 [00:11<00:07, 5501.45 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 52886/95244 [00:11<00:06, 6071.03 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 53507/95244 [00:11<00:06, 6088.98 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 54118/95244 [00:11<00:07, 5714.82 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 54700/95244 [00:11<00:07, 5274.84 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 55238/95244 [00:11<00:07, 5228.92 examples/s]Converting format of dataset (num_proc=128):  59%|█████▊    | 55769/95244 [00:12<00:08, 4726.93 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 56598/95244 [00:12<00:06, 5641.51 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 57293/95244 [00:12<00:06, 5987.25 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 57937/95244 [00:12<00:06, 6111.82 examples/s]Converting format of dataset (num_proc=128):  61%|██████▏   | 58562/95244 [00:12<00:06, 5453.43 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 59131/95244 [00:12<00:06, 5273.53 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 59675/95244 [00:12<00:06, 5232.99 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 60209/95244 [00:12<00:06, 5019.13 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 60721/95244 [00:12<00:07, 4913.10 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 61337/95244 [00:13<00:06, 5246.91 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 61872/95244 [00:13<00:06, 5057.33 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 62409/95244 [00:13<00:06, 5127.16 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 62933/95244 [00:13<00:06, 4979.18 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 63467/95244 [00:13<00:06, 5058.32 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 63976/95244 [00:13<00:06, 4871.12 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 64467/95244 [00:13<00:06, 4841.46 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 64953/95244 [00:13<00:06, 4687.52 examples/s]Converting format of dataset (num_proc=128):  69%|██████▊   | 65432/95244 [00:13<00:06, 4715.23 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 65934/95244 [00:14<00:06, 4759.14 examples/s]Converting format of dataset (num_proc=128):  70%|██████▉   | 66414/95244 [00:14<00:06, 4713.33 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 66924/95244 [00:14<00:05, 4816.76 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 67500/95244 [00:14<00:05, 5065.43 examples/s]Converting format of dataset (num_proc=128):  71%|███████▏  | 68014/95244 [00:14<00:05, 4967.09 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 68739/95244 [00:14<00:04, 5590.58 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 69300/95244 [00:14<00:05, 5188.07 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 69827/95244 [00:14<00:05, 4976.71 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 70331/95244 [00:14<00:05, 4886.03 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 70824/95244 [00:15<00:05, 4477.44 examples/s]Converting format of dataset (num_proc=128):  75%|███████▍  | 71372/95244 [00:15<00:05, 4736.03 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 71888/95244 [00:15<00:04, 4850.43 examples/s]Converting format of dataset (num_proc=128):  76%|███████▋  | 72689/95244 [00:15<00:03, 5741.81 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 73326/95244 [00:15<00:03, 5921.23 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 73927/95244 [00:15<00:03, 5940.49 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 74528/95244 [00:15<00:03, 5414.73 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 75083/95244 [00:15<00:04, 4989.87 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 75596/95244 [00:15<00:04, 4799.17 examples/s]Converting format of dataset (num_proc=128):  80%|███████▉  | 76089/95244 [00:16<00:04, 4671.22 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 76565/95244 [00:16<00:04, 4658.88 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 77042/95244 [00:16<00:04, 4401.62 examples/s]Converting format of dataset (num_proc=128):  81%|████████▏ | 77556/95244 [00:16<00:03, 4594.13 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 78021/95244 [00:16<00:03, 4548.48 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 78481/95244 [00:16<00:03, 4368.96 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 78970/95244 [00:16<00:03, 4510.57 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 79426/95244 [00:16<00:03, 4372.91 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 79956/95244 [00:16<00:03, 4628.84 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 80510/95244 [00:16<00:03, 4889.46 examples/s]Converting format of dataset (num_proc=128):  85%|████████▌ | 81090/95244 [00:17<00:02, 5117.91 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 81606/95244 [00:17<00:02, 4957.61 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 82108/95244 [00:17<00:02, 4684.95 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 82582/95244 [00:17<00:02, 4388.84 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 83031/95244 [00:17<00:02, 4287.45 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 83487/95244 [00:17<00:02, 4344.16 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 84072/95244 [00:17<00:02, 4758.54 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 84555/95244 [00:17<00:02, 4448.09 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 85011/95244 [00:18<00:02, 4340.15 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 85523/95244 [00:18<00:02, 4542.02 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 86019/95244 [00:18<00:01, 4633.74 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 86493/95244 [00:18<00:01, 4445.96 examples/s]Converting format of dataset (num_proc=128):  91%|█████████▏| 86944/95244 [00:18<00:01, 4255.08 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 87382/95244 [00:18<00:01, 4155.81 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 87805/95244 [00:18<00:01, 4174.66 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 88230/95244 [00:18<00:01, 4170.15 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 88801/95244 [00:18<00:01, 4559.23 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 89323/95244 [00:18<00:01, 4690.99 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 89824/95244 [00:19<00:01, 4766.31 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▍| 90304/95244 [00:19<00:01, 4370.50 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 90748/95244 [00:19<00:01, 4203.10 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 91179/95244 [00:19<00:01, 3881.19 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 91575/95244 [00:19<00:00, 3694.99 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 91951/95244 [00:19<00:00, 3327.67 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 92294/95244 [00:19<00:00, 3044.34 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 92612/95244 [00:19<00:00, 2897.24 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 92911/95244 [00:20<00:00, 2655.57 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 93183/95244 [00:20<00:00, 2625.31 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 93449/95244 [00:20<00:00, 2560.47 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 93714/95244 [00:20<00:00, 2515.26 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▊| 93969/95244 [00:20<00:00, 2504.05 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 94228/95244 [00:20<00:00, 2517.06 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 94534/95244 [00:20<00:00, 2596.62 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 94796/95244 [00:20<00:00, 2460.99 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 95054/95244 [00:21<00:00, 2137.91 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 95244/95244 [00:21<00:00, 4451.85 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):   1%|          | 1/83 [00:00<00:09,  9.09 examples/s]Converting format of dataset (num_proc=83):  35%|███▍      | 29/83 [00:00<00:00, 156.45 examples/s]Converting format of dataset (num_proc=83):  58%|█████▊    | 48/83 [00:00<00:00, 170.82 examples/s]Converting format of dataset (num_proc=83):  86%|████████▌ | 71/83 [00:00<00:00, 192.73 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 147.66 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 01:21:13.590305252 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/95244 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 744/95244 [01:31<3:14:29,  8.10 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1488/95244 [01:33<1:21:24, 19.19 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 2232/95244 [01:33<44:10, 35.09 examples/s]  Running tokenizer on dataset (num_proc=128):   3%|▎         | 2976/95244 [01:34<27:22, 56.19 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 3720/95244 [01:35<17:57, 84.97 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 4464/95244 [01:36<12:39, 119.57 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 5208/95244 [01:37<08:37, 173.82 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 6696/95244 [01:37<04:38, 318.33 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 7440/95244 [01:38<03:42, 394.87 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 8184/95244 [01:38<02:50, 512.10 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 8928/95244 [01:39<02:25, 594.98 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 9672/95244 [01:39<01:53, 755.32 examples/s]Running tokenizer on dataset (num_proc=128):  11%|█         | 10416/95244 [01:40<01:41, 836.58 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 11160/95244 [01:41<02:06, 664.20 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 12648/95244 [01:42<01:14, 1112.86 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 13392/95244 [01:42<01:17, 1062.97 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 14136/95244 [01:43<00:59, 1359.90 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 14880/95244 [01:43<00:57, 1401.17 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 16368/95244 [01:44<00:42, 1873.39 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 17112/95244 [01:45<01:08, 1143.05 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▊        | 17856/95244 [01:45<01:01, 1260.91 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 19344/95244 [01:46<00:39, 1898.39 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 20088/95244 [01:47<01:10, 1070.80 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 20832/95244 [01:48<01:17, 954.24 examples/s] Running tokenizer on dataset (num_proc=128):  23%|██▎       | 21576/95244 [01:49<01:01, 1195.02 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 22320/95244 [01:50<01:10, 1036.74 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▍       | 23808/95244 [01:50<00:42, 1696.48 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 24553/95244 [01:50<00:35, 2016.55 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 25297/95244 [01:50<00:36, 1913.65 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 26041/95244 [01:51<00:43, 1593.57 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 26785/95244 [01:51<00:35, 1929.24 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 27529/95244 [01:51<00:32, 2084.97 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 28274/95244 [01:52<00:26, 2541.45 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 29019/95244 [01:52<00:22, 2929.99 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 30509/95244 [01:52<00:16, 3880.72 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 31253/95244 [01:52<00:18, 3391.70 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 31997/95244 [01:52<00:17, 3627.02 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 33487/95244 [01:53<00:12, 5047.61 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 35720/95244 [01:53<00:08, 7166.54 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 37209/95244 [01:53<00:13, 4159.38 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 37954/95244 [01:54<00:17, 3295.76 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 39443/95244 [01:54<00:12, 4330.48 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 40187/95244 [01:55<00:18, 2909.48 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 40931/95244 [01:55<00:16, 3320.74 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 42419/95244 [01:55<00:12, 4237.76 examples/s]Running tokenizer on dataset (num_proc=128):  47%|████▋     | 44652/95244 [01:55<00:12, 3963.71 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 45396/95244 [01:56<00:12, 3869.08 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 46884/95244 [01:56<00:09, 4873.82 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 49116/95244 [01:56<00:09, 4715.27 examples/s]Running tokenizer on dataset (num_proc=128):  53%|█████▎    | 50604/95244 [01:56<00:07, 5774.80 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▍    | 52092/95244 [01:57<00:06, 6909.97 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 53580/95244 [01:57<00:05, 7512.93 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 55068/95244 [01:58<00:11, 3500.29 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 56556/95244 [01:58<00:12, 3122.72 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 57300/95244 [01:59<00:16, 2370.05 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 58044/95244 [01:59<00:15, 2467.84 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 58788/95244 [01:59<00:13, 2607.07 examples/s]Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 61020/95244 [02:00<00:08, 3939.59 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 62508/95244 [02:02<00:18, 1797.98 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 63252/95244 [02:02<00:16, 1966.89 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 63996/95244 [02:02<00:16, 1920.22 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 64740/95244 [02:03<00:17, 1744.19 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 65484/95244 [02:03<00:16, 1758.34 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 66972/95244 [02:03<00:11, 2444.98 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 67716/95244 [02:04<00:11, 2330.71 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 69204/95244 [02:04<00:09, 2730.68 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 70692/95244 [02:04<00:06, 3838.91 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 71436/95244 [02:04<00:05, 4060.39 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 73668/95244 [02:05<00:03, 6110.99 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 75156/95244 [02:05<00:03, 5266.22 examples/s]Running tokenizer on dataset (num_proc=128):  80%|████████  | 76644/95244 [02:05<00:04, 4480.63 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 77388/95244 [02:06<00:03, 4712.14 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 78876/95244 [02:06<00:02, 5502.02 examples/s]Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 81108/95244 [02:06<00:02, 6336.75 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 82596/95244 [02:06<00:01, 6380.46 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 84084/95244 [02:06<00:01, 6328.05 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 85572/95244 [02:07<00:01, 7588.60 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 87060/95244 [02:07<00:01, 7517.42 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 88548/95244 [02:07<00:01, 5674.29 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 89292/95244 [02:07<00:01, 4443.51 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 90036/95244 [02:08<00:01, 3722.79 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▌| 90780/95244 [02:08<00:01, 4164.60 examples/s]Running tokenizer on dataset (num_proc=128):  97%|█████████▋| 92268/95244 [02:08<00:00, 5475.31 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 93756/95244 [02:08<00:00, 4969.30 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 94500/95244 [02:09<00:00, 4740.30 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 95244/95244 [02:09<00:00, 737.01 examples/s] 
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<00:44,  1.84 examples/s]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:16,  5.00 examples/s]Running tokenizer on dataset (num_proc=83):   6%|▌         | 5/83 [00:00<00:11,  6.85 examples/s]Running tokenizer on dataset (num_proc=83):  10%|▉         | 8/83 [00:01<00:07, 10.01 examples/s]Running tokenizer on dataset (num_proc=83):  12%|█▏        | 10/83 [00:01<00:07, 10.42 examples/s]Running tokenizer on dataset (num_proc=83):  14%|█▍        | 12/83 [00:01<00:06, 10.58 examples/s]Running tokenizer on dataset (num_proc=83):  17%|█▋        | 14/83 [00:01<00:06, 10.90 examples/s]Running tokenizer on dataset (num_proc=83):  19%|█▉        | 16/83 [00:01<00:06,  9.90 examples/s]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:02<00:05, 11.73 examples/s]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:02<00:05, 11.66 examples/s]Running tokenizer on dataset (num_proc=83):  28%|██▊       | 23/83 [00:02<00:05, 10.41 examples/s]Running tokenizer on dataset (num_proc=83):  30%|███       | 25/83 [00:02<00:05, 10.71 examples/s]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:02<00:04, 12.41 examples/s]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:03<00:04, 11.51 examples/s]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:03<00:04, 11.90 examples/s]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:03<00:04, 10.62 examples/s]Running tokenizer on dataset (num_proc=83):  43%|████▎     | 36/83 [00:03<00:04, 10.92 examples/s]Running tokenizer on dataset (num_proc=83):  46%|████▌     | 38/83 [00:03<00:04, 11.24 examples/s]Running tokenizer on dataset (num_proc=83):  48%|████▊     | 40/83 [00:03<00:03, 11.50 examples/s]Running tokenizer on dataset (num_proc=83):  51%|█████     | 42/83 [00:04<00:03, 11.63 examples/s]Running tokenizer on dataset (num_proc=83):  53%|█████▎    | 44/83 [00:04<00:03, 11.62 examples/s]Running tokenizer on dataset (num_proc=83):  55%|█████▌    | 46/83 [00:04<00:03, 11.45 examples/s]Running tokenizer on dataset (num_proc=83):  58%|█████▊    | 48/83 [00:04<00:02, 13.12 examples/s]Running tokenizer on dataset (num_proc=83):  60%|██████    | 50/83 [00:04<00:02, 12.17 examples/s]Running tokenizer on dataset (num_proc=83):  63%|██████▎   | 52/83 [00:04<00:02, 12.03 examples/s]Running tokenizer on dataset (num_proc=83):  65%|██████▌   | 54/83 [00:05<00:02, 11.96 examples/s]Running tokenizer on dataset (num_proc=83):  67%|██████▋   | 56/83 [00:05<00:02, 10.21 examples/s]Running tokenizer on dataset (num_proc=83):  70%|██████▉   | 58/83 [00:05<00:02, 10.94 examples/s]Running tokenizer on dataset (num_proc=83):  72%|███████▏  | 60/83 [00:05<00:02, 11.30 examples/s]Running tokenizer on dataset (num_proc=83):  75%|███████▍  | 62/83 [00:05<00:02, 10.13 examples/s]Running tokenizer on dataset (num_proc=83):  77%|███████▋  | 64/83 [00:06<00:01, 10.44 examples/s]Running tokenizer on dataset (num_proc=83):  81%|████████  | 67/83 [00:06<00:01, 12.70 examples/s]Running tokenizer on dataset (num_proc=83):  83%|████████▎ | 69/83 [00:06<00:01, 12.67 examples/s]Running tokenizer on dataset (num_proc=83):  86%|████████▌ | 71/83 [00:06<00:00, 12.43 examples/s]Running tokenizer on dataset (num_proc=83):  88%|████████▊ | 73/83 [00:06<00:00, 12.26 examples/s]Running tokenizer on dataset (num_proc=83):  90%|█████████ | 75/83 [00:06<00:00, 12.32 examples/s]Running tokenizer on dataset (num_proc=83):  93%|█████████▎| 77/83 [00:07<00:00, 12.21 examples/s]Running tokenizer on dataset (num_proc=83):  95%|█████████▌| 79/83 [00:07<00:00, 13.52 examples/s]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [00:07<00:00, 13.42 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:07<00:00, 13.70 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:07<00:00, 11.04 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
[INFO|configuration_utils.py:765] 2025-10-22 01:23:36,251 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-10-22 01:23:36,262 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

`torch_dtype` is deprecated! Use `dtype` instead!
[WARNING|logging.py:328] 2025-10-22 01:23:37,783 >> `torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-22 01:23:38,273 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:  20%|██        | 1/5 [02:04<08:18, 124.51s/it]Fetching 5 files:  20%|██        | 1/5 [02:04<08:18, 124.50s/it]Fetching 5 files:  20%|██        | 1/5 [02:04<08:18, 124.51s/it]Fetching 5 files:  20%|██        | 1/5 [02:04<08:18, 124.53s/it]Fetching 5 files:  40%|████      | 2/5 [02:05<02:35, 51.67s/it] Fetching 5 files: 100%|██████████| 5/5 [02:05<00:00, 25.04s/it]
[INFO|modeling_utils.py:2341] 2025-10-22 01:25:43,603 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-22 01:25:43,617 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2341] 2025-10-22 01:25:43,622 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
Fetching 5 files:  40%|████      | 2/5 [02:05<02:35, 51.68s/it] Fetching 5 files: 100%|██████████| 5/5 [02:05<00:00, 25.04s/it]
Fetching 5 files:  40%|████      | 2/5 [02:05<02:35, 51.68s/it] Fetching 5 files: 100%|██████████| 5/5 [02:05<00:00, 25.04s/it]
Fetching 5 files:  40%|████      | 2/5 [02:05<02:35, 51.69s/it] Fetching 5 files: 100%|██████████| 5/5 [02:05<00:00, 25.04s/it]
[INFO|modeling_utils.py:2341] 2025-10-22 01:25:43,641 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:23,  5.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:23,  5.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:06<00:24,  6.03s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:23,  5.97s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:11<00:17,  5.91s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:11<00:17,  5.94s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:11<00:17,  5.91s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:11<00:17,  5.92s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:17<00:11,  5.94s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:17<00:11,  5.94s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:17<00:11,  5.94s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:17<00:11,  5.95s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:23<00:05,  5.94s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:23<00:05,  5.94s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:23<00:05,  5.94s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:23<00:05,  5.95s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:25<00:00,  5.09s/it]
[INFO|configuration_utils.py:941] 2025-10-22 01:26:09,553 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-22 01:26:09,554 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-10-22 01:26:09,674 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[WARNING|trainer.py:906] 2025-10-22 01:26:16,880 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-22 01:26:16,893 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-22 01:26:17,281 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:1335] 2025-10-22 01:26:17,560 >> skipped Embedding(152064, 3584): 519.75M params
[INFO|trainer.py:1338] 2025-10-22 01:26:17,562 >> skipped: 519.75M params
[INFO|trainer.py:2519] 2025-10-22 01:26:17,706 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-22 01:26:17,706 >>   Num examples = 95,244
[INFO|trainer.py:2521] 2025-10-22 01:26:17,707 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-22 01:26:17,707 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2525] 2025-10-22 01:26:17,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2025-10-22 01:26:17,707 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2527] 2025-10-22 01:26:17,707 >>   Total optimization steps = 2,235
[INFO|trainer.py:2528] 2025-10-22 01:26:17,710 >>   Number of trainable parameters = 80,740,352
[INFO|integration_utils.py:867] 2025-10-22 01:26:17,777 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251022_012618-bf84ss2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen/Qwen2.5-VL-7B-Instruct_roboG_stagepoc_two_frames_detection_cotrain_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/bf84ss2u
  0%|          | 0/2235 [00:00<?, ?it/s][rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 122, in compute_loss
[rank2]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:               ^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank2]:     return model_forward(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank2]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank2]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/peft_model.py", line 1757, in forward
[rank2]:     return self.base_model(
[rank2]:            ^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
[rank2]:     return self.model.forward(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank2]:     output = func(self, *args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1503, in forward
[rank2]:     loss = self.loss_function(
[rank2]:            ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 55, in ForCausalLMLoss
[rank2]:     logits = logits.float()
[rank2]:              ^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 2 has a total capacity of 39.49 GiB of which 9.13 GiB is free. Including non-PyTorch memory, this process has 30.33 GiB memory in use. Of the allocated memory 29.11 GiB is allocated by PyTorch, and 128.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 122, in compute_loss
[rank1]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:               ^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/peft_model.py", line 1757, in forward
[rank1]:     return self.base_model(
[rank1]:            ^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
[rank1]:     return self.model.forward(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank1]:     output = func(self, *args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1503, in forward
[rank1]:     loss = self.loss_function(
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 55, in ForCausalLMLoss
[rank1]:     logits = logits.float()
[rank1]:              ^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 1 has a total capacity of 39.49 GiB of which 9.13 GiB is free. Including non-PyTorch memory, this process has 30.33 GiB memory in use. Of the allocated memory 29.11 GiB is allocated by PyTorch, and 128.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 122, in compute_loss
    return super().compute_loss(model, inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/peft_model.py", line 1757, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1503, in forward
    loss = self.loss_function(
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 55, in ForCausalLMLoss
    logits = logits.float()
             ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 0 has a total capacity of 39.49 GiB of which 9.17 GiB is free. Including non-PyTorch memory, this process has 30.29 GiB memory in use. Of the allocated memory 29.11 GiB is allocated by PyTorch, and 82.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 122, in compute_loss
[rank0]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:               ^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/peft_model.py", line 1757, in forward
[rank0]:     return self.base_model(
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1503, in forward
[rank0]:     loss = self.loss_function(
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 55, in ForCausalLMLoss
[rank0]:     logits = logits.float()
[rank0]:              ^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 0 has a total capacity of 39.49 GiB of which 9.17 GiB is free. Including non-PyTorch memory, this process has 30.29 GiB memory in use. Of the allocated memory 29.11 GiB is allocated by PyTorch, and 82.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 122, in compute_loss
[rank3]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:               ^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank3]:     return model_forward(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank3]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank3]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/peft_model.py", line 1757, in forward
[rank3]:     return self.base_model(
[rank3]:            ^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
[rank3]:     return self.model.forward(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank3]:     output = func(self, *args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1503, in forward
[rank3]:     loss = self.loss_function(
[rank3]:            ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 55, in ForCausalLMLoss
[rank3]:     logits = logits.float()
[rank3]:              ^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 3 has a total capacity of 39.49 GiB of which 9.15 GiB is free. Including non-PyTorch memory, this process has 30.31 GiB memory in use. Of the allocated memory 29.12 GiB is allocated by PyTorch, and 99.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1022 01:26:30.035000 3689709 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3689713 closing signal SIGTERM
W1022 01:26:30.047000 3689709 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3689714 closing signal SIGTERM
E1022 01:26:30.913000 3689709 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 3689715) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-22_01:26:30
  host      : hkn0523.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3689716)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-22_01:26:30
  host      : hkn0523.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3689715)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '35079', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen2_5vl_roboG_poc_two_frames_det_cotrain_lora.yaml']' returned non-zero exit status 1.
srun: error: hkn0523: task 0: Exited with exit code 1

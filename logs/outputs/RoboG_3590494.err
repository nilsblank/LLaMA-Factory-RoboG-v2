GpuFreq=control_disabled
[W1022 11:29:34.592253474 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 11:29:34.592269146 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 11:29:34.592272120 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1022 11:29:34.592556812 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,100 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-10-22 11:29:35,410 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:381] 2025-10-22 11:29:35,410 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/preprocessor_config.json
[INFO|image_processing_base.py:381] 2025-10-22 11:29:35,418 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-22 11:29:35,463 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-10-22 11:29:35,464 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-10-22 11:29:35,777 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:724] 2025-10-22 11:29:35,781 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-22 11:29:35,789 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1114] 2025-10-22 11:29:35,790 >> loading configuration file None
[INFO|processing_utils.py:1199] 2025-10-22 11:29:36,214 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 11:29:36.607544199 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W1022 11:29:36.607616281 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 11:29:36.619338694 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Converting format of dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 1/56022 [00:00<2:43:04,  5.73 examples/s]Converting format of dataset (num_proc=128):   0%|          | 112/56022 [00:00<01:56, 481.65 examples/s]Converting format of dataset (num_proc=128):   1%|          | 475/56022 [00:00<00:33, 1675.55 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1259/56022 [00:00<00:14, 3841.83 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2148/56022 [00:00<00:09, 5480.75 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 2737/56022 [00:00<00:11, 4471.44 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3242/56022 [00:01<00:16, 3177.28 examples/s]Converting format of dataset (num_proc=128):   6%|▋         | 3641/56022 [00:01<00:15, 3323.60 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 4037/56022 [00:01<00:15, 3379.01 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4514/56022 [00:01<00:13, 3710.01 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5024/56022 [00:01<00:12, 4065.43 examples/s]Converting format of dataset (num_proc=128):  10%|▉         | 5467/56022 [00:01<00:12, 4160.62 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 5910/56022 [00:01<00:12, 4171.85 examples/s]Converting format of dataset (num_proc=128):  11%|█▏        | 6422/56022 [00:01<00:11, 4434.82 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6880/56022 [00:01<00:11, 4440.26 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7335/56022 [00:01<00:11, 4173.48 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 7764/56022 [00:02<00:12, 3978.64 examples/s]Converting format of dataset (num_proc=128):  15%|█▍        | 8171/56022 [00:02<00:12, 3961.21 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8573/56022 [00:02<00:12, 3862.51 examples/s]Converting format of dataset (num_proc=128):  16%|█▌        | 8964/56022 [00:02<00:12, 3752.07 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9343/56022 [00:02<00:12, 3612.06 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9709/56022 [00:02<00:13, 3558.73 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 10067/56022 [00:02<00:12, 3538.81 examples/s]Converting format of dataset (num_proc=128):  19%|█▊        | 10425/56022 [00:02<00:13, 3340.84 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10809/56022 [00:02<00:13, 3463.69 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 11177/56022 [00:03<00:12, 3522.23 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11533/56022 [00:03<00:12, 3482.33 examples/s]Converting format of dataset (num_proc=128):  21%|██▏       | 11936/56022 [00:03<00:12, 3628.57 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12303/56022 [00:03<00:12, 3548.68 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 12664/56022 [00:03<00:12, 3378.45 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 13006/56022 [00:03<00:12, 3332.13 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 13354/56022 [00:03<00:12, 3372.21 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 13730/56022 [00:03<00:12, 3479.21 examples/s]Converting format of dataset (num_proc=128):  25%|██▌       | 14080/56022 [00:03<00:12, 3330.01 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14480/56022 [00:04<00:11, 3517.49 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 14846/56022 [00:04<00:11, 3555.62 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15206/56022 [00:04<00:11, 3542.87 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15574/56022 [00:04<00:11, 3579.80 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15936/56022 [00:04<00:11, 3539.84 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16330/56022 [00:04<00:11, 3598.53 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 16692/56022 [00:04<00:11, 3505.12 examples/s]Converting format of dataset (num_proc=128):  30%|███       | 17064/56022 [00:04<00:10, 3567.01 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 17422/56022 [00:04<00:11, 3431.66 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17772/56022 [00:04<00:11, 3403.61 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 18140/56022 [00:05<00:10, 3471.38 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18488/56022 [00:05<00:11, 3399.94 examples/s]Converting format of dataset (num_proc=128):  34%|███▎      | 18831/56022 [00:05<00:10, 3394.88 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19187/56022 [00:05<00:10, 3442.91 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 19533/56022 [00:05<00:10, 3351.42 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 19878/56022 [00:05<00:10, 3370.54 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 20217/56022 [00:05<00:11, 3094.93 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20607/56022 [00:05<00:10, 3315.94 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20945/56022 [00:05<00:10, 3322.13 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21283/56022 [00:06<00:10, 3250.22 examples/s]Converting format of dataset (num_proc=128):  39%|███▊      | 21611/56022 [00:06<00:10, 3167.29 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 21982/56022 [00:06<00:10, 3319.67 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 22319/56022 [00:06<00:10, 3259.29 examples/s]Converting format of dataset (num_proc=128):  40%|████      | 22665/56022 [00:06<00:10, 3297.44 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 22997/56022 [00:06<00:10, 3296.44 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23351/56022 [00:06<00:09, 3366.00 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23689/56022 [00:06<00:10, 3221.31 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24013/56022 [00:06<00:10, 3194.10 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24338/56022 [00:06<00:10, 3138.40 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24692/56022 [00:07<00:09, 3249.32 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 25019/56022 [00:07<00:09, 3127.66 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 25345/56022 [00:07<00:09, 3162.97 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25724/56022 [00:07<00:09, 3334.69 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26060/56022 [00:07<00:09, 3161.21 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26397/56022 [00:07<00:09, 3206.50 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 26720/56022 [00:07<00:09, 3170.53 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 27039/56022 [00:07<00:09, 3093.77 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27350/56022 [00:07<00:09, 3093.98 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27662/56022 [00:08<00:09, 3013.46 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 28002/56022 [00:08<00:08, 3119.33 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28319/56022 [00:08<00:09, 2860.72 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28640/56022 [00:08<00:09, 2955.74 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 28967/56022 [00:08<00:08, 3028.63 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 29326/56022 [00:08<00:08, 3184.96 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29716/56022 [00:08<00:07, 3365.96 examples/s]Converting format of dataset (num_proc=128):  54%|█████▎    | 30076/56022 [00:08<00:07, 3419.67 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30420/56022 [00:08<00:08, 2908.66 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 30726/56022 [00:09<00:09, 2750.37 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31142/56022 [00:09<00:08, 3079.27 examples/s]Converting format of dataset (num_proc=128):  56%|█████▋    | 31553/56022 [00:09<00:07, 3351.69 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 31901/56022 [00:09<00:07, 3166.06 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32227/56022 [00:09<00:07, 2996.22 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32534/56022 [00:09<00:08, 2893.57 examples/s]Converting format of dataset (num_proc=128):  59%|█████▊    | 32832/56022 [00:09<00:07, 2914.52 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 33130/56022 [00:09<00:08, 2767.78 examples/s]Converting format of dataset (num_proc=128):  60%|█████▉    | 33411/56022 [00:09<00:08, 2659.06 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 33683/56022 [00:10<00:08, 2640.20 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 33971/56022 [00:10<00:08, 2702.27 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 34254/56022 [00:10<00:07, 2737.70 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34532/56022 [00:10<00:07, 2712.05 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34821/56022 [00:10<00:07, 2750.30 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35098/56022 [00:10<00:08, 2566.73 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35442/56022 [00:10<00:07, 2810.00 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35729/56022 [00:10<00:07, 2641.91 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 36015/56022 [00:10<00:07, 2695.37 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 36332/56022 [00:11<00:06, 2827.22 examples/s]Converting format of dataset (num_proc=128):  65%|██████▌   | 36619/56022 [00:11<00:07, 2736.95 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 36909/56022 [00:11<00:06, 2775.72 examples/s]Converting format of dataset (num_proc=128):  66%|██████▋   | 37190/56022 [00:11<00:06, 2750.06 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37467/56022 [00:11<00:06, 2741.24 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37743/56022 [00:11<00:06, 2666.35 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 38079/56022 [00:11<00:06, 2860.55 examples/s]Converting format of dataset (num_proc=128):  69%|██████▊   | 38470/56022 [00:11<00:05, 3163.65 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38789/56022 [00:11<00:05, 3076.93 examples/s]Converting format of dataset (num_proc=128):  70%|██████▉   | 39100/56022 [00:12<00:05, 3026.54 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 39405/56022 [00:12<00:05, 2770.54 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39782/56022 [00:12<00:05, 3012.25 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40121/56022 [00:12<00:05, 3116.53 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40439/56022 [00:12<00:05, 2984.68 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40786/56022 [00:12<00:04, 3116.50 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 41102/56022 [00:12<00:04, 3109.95 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41416/56022 [00:12<00:05, 2896.05 examples/s]Converting format of dataset (num_proc=128):  75%|███████▍  | 41753/56022 [00:12<00:04, 3023.87 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42061/56022 [00:12<00:04, 2952.39 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42360/56022 [00:13<00:04, 2821.00 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42652/56022 [00:13<00:04, 2844.21 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 42946/56022 [00:13<00:04, 2864.73 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43266/56022 [00:13<00:04, 2946.83 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43565/56022 [00:13<00:04, 2875.23 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43884/56022 [00:13<00:04, 2959.71 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44246/56022 [00:13<00:03, 3148.23 examples/s]Converting format of dataset (num_proc=128):  80%|███████▉  | 44563/56022 [00:13<00:03, 3016.85 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 44867/56022 [00:13<00:03, 2915.97 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 45162/56022 [00:14<00:03, 2749.03 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 45497/56022 [00:14<00:03, 2910.98 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 45793/56022 [00:14<00:03, 2799.24 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 46076/56022 [00:14<00:03, 2558.38 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46362/56022 [00:14<00:03, 2629.48 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46633/56022 [00:14<00:03, 2648.90 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 46920/56022 [00:14<00:03, 2703.10 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47203/56022 [00:14<00:03, 2724.31 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 47502/56022 [00:14<00:03, 2800.95 examples/s]Converting format of dataset (num_proc=128):  85%|████████▌ | 47785/56022 [00:15<00:02, 2767.18 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48107/56022 [00:15<00:02, 2897.00 examples/s]Converting format of dataset (num_proc=128):  86%|████████▋ | 48399/56022 [00:15<00:02, 2781.22 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48679/56022 [00:15<00:02, 2631.62 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48994/56022 [00:15<00:02, 2749.56 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49299/56022 [00:15<00:02, 2820.41 examples/s]Converting format of dataset (num_proc=128):  89%|████████▊ | 49605/56022 [00:15<00:02, 2888.74 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 49936/56022 [00:15<00:02, 2953.62 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 50233/56022 [00:15<00:02, 2833.56 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 50518/56022 [00:16<00:01, 2798.69 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 50820/56022 [00:16<00:01, 2861.30 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 51108/56022 [00:16<00:01, 2785.42 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51390/56022 [00:16<00:01, 2598.15 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51653/56022 [00:16<00:01, 2550.12 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 51928/56022 [00:16<00:01, 2583.66 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 52232/56022 [00:16<00:01, 2709.77 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52651/56022 [00:16<00:01, 3133.45 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▍| 52972/56022 [00:16<00:00, 3095.46 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 53401/56022 [00:16<00:00, 3425.35 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 53748/56022 [00:17<00:00, 3385.71 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54089/56022 [00:17<00:00, 3287.85 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54423/56022 [00:17<00:00, 2999.04 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54729/56022 [00:17<00:00, 2853.44 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 55024/56022 [00:17<00:00, 2644.18 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▊| 55298/56022 [00:17<00:00, 2327.70 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55540/56022 [00:17<00:00, 2059.03 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 55762/56022 [00:18<00:00, 1821.78 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 55957/56022 [00:18<00:00, 1576.30 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56022/56022 [00:18<00:00, 3007.14 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):   1%|          | 1/83 [00:00<00:12,  6.53 examples/s]Converting format of dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:06, 12.46 examples/s]Converting format of dataset (num_proc=83):  24%|██▍       | 20/83 [00:00<00:00, 74.66 examples/s]Converting format of dataset (num_proc=83):  53%|█████▎    | 44/83 [00:00<00:00, 133.81 examples/s]Converting format of dataset (num_proc=83):  98%|█████████▊| 81/83 [00:00<00:00, 213.91 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:01<00:00, 78.11 examples/s] 
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 11:30:00.557068057 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56022 [02:34<5:26:27,  2.84 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56022 [02:46<2:28:53,  6.17 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56022 [02:56<1:28:59, 10.25 examples/s]Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56022 [02:56<54:05, 16.72 examples/s]  Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56022 [03:00<36:56, 24.29 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56022 [03:02<25:40, 34.66 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 3066/56022 [03:03<17:41, 49.88 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 3504/56022 [03:04<12:24, 70.50 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 3942/56022 [03:04<08:51, 98.01 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4380/56022 [03:05<06:38, 129.70 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 5256/56022 [03:06<03:34, 236.36 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 5694/56022 [03:11<05:15, 159.71 examples/s]Running tokenizer on dataset (num_proc=128):  11%|█         | 6132/56022 [03:12<04:14, 195.87 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6570/56022 [03:16<05:23, 152.83 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7008/56022 [03:20<05:54, 138.31 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7446/56022 [03:22<04:53, 165.40 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 7884/56022 [03:22<03:47, 211.25 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8322/56022 [03:23<02:55, 271.43 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 8760/56022 [03:23<02:14, 352.64 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9198/56022 [03:25<02:25, 322.72 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9636/56022 [03:27<02:47, 277.40 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 10074/56022 [03:32<04:33, 167.71 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10512/56022 [03:36<05:31, 137.25 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10950/56022 [03:40<05:35, 134.32 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 11388/56022 [03:41<04:18, 172.53 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 11826/56022 [03:41<03:02, 242.08 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12264/56022 [03:42<02:27, 296.18 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12701/56022 [03:44<03:02, 237.00 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 13139/56022 [03:45<02:21, 302.75 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13577/56022 [03:47<02:50, 249.33 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14014/56022 [03:48<02:25, 288.88 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14452/56022 [03:49<01:59, 347.42 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15328/56022 [03:50<01:27, 462.54 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 15766/56022 [03:53<02:16, 295.28 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16203/56022 [03:54<01:54, 346.60 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16641/56022 [03:55<02:00, 327.36 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 17079/56022 [03:57<02:01, 319.27 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 17517/56022 [03:57<01:44, 369.53 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17955/56022 [03:59<01:45, 360.32 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 18831/56022 [04:00<01:19, 465.48 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19707/56022 [04:01<01:06, 542.59 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20145/56022 [04:02<01:02, 573.19 examples/s]Running tokenizer on dataset (num_proc=128):  37%|███▋      | 20583/56022 [04:06<02:02, 290.11 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21020/56022 [04:06<01:33, 373.16 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21458/56022 [04:07<01:40, 343.26 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21896/56022 [04:10<01:58, 289.05 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22334/56022 [04:10<01:36, 347.78 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 22772/56022 [04:11<01:20, 413.39 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23210/56022 [04:11<00:59, 547.92 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23648/56022 [04:11<00:47, 686.27 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 24086/56022 [04:11<00:39, 812.34 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24960/56022 [04:12<00:22, 1362.87 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▌     | 25397/56022 [04:12<00:22, 1382.94 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25835/56022 [04:12<00:24, 1252.49 examples/s]Running tokenizer on dataset (num_proc=128):  47%|████▋     | 26273/56022 [04:13<00:27, 1080.18 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26711/56022 [04:14<00:43, 675.32 examples/s] Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27149/56022 [04:14<00:34, 845.94 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27587/56022 [04:14<00:26, 1056.19 examples/s]Running tokenizer on dataset (num_proc=128):  50%|█████     | 28024/56022 [04:15<00:22, 1245.88 examples/s]Running tokenizer on dataset (num_proc=128):  51%|█████     | 28461/56022 [04:15<00:22, 1230.01 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 29336/56022 [04:15<00:13, 1972.15 examples/s]Running tokenizer on dataset (num_proc=128):  53%|█████▎    | 29774/56022 [04:16<00:15, 1646.31 examples/s]Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30212/56022 [04:17<00:35, 721.68 examples/s] Running tokenizer on dataset (num_proc=128):  55%|█████▍    | 30650/56022 [04:18<00:33, 749.20 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▌    | 31087/56022 [04:18<00:34, 727.74 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 31962/56022 [04:19<00:26, 908.43 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32399/56022 [04:21<00:45, 516.23 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 32837/56022 [04:22<00:43, 533.59 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 33274/56022 [04:22<00:34, 655.45 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33711/56022 [04:22<00:27, 813.78 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 34149/56022 [04:23<00:31, 687.32 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 34586/56022 [04:23<00:27, 780.78 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35024/56022 [04:26<00:48, 433.25 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35461/56022 [04:26<00:34, 587.47 examples/s]Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 35898/56022 [04:26<00:35, 566.32 examples/s]Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 36335/56022 [04:27<00:32, 608.85 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 36772/56022 [04:27<00:23, 813.30 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 37210/56022 [04:27<00:18, 1008.79 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 37648/56022 [04:28<00:17, 1052.61 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38086/56022 [04:28<00:17, 1053.15 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 38524/56022 [04:30<00:30, 574.16 examples/s] Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38961/56022 [04:30<00:24, 710.22 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 39399/56022 [04:30<00:17, 937.07 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 39837/56022 [04:32<00:31, 505.94 examples/s]Running tokenizer on dataset (num_proc=128):  72%|███████▏  | 40275/56022 [04:32<00:25, 620.03 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41587/56022 [04:33<00:12, 1129.66 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 42024/56022 [04:33<00:12, 1163.94 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42461/56022 [04:33<00:10, 1301.37 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42899/56022 [04:33<00:08, 1494.72 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 43336/56022 [04:34<00:10, 1226.34 examples/s]Running tokenizer on dataset (num_proc=128):  78%|███████▊  | 43773/56022 [04:34<00:10, 1156.76 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44211/56022 [04:35<00:11, 985.79 examples/s] Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 44648/56022 [04:35<00:11, 986.19 examples/s]Running tokenizer on dataset (num_proc=128):  80%|████████  | 45086/56022 [04:36<00:10, 1016.51 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 45524/56022 [04:36<00:08, 1264.69 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 45962/56022 [04:37<00:09, 1007.90 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46400/56022 [04:37<00:08, 1092.84 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▎ | 46838/56022 [04:40<00:25, 364.36 examples/s] Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 47275/56022 [04:41<00:21, 413.60 examples/s]Running tokenizer on dataset (num_proc=128):  86%|████████▌ | 48150/56022 [04:41<00:12, 638.65 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 48588/56022 [04:43<00:17, 419.89 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49025/56022 [04:45<00:17, 390.85 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49462/56022 [04:46<00:18, 358.30 examples/s]Running tokenizer on dataset (num_proc=128):  89%|████████▉ | 49900/56022 [04:47<00:16, 362.41 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50337/56022 [04:52<00:29, 192.49 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50774/56022 [04:54<00:24, 212.66 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 51212/56022 [04:56<00:22, 216.98 examples/s]Running tokenizer on dataset (num_proc=128):  92%|█████████▏| 51649/56022 [04:56<00:14, 294.07 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 52086/56022 [04:57<00:13, 293.56 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 52523/56022 [05:00<00:15, 228.85 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 52960/56022 [05:01<00:10, 292.76 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 53835/56022 [05:03<00:05, 365.75 examples/s]Running tokenizer on dataset (num_proc=128):  97%|█████████▋| 54273/56022 [05:06<00:06, 262.82 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 54711/56022 [05:06<00:04, 322.35 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55148/56022 [05:06<00:02, 404.15 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 55585/56022 [05:08<00:01, 382.45 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [05:08<00:00, 479.20 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [05:08<00:00, 181.46 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<01:01,  1.33 examples/s]Running tokenizer on dataset (num_proc=83):   2%|▏         | 2/83 [00:00<00:30,  2.70 examples/s]Running tokenizer on dataset (num_proc=83):   6%|▌         | 5/83 [00:01<00:11,  6.64 examples/s]Running tokenizer on dataset (num_proc=83):   8%|▊         | 7/83 [00:01<00:09,  7.76 examples/s]Running tokenizer on dataset (num_proc=83):  11%|█         | 9/83 [00:01<00:08,  8.49 examples/s]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:01<00:08,  8.69 examples/s]Running tokenizer on dataset (num_proc=83):  16%|█▌        | 13/83 [00:01<00:07,  9.49 examples/s]Running tokenizer on dataset (num_proc=83):  18%|█▊        | 15/83 [00:02<00:07,  9.69 examples/s]Running tokenizer on dataset (num_proc=83):  20%|██        | 17/83 [00:02<00:06,  9.85 examples/s]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:02<00:06,  9.88 examples/s]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:02<00:06,  9.69 examples/s]Running tokenizer on dataset (num_proc=83):  28%|██▊       | 23/83 [00:02<00:05, 10.25 examples/s]Running tokenizer on dataset (num_proc=83):  30%|███       | 25/83 [00:02<00:05, 10.39 examples/s]Running tokenizer on dataset (num_proc=83):  33%|███▎      | 27/83 [00:03<00:05,  9.56 examples/s]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:03<00:05,  9.40 examples/s]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:03<00:05,  9.87 examples/s]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:03<00:05,  8.86 examples/s]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:03<00:04, 10.71 examples/s]Running tokenizer on dataset (num_proc=83):  43%|████▎     | 36/83 [00:04<00:04, 10.64 examples/s]Running tokenizer on dataset (num_proc=83):  46%|████▌     | 38/83 [00:04<00:04, 10.38 examples/s]Running tokenizer on dataset (num_proc=83):  48%|████▊     | 40/83 [00:04<00:04,  9.05 examples/s]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:04<00:03, 10.87 examples/s]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:04<00:03, 10.65 examples/s]Running tokenizer on dataset (num_proc=83):  57%|█████▋    | 47/83 [00:05<00:02, 12.25 examples/s]Running tokenizer on dataset (num_proc=83):  59%|█████▉    | 49/83 [00:05<00:03, 10.10 examples/s]Running tokenizer on dataset (num_proc=83):  61%|██████▏   | 51/83 [00:05<00:03, 10.22 examples/s]Running tokenizer on dataset (num_proc=83):  64%|██████▍   | 53/83 [00:05<00:02, 10.12 examples/s]Running tokenizer on dataset (num_proc=83):  66%|██████▋   | 55/83 [00:05<00:02, 10.36 examples/s]Running tokenizer on dataset (num_proc=83):  69%|██████▊   | 57/83 [00:06<00:02, 10.32 examples/s]Running tokenizer on dataset (num_proc=83):  71%|███████   | 59/83 [00:06<00:02, 10.28 examples/s]Running tokenizer on dataset (num_proc=83):  73%|███████▎  | 61/83 [00:06<00:02, 10.26 examples/s]Running tokenizer on dataset (num_proc=83):  76%|███████▌  | 63/83 [00:06<00:01, 11.52 examples/s]Running tokenizer on dataset (num_proc=83):  78%|███████▊  | 65/83 [00:06<00:01,  9.64 examples/s]Running tokenizer on dataset (num_proc=83):  81%|████████  | 67/83 [00:07<00:01, 10.10 examples/s]Running tokenizer on dataset (num_proc=83):  83%|████████▎ | 69/83 [00:07<00:01, 11.36 examples/s]Running tokenizer on dataset (num_proc=83):  86%|████████▌ | 71/83 [00:07<00:01, 11.01 examples/s]Running tokenizer on dataset (num_proc=83):  88%|████████▊ | 73/83 [00:07<00:00, 11.01 examples/s]Running tokenizer on dataset (num_proc=83):  90%|█████████ | 75/83 [00:07<00:00,  9.62 examples/s]Running tokenizer on dataset (num_proc=83):  93%|█████████▎| 77/83 [00:08<00:00, 10.65 examples/s]Running tokenizer on dataset (num_proc=83):  95%|█████████▌| 79/83 [00:08<00:00, 10.19 examples/s]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [00:08<00:00, 10.08 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:08<00:00, 11.41 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:08<00:00,  9.59 examples/s]
[INFO|configuration_utils.py:763] 2025-10-22 11:35:22,648 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/config.json
[INFO|configuration_utils.py:839] 2025-10-22 11:35:22,658 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_size": 2560,
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "pad_token_id": 151643,
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": false,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": false,
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "dtype": "bfloat16",
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[WARNING|logging.py:328] 2025-10-22 11:35:24,078 >> `torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1169] 2025-10-22 11:35:24,079 >> loading weights file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-22 11:35:24,081 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-22 11:35:24,098 >> Generate config GenerationConfig {
  "eos_token_id": 151645,
  "pad_token_id": 151643
}

[INFO|modeling_utils.py:2341] 2025-10-22 11:35:24,103 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-22 11:35:24,126 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]
[INFO|configuration_utils.py:939] 2025-10-22 11:35:38,096 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-22 11:35:38,096 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-22 11:35:38,096 >> Could not locate the custom_generate/generate.py inside /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_temporal_grounding_box_train.
[WARNING|trainer.py:906] 2025-10-22 11:35:38,154 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-22 11:35:38,166 >> Using auto half precision backend
[INFO|trainer.py:4643] 2025-10-22 11:35:38,584 >> 
***** Running Prediction *****
[INFO|trainer.py:4645] 2025-10-22 11:35:38,584 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-22 11:35:38,584 >>   Batch size = 8
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 165, in run_sft
[rank1]:     predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 255, in predict
[rank1]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4567, in predict
[rank1]:     output = eval_loop(
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank1]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank1]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 271, in prediction_step
[rank1]:     loss, generated_tokens, _ = super().prediction_step(
[rank1]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank1]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank1]:     self._validate_model_kwargs(model_kwargs.copy())
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank1]:     raise ValueError(
[rank1]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 165, in run_sft
[rank2]:     predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 255, in predict
[rank2]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4567, in predict
[rank2]:     output = eval_loop(
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank2]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank2]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 271, in prediction_step
[rank2]:     loss, generated_tokens, _ = super().prediction_step(
[rank2]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank2]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank2]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank2]:     self._validate_model_kwargs(model_kwargs.copy())
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank2]:     raise ValueError(
[rank2]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 165, in run_sft
[rank3]:     predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
[rank3]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 255, in predict
[rank3]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4567, in predict
[rank3]:     output = eval_loop(
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank3]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank3]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 271, in prediction_step
[rank3]:     loss, generated_tokens, _ = super().prediction_step(
[rank3]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank3]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank3]:     self._validate_model_kwargs(model_kwargs.copy())
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank3]:     raise ValueError(
[rank3]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 165, in run_sft
[rank0]:     predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 255, in predict
[rank0]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4567, in predict
[rank0]:     output = eval_loop(
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank0]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 271, in prediction_step
[rank0]:     loss, generated_tokens, _ = super().prediction_step(
[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank0]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank0]:     self._validate_model_kwargs(model_kwargs.copy())
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank0]:     raise ValueError(
[rank0]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank0]:[W1022 11:35:41.363716178 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1022 11:35:43.688000 3410619 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3410621 closing signal SIGTERM
W1022 11:35:43.713000 3410619 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3410622 closing signal SIGTERM
W1022 11:35:43.714000 3410619 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3410623 closing signal SIGTERM
E1022 11:35:43.942000 3410619 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 3 (pid: 3410624) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-22_11:35:43
  host      : hkn0814.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3410624)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '43287', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen.yaml']' returned non-zero exit status 1.
srun: error: hkn0814: task 0: Exited with exit code 1

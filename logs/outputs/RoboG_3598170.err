GpuFreq=control_disabled
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[W1025 01:59:39.963382613 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1025 01:59:39.963382002 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1025 01:59:39.963384931 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1025 01:59:39.963385569 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:40,672 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-25 01:59:40,867 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-25 01:59:41,364 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-25 01:59:41,624 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-25 01:59:41,634 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-25 01:59:41,884 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-25 01:59:42,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-25 01:59:42,483 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-25 01:59:42,484 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-10-25 01:59:43,079 >> loading configuration file processor_config.json from cache at None
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1025 01:59:43.875428426 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W1025 01:59:43.877095790 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1025 01:59:43.919613523 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-10-25 01:59:43,448 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-4B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

Converting format of dataset (num_proc=128):   0%|          | 0/30361 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 1/30361 [00:00<5:16:36,  1.60 examples/s]Converting format of dataset (num_proc=128):   0%|          | 119/30361 [00:00<02:17, 219.89 examples/s]Converting format of dataset (num_proc=128):   1%|          | 207/30361 [00:00<01:24, 357.18 examples/s]Converting format of dataset (num_proc=128):   1%|          | 284/30361 [00:00<01:06, 451.32 examples/s]Converting format of dataset (num_proc=128):   1%|          | 376/30361 [00:01<00:53, 561.71 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1057/30361 [00:01<00:13, 2178.11 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 1343/30361 [00:01<00:13, 2106.63 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 1648/30361 [00:01<00:12, 2344.65 examples/s]Converting format of dataset (num_proc=128):   6%|▋         | 1920/30361 [00:01<00:12, 2307.40 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 2262/30361 [00:01<00:11, 2536.97 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 2536/30361 [00:01<00:11, 2321.03 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 2786/30361 [00:01<00:11, 2365.20 examples/s]Converting format of dataset (num_proc=128):  10%|█         | 3040/30361 [00:01<00:11, 2310.85 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 3282/30361 [00:02<00:11, 2318.19 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 3521/30361 [00:02<00:13, 2032.84 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 3838/30361 [00:02<00:11, 2279.22 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 4076/30361 [00:02<00:11, 2201.68 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 4355/30361 [00:02<00:11, 2324.07 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 4595/30361 [00:02<00:11, 2242.79 examples/s]Converting format of dataset (num_proc=128):  16%|█▌        | 4846/30361 [00:02<00:11, 2314.48 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 5083/30361 [00:02<00:11, 2181.20 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 5306/30361 [00:03<00:11, 2133.33 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 5580/30361 [00:03<00:10, 2292.64 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 5814/30361 [00:03<00:11, 2133.80 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 6063/30361 [00:03<00:10, 2219.70 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 6347/30361 [00:03<00:10, 2365.18 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 6601/30361 [00:03<00:09, 2413.78 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 6912/30361 [00:03<00:09, 2575.26 examples/s]Converting format of dataset (num_proc=128):  24%|██▎       | 7173/30361 [00:03<00:09, 2548.66 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 7455/30361 [00:03<00:08, 2615.84 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 7772/30361 [00:03<00:08, 2774.06 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 8051/30361 [00:04<00:08, 2741.31 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 8327/30361 [00:04<00:08, 2668.57 examples/s]Converting format of dataset (num_proc=128):  29%|██▊       | 8690/30361 [00:04<00:07, 2945.16 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 8988/30361 [00:04<00:07, 2712.87 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 9267/30361 [00:04<00:08, 2610.57 examples/s]Converting format of dataset (num_proc=128):  31%|███▏      | 9532/30361 [00:04<00:08, 2533.81 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 9817/30361 [00:04<00:07, 2618.19 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 10084/30361 [00:04<00:08, 2329.59 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 10324/30361 [00:05<00:10, 1861.11 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 10528/30361 [00:05<00:11, 1755.26 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 10788/30361 [00:05<00:10, 1945.58 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 11002/30361 [00:05<00:09, 1948.72 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 11260/30361 [00:05<00:09, 2109.90 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 11540/30361 [00:05<00:08, 2293.90 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 11802/30361 [00:05<00:07, 2382.19 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 12063/30361 [00:05<00:07, 2446.59 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 12313/30361 [00:05<00:07, 2459.12 examples/s]Converting format of dataset (num_proc=128):  41%|████▏     | 12563/30361 [00:06<00:07, 2363.83 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 12803/30361 [00:06<00:09, 1919.46 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 13011/30361 [00:06<00:12, 1384.74 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 13180/30361 [00:06<00:16, 1066.59 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 13317/30361 [00:06<00:17, 973.05 examples/s] Converting format of dataset (num_proc=128):  44%|████▍     | 13435/30361 [00:07<00:18, 908.57 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 13539/30361 [00:07<00:18, 901.74 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 13638/30361 [00:07<00:19, 872.74 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 13731/30361 [00:07<00:19, 840.78 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 13819/30361 [00:07<00:19, 842.63 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 13906/30361 [00:07<00:19, 840.82 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 14005/30361 [00:07<00:19, 846.56 examples/s]Converting format of dataset (num_proc=128):  46%|████▋     | 14101/30361 [00:07<00:18, 871.25 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 14200/30361 [00:08<00:17, 901.11 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 14298/30361 [00:08<00:17, 917.71 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 14391/30361 [00:08<00:17, 911.27 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 14502/30361 [00:08<00:16, 965.15 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 14600/30361 [00:08<00:16, 963.04 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 14720/30361 [00:08<00:15, 1030.20 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 14873/30361 [00:08<00:13, 1176.30 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 15161/30361 [00:08<00:09, 1675.12 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 15469/30361 [00:08<00:07, 2089.42 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 15681/30361 [00:08<00:07, 2048.98 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 15887/30361 [00:09<00:07, 1851.41 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 16106/30361 [00:09<00:07, 1940.07 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 16348/30361 [00:09<00:06, 2073.65 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 16648/30361 [00:09<00:05, 2302.54 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 16916/30361 [00:09<00:05, 2402.91 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 17159/30361 [00:09<00:05, 2314.76 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 17417/30361 [00:09<00:05, 2382.24 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 17664/30361 [00:09<00:05, 2361.13 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 17935/30361 [00:09<00:05, 2460.47 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 18237/30361 [00:10<00:04, 2623.03 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 18514/30361 [00:10<00:04, 2659.27 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 18781/30361 [00:10<00:04, 2659.07 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 19062/30361 [00:10<00:04, 2696.36 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 19373/30361 [00:10<00:03, 2816.66 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 19657/30361 [00:10<00:04, 2575.53 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 19924/30361 [00:10<00:04, 2596.32 examples/s]Converting format of dataset (num_proc=128):  66%|██████▋   | 20188/30361 [00:10<00:03, 2543.81 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 20510/30361 [00:10<00:03, 2734.02 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 20787/30361 [00:10<00:03, 2523.29 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 21051/30361 [00:11<00:03, 2545.17 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 21337/30361 [00:11<00:03, 2631.16 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 21767/30361 [00:11<00:02, 3106.80 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 22083/30361 [00:11<00:02, 2815.59 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 22485/30361 [00:11<00:02, 3111.69 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 22805/30361 [00:11<00:02, 3098.95 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 23147/30361 [00:11<00:02, 3187.61 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 23473/30361 [00:11<00:02, 3175.00 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 23810/30361 [00:11<00:02, 3228.15 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 24136/30361 [00:12<00:01, 3144.03 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 24453/30361 [00:12<00:01, 3148.76 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 24770/30361 [00:12<00:01, 3108.22 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 25083/30361 [00:12<00:01, 3028.44 examples/s]Converting format of dataset (num_proc=128):  84%|████████▎ | 25412/30361 [00:12<00:01, 3102.04 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 25726/30361 [00:12<00:01, 3002.01 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 26062/30361 [00:12<00:01, 3071.58 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 26415/30361 [00:12<00:01, 3177.24 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 26750/30361 [00:12<00:01, 3218.26 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 27111/30361 [00:12<00:00, 3331.34 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 27447/30361 [00:13<00:00, 3242.60 examples/s]Converting format of dataset (num_proc=128):  91%|█████████▏| 27775/30361 [00:13<00:00, 3163.38 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 28095/30361 [00:13<00:00, 3115.93 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 28503/30361 [00:13<00:00, 3379.24 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 28848/30361 [00:13<00:00, 3257.58 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 29178/30361 [00:13<00:00, 3247.39 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 29560/30361 [00:13<00:00, 3309.29 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 30022/30361 [00:13<00:00, 3623.17 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 30361/30361 [00:17<00:00, 1784.67 examples/s]
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
Converting format of dataset (num_proc=62):   0%|          | 0/62 [00:00<?, ? examples/s]Converting format of dataset (num_proc=62):  10%|▉         | 6/62 [00:00<00:00, 59.80 examples/s]Converting format of dataset (num_proc=62):  19%|█▉        | 12/62 [00:00<00:01, 27.04 examples/s]Converting format of dataset (num_proc=62):  31%|███       | 19/62 [00:00<00:01, 38.46 examples/s]Converting format of dataset (num_proc=62):  40%|████      | 25/62 [00:01<00:01, 19.59 examples/s]Converting format of dataset (num_proc=62):  47%|████▋     | 29/62 [00:01<00:01, 21.10 examples/s]Converting format of dataset (num_proc=62):  55%|█████▍    | 34/62 [00:01<00:01, 24.47 examples/s]Converting format of dataset (num_proc=62):  69%|██████▉   | 43/62 [00:01<00:00, 35.14 examples/s]Converting format of dataset (num_proc=62):  98%|█████████▊| 61/62 [00:01<00:00, 64.18 examples/s]Converting format of dataset (num_proc=62): 100%|██████████| 62/62 [00:03<00:00, 15.58 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1025 02:00:08.400592785 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/30361 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 237/30361 [01:19<2:48:52,  2.97 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 474/30361 [01:20<1:10:12,  7.09 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 711/30361 [01:22<39:18, 12.57 examples/s]  Running tokenizer on dataset (num_proc=128):   3%|▎         | 949/30361 [01:22<23:52, 20.54 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 1186/30361 [01:22<15:21, 31.66 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 1424/30361 [01:23<10:10, 47.41 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 1662/30361 [01:30<11:42, 40.88 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 1900/30361 [01:31<08:14, 57.56 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 2137/30361 [01:31<05:52, 80.07 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 2375/30361 [01:33<05:09, 90.39 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 2613/30361 [01:33<03:52, 119.36 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 2851/30361 [01:34<03:01, 151.32 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 3088/30361 [01:36<02:57, 153.96 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 3563/30361 [01:36<01:48, 246.50 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 3800/30361 [01:36<01:25, 309.05 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 4037/30361 [01:37<01:16, 345.20 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 4275/30361 [01:37<01:01, 420.91 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 4513/30361 [01:38<01:08, 380.11 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 4751/30361 [01:38<01:03, 404.51 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 4989/30361 [01:39<01:09, 367.41 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 5227/30361 [01:39<00:57, 438.53 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 5464/30361 [01:41<01:25, 290.34 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 5702/30361 [01:41<01:05, 377.60 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 5940/30361 [01:42<01:11, 343.36 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 6178/30361 [01:42<01:03, 378.66 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 6890/30361 [01:43<00:33, 696.23 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 7127/30361 [01:43<00:33, 686.36 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 7365/30361 [01:43<00:31, 732.20 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 7603/30361 [01:44<00:32, 700.45 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 7841/30361 [01:44<00:31, 716.51 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 8078/30361 [01:44<00:26, 854.99 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 8316/30361 [01:44<00:21, 1020.32 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 8791/30361 [01:46<00:38, 566.04 examples/s] Running tokenizer on dataset (num_proc=128):  30%|██▉       | 9028/30361 [01:46<00:39, 545.41 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 9502/30361 [01:46<00:25, 832.65 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 9740/30361 [01:47<00:36, 562.19 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 9977/30361 [01:47<00:30, 657.88 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 10214/30361 [01:49<00:51, 387.57 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 10451/30361 [01:49<00:58, 337.50 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 10926/30361 [01:50<00:38, 502.84 examples/s]Running tokenizer on dataset (num_proc=128):  37%|███▋      | 11163/30361 [01:51<00:56, 338.50 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 11400/30361 [01:51<00:45, 413.00 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 11638/30361 [01:54<01:18, 239.24 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 11875/30361 [01:54<01:11, 259.54 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 12112/30361 [01:56<01:20, 226.10 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 12349/30361 [01:56<01:06, 272.49 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 12586/30361 [01:57<01:00, 292.24 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 12823/30361 [01:57<00:50, 344.29 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 13060/30361 [01:58<00:45, 378.25 examples/s]Running tokenizer on dataset (num_proc=128):  44%|████▍     | 13297/30361 [01:59<00:49, 343.26 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 13534/30361 [01:59<00:54, 310.49 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▌     | 13771/30361 [02:00<00:43, 377.43 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 14008/30361 [02:00<00:37, 433.86 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 14482/30361 [02:00<00:23, 663.93 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 14719/30361 [02:01<00:21, 719.30 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 14956/30361 [02:01<00:21, 729.97 examples/s]Running tokenizer on dataset (num_proc=128):  50%|█████     | 15193/30361 [02:02<00:25, 589.62 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 15904/30361 [02:02<00:21, 677.69 examples/s]Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 16378/30361 [02:03<00:15, 903.17 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▍    | 16615/30361 [02:03<00:15, 904.27 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▌    | 16852/30361 [02:03<00:14, 955.21 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 17089/30361 [02:03<00:12, 1093.50 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 17326/30361 [02:03<00:11, 1167.28 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 17563/30361 [02:04<00:09, 1291.77 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 17800/30361 [02:04<00:10, 1156.70 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 18748/30361 [02:04<00:06, 1864.22 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 18985/30361 [02:05<00:08, 1342.17 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 19222/30361 [02:05<00:08, 1268.73 examples/s]Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 19696/30361 [02:05<00:06, 1631.89 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 19933/30361 [02:05<00:08, 1224.06 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 20170/30361 [02:05<00:07, 1319.36 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 20407/30361 [02:06<00:10, 949.48 examples/s] Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 20644/30361 [02:06<00:09, 987.51 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 21118/30361 [02:07<00:11, 778.21 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 21355/30361 [02:07<00:10, 877.96 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 21592/30361 [02:07<00:09, 937.01 examples/s]Running tokenizer on dataset (num_proc=128):  72%|███████▏  | 21829/30361 [02:07<00:09, 932.00 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 22066/30361 [02:08<00:07, 1068.30 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 22540/30361 [02:08<00:05, 1398.35 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 23014/30361 [02:08<00:07, 1034.18 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 23251/30361 [02:09<00:06, 1155.24 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 23488/30361 [02:09<00:05, 1201.67 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 23962/30361 [02:09<00:05, 1191.92 examples/s]Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 24199/30361 [02:09<00:05, 1230.11 examples/s]Running tokenizer on dataset (num_proc=128):  80%|████████  | 24436/30361 [02:09<00:04, 1262.53 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 24673/30361 [02:10<00:05, 1117.81 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 24910/30361 [02:11<00:11, 488.18 examples/s] Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 25621/30361 [02:12<00:07, 676.57 examples/s]Running tokenizer on dataset (num_proc=128):  86%|████████▌ | 26095/30361 [02:12<00:05, 760.93 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 26569/30361 [02:13<00:04, 835.68 examples/s]Running tokenizer on dataset (num_proc=128):  89%|████████▉ | 27043/30361 [02:13<00:03, 957.95 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 27280/30361 [02:13<00:03, 849.30 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 27517/30361 [02:14<00:03, 943.73 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 27754/30361 [02:15<00:04, 537.27 examples/s]Running tokenizer on dataset (num_proc=128):  92%|█████████▏| 27991/30361 [02:15<00:04, 548.60 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 28228/30361 [02:16<00:05, 407.98 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 28702/30361 [02:17<00:03, 533.27 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▌| 28939/30361 [02:19<00:04, 284.74 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 29176/30361 [02:19<00:03, 317.66 examples/s]Running tokenizer on dataset (num_proc=128):  97%|█████████▋| 29413/30361 [02:20<00:03, 304.74 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 29650/30361 [02:20<00:01, 391.56 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 29887/30361 [02:21<00:01, 308.37 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 30124/30361 [02:22<00:00, 333.68 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 30124/30361 [02:32<00:00, 333.68 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 30361/30361 [03:01<00:00, 20.18 examples/s] Running tokenizer on dataset (num_proc=128): 100%|██████████| 30361/30361 [03:01<00:00, 167.43 examples/s]
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
Running tokenizer on dataset (num_proc=62):   0%|          | 0/62 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=62):   2%|▏         | 1/62 [00:00<00:41,  1.48 examples/s]Running tokenizer on dataset (num_proc=62):   5%|▍         | 3/62 [00:00<00:16,  3.67 examples/s]Running tokenizer on dataset (num_proc=62):   8%|▊         | 5/62 [00:01<00:12,  4.58 examples/s]Running tokenizer on dataset (num_proc=62):  13%|█▎        | 8/62 [00:01<00:09,  5.93 examples/s]Running tokenizer on dataset (num_proc=62):  19%|█▉        | 12/62 [00:02<00:07,  6.99 examples/s]Running tokenizer on dataset (num_proc=62):  27%|██▋       | 17/62 [00:02<00:05,  8.59 examples/s]Running tokenizer on dataset (num_proc=62):  34%|███▍      | 21/62 [00:02<00:03, 10.90 examples/s]Running tokenizer on dataset (num_proc=62):  40%|████      | 25/62 [00:02<00:02, 13.04 examples/s]Running tokenizer on dataset (num_proc=62):  44%|████▎     | 27/62 [00:03<00:02, 12.57 examples/s]Running tokenizer on dataset (num_proc=62):  47%|████▋     | 29/62 [00:03<00:02, 12.08 examples/s]Running tokenizer on dataset (num_proc=62):  50%|█████     | 31/62 [00:03<00:02, 11.39 examples/s]Running tokenizer on dataset (num_proc=62):  53%|█████▎    | 33/62 [00:03<00:02, 12.32 examples/s]Running tokenizer on dataset (num_proc=62):  56%|█████▋    | 35/62 [00:03<00:02, 10.72 examples/s]Running tokenizer on dataset (num_proc=62):  60%|█████▉    | 37/62 [00:04<00:03,  7.75 examples/s]Running tokenizer on dataset (num_proc=62):  66%|██████▌   | 41/62 [00:05<00:03,  6.16 examples/s]Running tokenizer on dataset (num_proc=62):  81%|████████  | 50/62 [00:05<00:01, 10.87 examples/s]Running tokenizer on dataset (num_proc=62):  87%|████████▋ | 54/62 [00:05<00:00, 12.61 examples/s]Running tokenizer on dataset (num_proc=62):  90%|█████████ | 56/62 [00:06<00:00, 10.83 examples/s]Running tokenizer on dataset (num_proc=62):  97%|█████████▋| 60/62 [00:06<00:00, 10.67 examples/s]Running tokenizer on dataset (num_proc=62): 100%|██████████| 62/62 [00:06<00:00,  9.49 examples/s]
[INFO|configuration_utils.py:765] 2025-10-25 02:03:21,294 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/config.json
[INFO|configuration_utils.py:839] 2025-10-25 02:03:21,341 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
num_proc must be <= 62. Reducing num_proc to 62 for dataset of size 62.
[WARNING|logging.py:328] 2025-10-25 02:03:21,763 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-25 02:03:21,767 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-25 02:03:21,774 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-25 02:03:21,785 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-10-25 02:03:21,791 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:2341] 2025-10-25 02:03:22,276 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
[INFO|configuration_utils.py:941] 2025-10-25 02:03:36,571 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-25 02:03:36,572 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-25 02:03:36,698 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-4B-Instruct.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[WARNING|trainer.py:906] 2025-10-25 02:03:36,736 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-25 02:03:36,766 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-25 02:03:37,210 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:2519] 2025-10-25 02:03:37,837 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-25 02:03:37,838 >>   Num examples = 30,361
[INFO|trainer.py:2521] 2025-10-25 02:03:37,838 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-25 02:03:37,838 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:2525] 2025-10-25 02:03:37,838 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-10-25 02:03:37,838 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2527] 2025-10-25 02:03:37,838 >>   Total optimization steps = 1,425
[INFO|trainer.py:2528] 2025-10-25 02:03:37,839 >>   Number of trainable parameters = 4,106,660,864
[INFO|integration_utils.py:867] 2025-10-25 02:03:37,840 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251025_020338-cxkqtmjw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen/Qwen3-VL-4B-Instruct_roboG_stagepoc_grounding_only_video_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/cxkqtmjw
  0%|          | 0/1425 [00:00<?, ?it/s]Traceback (most recent call last):
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 139, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
[rank2]:     ret = func(*args, **kwargs)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
[rank2]:     has_complex = self._init_group(
[rank2]:                   ^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 181, in _init_group
[rank2]:     state["exp_avg_sq"] = torch.zeros_like(
[rank2]:                           ^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacity of 39.49 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 39.45 GiB memory in use. Of the allocated memory 38.09 GiB is allocated by PyTorch, and 269.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 139, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
[rank1]:     ret = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
[rank1]:     has_complex = self._init_group(
[rank1]:                   ^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 177, in _init_group
[rank1]:     state["exp_avg"] = torch.zeros_like(
[rank1]:                        ^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 39.49 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 39.45 GiB memory in use. Of the allocated memory 38.13 GiB is allocated by PyTorch, and 222.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 139, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank3]:     self.optimizer.step()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank3]:     self.optimizer.step(closure)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
[rank3]:     ret = func(*args, **kwargs)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
[rank3]:     has_complex = self._init_group(
[rank3]:                   ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 177, in _init_group
[rank3]:     state["exp_avg"] = torch.zeros_like(
[rank3]:                        ^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 3 has a total capacity of 39.49 GiB of which 48.00 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.04 GiB is allocated by PyTorch, and 275.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 139, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
    self.optimizer.step()
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
    self.optimizer.step(closure)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 177, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 39.45 GiB memory in use. Of the allocated memory 38.14 GiB is allocated by PyTorch, and 221.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 139, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
[rank0]:     ret = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
[rank0]:     has_complex = self._init_group(
[rank0]:                   ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/optim/adam.py", line 177, in _init_group
[rank0]:     state["exp_avg"] = torch.zeros_like(
[rank0]:                        ^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 39.45 GiB memory in use. Of the allocated memory 38.14 GiB is allocated by PyTorch, and 221.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1025 02:04:00.016000 1325594 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1325598 closing signal SIGTERM
W1025 02:04:00.017000 1325594 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1325599 closing signal SIGTERM
W1025 02:04:00.018000 1325594 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1325601 closing signal SIGTERM
E1025 02:04:00.483000 1325594 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 1325600) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-25_02:04:00
  host      : hkn0405.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1325600)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '35859', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen_only_video.yaml']' returned non-zero exit status 1.
srun: error: hkn0405: task 0: Exited with exit code 1

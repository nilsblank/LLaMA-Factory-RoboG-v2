GpuFreq=control_disabled
[W1021 23:03:55.325283828 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 23:03:55.326204911 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 23:03:55.337297779 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 23:03:55.352841094 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:58,183 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 23:03:58,363 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-21 23:03:58,848 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-21 23:03:59,089 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-21 23:03:59,095 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 23:03:59,520 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 23:03:59,676 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-21 23:04:00,048 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-21 23:04:00,049 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-10-21 23:04:00,641 >> loading configuration file processor_config.json from cache at None
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1021 23:04:00.406675230 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1021 23:04:00.468945125 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1021 23:04:00.520517921 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-10-21 23:04:00,926 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-4B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

Converting format of dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 43/56022 [00:00<02:13, 418.76 examples/s]Converting format of dataset (num_proc=128):   1%|▏         | 819/56022 [00:00<00:11, 4667.48 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1494/56022 [00:00<00:09, 5608.34 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2135/56022 [00:00<00:09, 5909.87 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 2767/56022 [00:00<00:08, 6049.84 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3441/56022 [00:00<00:08, 6263.66 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 4089/56022 [00:00<00:08, 6317.00 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4751/56022 [00:00<00:08, 6408.19 examples/s]Converting format of dataset (num_proc=128):  10%|▉         | 5406/56022 [00:00<00:07, 6435.47 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 6052/56022 [00:01<00:07, 6336.10 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6710/56022 [00:01<00:07, 6404.03 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7352/56022 [00:01<00:07, 6408.09 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 7994/56022 [00:01<00:07, 6389.87 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8643/56022 [00:01<00:07, 6401.63 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9310/56022 [00:01<00:07, 6474.55 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 9958/56022 [00:01<00:07, 6451.48 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10604/56022 [00:01<00:07, 6377.58 examples/s]Converting format of dataset (num_proc=128):  20%|██        | 11262/56022 [00:01<00:06, 6437.30 examples/s]Converting format of dataset (num_proc=128):  21%|██▏       | 11921/56022 [00:01<00:06, 6466.50 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12575/56022 [00:02<00:06, 6461.43 examples/s]Converting format of dataset (num_proc=128):  24%|██▎       | 13223/56022 [00:02<00:06, 6427.46 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 13867/56022 [00:02<00:06, 6378.64 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14548/56022 [00:02<00:06, 6489.59 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15200/56022 [00:02<00:06, 6483.70 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15850/56022 [00:02<00:06, 6364.05 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16523/56022 [00:02<00:06, 6461.13 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 17172/56022 [00:02<00:06, 6423.37 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17819/56022 [00:02<00:05, 6431.82 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18476/56022 [00:02<00:05, 6462.16 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19123/56022 [00:03<00:05, 6442.14 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 19768/56022 [00:03<00:05, 6436.28 examples/s]Converting format of dataset (num_proc=128):  36%|███▋      | 20413/56022 [00:03<00:05, 6438.50 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21059/56022 [00:03<00:05, 6434.89 examples/s]Converting format of dataset (num_proc=128):  39%|███▊      | 21705/56022 [00:03<00:05, 6340.12 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 22351/56022 [00:03<00:05, 6374.58 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 23034/56022 [00:03<00:05, 6488.29 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23684/56022 [00:03<00:05, 6372.00 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24325/56022 [00:03<00:04, 6378.32 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 24975/56022 [00:03<00:04, 6413.16 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25629/56022 [00:04<00:04, 6450.20 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26278/56022 [00:04<00:04, 6415.37 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 26921/56022 [00:04<00:04, 6400.40 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27569/56022 [00:04<00:04, 6397.10 examples/s]Converting format of dataset (num_proc=128):  50%|█████     | 28225/56022 [00:04<00:04, 6444.04 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 28871/56022 [00:04<00:04, 6424.99 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29516/56022 [00:04<00:04, 6409.20 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30157/56022 [00:04<00:04, 6392.93 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 30805/56022 [00:04<00:03, 6415.88 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31451/56022 [00:04<00:03, 6426.58 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 32094/56022 [00:05<00:03, 6361.81 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32751/56022 [00:05<00:03, 6421.15 examples/s]Converting format of dataset (num_proc=128):  60%|█████▉    | 33395/56022 [00:05<00:03, 6369.30 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 34045/56022 [00:05<00:03, 6406.02 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34692/56022 [00:05<00:03, 6413.00 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35336/56022 [00:05<00:03, 6386.37 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35990/56022 [00:05<00:03, 6428.48 examples/s]Converting format of dataset (num_proc=128):  65%|██████▌   | 36634/56022 [00:05<00:03, 6394.32 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37274/56022 [00:05<00:02, 6374.25 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 37915/56022 [00:05<00:02, 6381.67 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38555/56022 [00:06<00:02, 6310.02 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 39247/56022 [00:06<00:02, 6481.60 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39897/56022 [00:06<00:02, 6462.57 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40545/56022 [00:06<00:02, 6383.10 examples/s]Converting format of dataset (num_proc=128):  74%|███████▎  | 41200/56022 [00:06<00:02, 6432.15 examples/s]Converting format of dataset (num_proc=128):  75%|███████▍  | 41846/56022 [00:06<00:02, 6431.01 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42490/56022 [00:06<00:02, 6396.70 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43131/56022 [00:06<00:02, 6380.27 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43772/56022 [00:06<00:01, 6335.09 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44412/56022 [00:06<00:01, 6353.61 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 45062/56022 [00:07<00:01, 6384.17 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 45701/56022 [00:07<00:01, 6367.79 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46352/56022 [00:07<00:01, 6364.30 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47009/56022 [00:07<00:01, 6423.57 examples/s]Converting format of dataset (num_proc=128):  85%|████████▌ | 47653/56022 [00:07<00:01, 6400.23 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48294/56022 [00:07<00:01, 6402.88 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48944/56022 [00:07<00:01, 6418.27 examples/s]Converting format of dataset (num_proc=128):  89%|████████▊ | 49588/56022 [00:07<00:01, 6421.41 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 50231/56022 [00:07<00:00, 6403.41 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 50874/56022 [00:08<00:00, 6342.29 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51521/56022 [00:08<00:00, 6377.13 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 52178/56022 [00:08<00:00, 6409.31 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52825/56022 [00:08<00:00, 6394.09 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 53478/56022 [00:08<00:00, 6434.26 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54122/56022 [00:08<00:00, 6375.69 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54761/56022 [00:08<00:00, 6329.59 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55420/56022 [00:08<00:00, 6373.96 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56022/56022 [00:08<00:00, 6301.23 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):   8%|▊         | 7/83 [00:00<00:01, 66.66 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 291.22 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1021 23:04:15.697591436 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56022 [03:19<7:01:08,  2.20 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56022 [03:30<3:05:41,  4.95 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56022 [03:34<1:43:47,  8.78 examples/s]Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56022 [03:37<1:05:18, 13.85 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56022 [03:40<43:09, 20.79 examples/s]  Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56022 [03:41<29:01, 30.66 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 3066/56022 [03:42<19:57, 44.22 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 3504/56022 [03:44<14:53, 58.80 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 3942/56022 [03:44<10:20, 83.88 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4380/56022 [03:44<07:13, 119.18 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 4818/56022 [03:51<08:48, 96.95 examples/s] Running tokenizer on dataset (num_proc=128):   9%|▉         | 5256/56022 [03:52<06:42, 126.08 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 5694/56022 [03:52<04:56, 169.84 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6570/56022 [03:55<03:43, 221.46 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7008/56022 [03:56<03:09, 257.99 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7446/56022 [04:00<04:32, 178.35 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 7884/56022 [04:01<03:41, 217.58 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8322/56022 [04:02<03:15, 243.45 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9198/56022 [04:04<02:22, 327.48 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9636/56022 [04:05<02:09, 358.94 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 10074/56022 [04:10<03:50, 198.99 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10512/56022 [04:11<03:21, 226.19 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10950/56022 [04:12<02:49, 266.40 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 11388/56022 [04:14<02:59, 248.10 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 11826/56022 [04:18<04:14, 173.45 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12263/56022 [04:19<03:29, 208.91 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12701/56022 [04:23<04:10, 172.67 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 13139/56022 [04:24<03:14, 219.96 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13577/56022 [04:25<02:43, 260.28 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14015/56022 [04:26<02:32, 275.75 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14453/56022 [04:27<02:04, 335.04 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 14891/56022 [04:27<01:39, 413.46 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15329/56022 [04:28<01:41, 401.98 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 15767/56022 [04:28<01:13, 549.65 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16204/56022 [04:29<00:53, 740.31 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16642/56022 [04:29<00:46, 852.78 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 17080/56022 [04:30<01:02, 624.64 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 17518/56022 [04:31<01:00, 636.32 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17956/56022 [04:35<02:38, 239.79 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 18394/56022 [04:36<02:04, 303.13 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 18831/56022 [04:36<01:33, 398.75 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 19269/56022 [04:38<01:51, 329.37 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19707/56022 [04:39<01:39, 366.22 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20144/56022 [04:41<01:52, 318.10 examples/s]Running tokenizer on dataset (num_proc=128):  37%|███▋      | 20582/56022 [04:41<01:35, 369.35 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21020/56022 [04:42<01:16, 456.06 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21458/56022 [04:43<01:17, 446.67 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21896/56022 [04:43<01:02, 546.37 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22334/56022 [04:44<00:58, 571.47 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 22772/56022 [04:44<00:48, 686.71 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23209/56022 [04:47<01:27, 372.96 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23646/56022 [04:47<01:04, 498.22 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 24084/56022 [04:47<00:56, 560.93 examples/s]Running tokenizer on dataset (num_proc=128):  44%|████▍     | 24522/56022 [04:47<00:41, 752.60 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24960/56022 [04:48<00:32, 967.55 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25835/56022 [04:48<00:20, 1445.96 examples/s]Running tokenizer on dataset (num_proc=128):  47%|████▋     | 26273/56022 [04:48<00:23, 1245.10 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26711/56022 [04:50<00:42, 693.95 examples/s] Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27149/56022 [04:50<00:32, 885.80 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27586/56022 [04:50<00:32, 887.91 examples/s]Running tokenizer on dataset (num_proc=128):  50%|█████     | 28023/56022 [04:51<00:25, 1077.26 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 29336/56022 [04:51<00:13, 1961.19 examples/s]Running tokenizer on dataset (num_proc=128):  53%|█████▎    | 29774/56022 [04:53<00:41, 637.44 examples/s] Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30212/56022 [04:56<01:12, 357.54 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 31525/56022 [04:57<00:39, 619.37 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 31962/56022 [04:57<00:33, 709.29 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32400/56022 [04:57<00:28, 816.91 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33712/56022 [04:58<00:15, 1415.10 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 34149/56022 [04:58<00:18, 1192.14 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35025/56022 [04:59<00:16, 1235.90 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35462/56022 [05:00<00:21, 954.35 examples/s] Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 35899/56022 [05:01<00:24, 808.51 examples/s]Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 36337/56022 [05:01<00:23, 848.50 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 36775/56022 [05:01<00:21, 876.53 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 37213/56022 [05:02<00:26, 711.31 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 37650/56022 [05:03<00:20, 891.28 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38088/56022 [05:03<00:22, 780.39 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 38526/56022 [05:04<00:22, 788.61 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38963/56022 [05:04<00:17, 966.79 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 39401/56022 [05:05<00:20, 801.46 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 39838/56022 [05:05<00:15, 1012.75 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 40712/56022 [05:05<00:10, 1428.58 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 41149/56022 [05:05<00:08, 1683.58 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41586/56022 [05:06<00:07, 1885.56 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 42024/56022 [05:06<00:12, 1144.63 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42461/56022 [05:07<00:11, 1191.82 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42899/56022 [05:07<00:11, 1165.41 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 43337/56022 [05:08<00:13, 966.41 examples/s] Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44212/56022 [05:08<00:07, 1521.47 examples/s]Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 44650/56022 [05:08<00:08, 1407.60 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 45525/56022 [05:08<00:05, 2083.81 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 45963/56022 [05:09<00:06, 1558.21 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46401/56022 [05:10<00:08, 1127.51 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▎ | 46838/56022 [05:10<00:08, 1071.57 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 47276/56022 [05:11<00:10, 841.61 examples/s] Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 47714/56022 [05:11<00:08, 938.41 examples/s]Running tokenizer on dataset (num_proc=128):  86%|████████▌ | 48151/56022 [05:13<00:13, 602.25 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49025/56022 [05:13<00:06, 1031.47 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49463/56022 [05:13<00:06, 1033.21 examples/s]Running tokenizer on dataset (num_proc=128):  89%|████████▉ | 49900/56022 [05:14<00:06, 997.88 examples/s] Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50337/56022 [05:16<00:12, 438.95 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50774/56022 [05:21<00:24, 215.56 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 51211/56022 [05:22<00:18, 264.84 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 52086/56022 [05:22<00:09, 414.35 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 52524/56022 [05:22<00:06, 502.55 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 52961/56022 [05:23<00:05, 561.87 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▌| 53398/56022 [05:24<00:05, 483.69 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 54710/56022 [05:26<00:02, 641.40 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55147/56022 [05:26<00:01, 685.93 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 55585/56022 [05:27<00:00, 694.66 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [05:28<00:00, 502.96 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [05:28<00:00, 170.31 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<00:43,  1.89 examples/s]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:14,  5.50 examples/s]Running tokenizer on dataset (num_proc=83):   7%|▋         | 6/83 [00:00<00:08,  9.35 examples/s]Running tokenizer on dataset (num_proc=83):  10%|▉         | 8/83 [00:00<00:06, 11.35 examples/s]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:01<00:05, 13.29 examples/s]Running tokenizer on dataset (num_proc=83):  16%|█▌        | 13/83 [00:01<00:04, 14.27 examples/s]Running tokenizer on dataset (num_proc=83):  18%|█▊        | 15/83 [00:01<00:04, 15.09 examples/s]Running tokenizer on dataset (num_proc=83):  20%|██        | 17/83 [00:01<00:04, 15.81 examples/s]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:01<00:03, 16.05 examples/s]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:01<00:03, 16.09 examples/s]Running tokenizer on dataset (num_proc=83):  29%|██▉       | 24/83 [00:01<00:03, 18.76 examples/s]Running tokenizer on dataset (num_proc=83):  31%|███▏      | 26/83 [00:01<00:03, 18.50 examples/s]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:02<00:03, 15.41 examples/s]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:02<00:03, 15.98 examples/s]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:02<00:03, 16.17 examples/s]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:02<00:02, 16.48 examples/s]Running tokenizer on dataset (num_proc=83):  43%|████▎     | 36/83 [00:02<00:02, 16.87 examples/s]Running tokenizer on dataset (num_proc=83):  46%|████▌     | 38/83 [00:02<00:02, 16.55 examples/s]Running tokenizer on dataset (num_proc=83):  48%|████▊     | 40/83 [00:02<00:02, 16.51 examples/s]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:02<00:02, 19.01 examples/s]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:03<00:02, 18.40 examples/s]Running tokenizer on dataset (num_proc=83):  57%|█████▋    | 47/83 [00:03<00:01, 18.24 examples/s]Running tokenizer on dataset (num_proc=83):  59%|█████▉    | 49/83 [00:03<00:01, 18.33 examples/s]Running tokenizer on dataset (num_proc=83):  61%|██████▏   | 51/83 [00:03<00:02, 15.86 examples/s]Running tokenizer on dataset (num_proc=83):  64%|██████▍   | 53/83 [00:03<00:01, 16.16 examples/s]Running tokenizer on dataset (num_proc=83):  66%|██████▋   | 55/83 [00:03<00:01, 16.45 examples/s]Running tokenizer on dataset (num_proc=83):  69%|██████▊   | 57/83 [00:03<00:01, 16.61 examples/s]Running tokenizer on dataset (num_proc=83):  71%|███████   | 59/83 [00:03<00:01, 16.19 examples/s]Running tokenizer on dataset (num_proc=83):  75%|███████▍  | 62/83 [00:04<00:01, 16.48 examples/s]Running tokenizer on dataset (num_proc=83):  78%|███████▊  | 65/83 [00:04<00:01, 16.81 examples/s]Running tokenizer on dataset (num_proc=83):  82%|████████▏ | 68/83 [00:04<00:00, 17.14 examples/s]Running tokenizer on dataset (num_proc=83):  84%|████████▍ | 70/83 [00:04<00:00, 17.29 examples/s]Running tokenizer on dataset (num_proc=83):  87%|████████▋ | 72/83 [00:04<00:00, 17.51 examples/s]Running tokenizer on dataset (num_proc=83):  89%|████████▉ | 74/83 [00:04<00:00, 17.16 examples/s]Running tokenizer on dataset (num_proc=83):  93%|█████████▎| 77/83 [00:04<00:00, 19.35 examples/s]Running tokenizer on dataset (num_proc=83):  95%|█████████▌| 79/83 [00:05<00:00, 18.39 examples/s]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [00:05<00:00, 15.39 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:05<00:00, 15.46 examples/s]
[INFO|configuration_utils.py:765] 2025-10-21 23:09:57,818 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/config.json
[INFO|configuration_utils.py:839] 2025-10-21 23:09:57,822 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
[WARNING|logging.py:328] 2025-10-21 23:09:58,277 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-21 23:09:58,279 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-21 23:09:58,284 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-21 23:09:58,292 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-10-21 23:09:58,297 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-21 23:09:58,309 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.55s/it]
[INFO|configuration_utils.py:941] 2025-10-21 23:10:25,668 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-21 23:10:25,668 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-21 23:10:25,781 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-4B-Instruct.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[WARNING|trainer.py:906] 2025-10-21 23:10:25,801 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-21 23:10:25,805 >> Using auto half precision backend
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-21 23:10:26,076 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:1335] 2025-10-21 23:10:28,850 >> skipped Embedding(2304, 1024): 2.25M params
[INFO|trainer.py:1335] 2025-10-21 23:10:28,851 >> skipped Embedding(151936, 2560): 373.1875M params
[INFO|trainer.py:1338] 2025-10-21 23:10:28,851 >> skipped: 373.1875M params
[INFO|trainer.py:2519] 2025-10-21 23:10:29,008 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-21 23:10:29,008 >>   Num examples = 56,022
[INFO|trainer.py:2521] 2025-10-21 23:10:29,008 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-21 23:10:29,008 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:2525] 2025-10-21 23:10:29,008 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-10-21 23:10:29,008 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2527] 2025-10-21 23:10:29,008 >>   Total optimization steps = 2,628
[INFO|trainer.py:2528] 2025-10-21 23:10:29,009 >>   Number of trainable parameters = 4,106,660,864
[INFO|integration_utils.py:867] 2025-10-21 23:10:29,010 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251021_231029-kutd5vx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen/Qwen3-VL-4B-Instruct_roboG_qwen3_vl_temporal_grounding_box_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/kutd5vx9
  0%|          | 0/2628 [00:00<?, ?it/s]  0%|          | 1/2628 [00:05<3:53:26,  5.33s/it]  0%|          | 2/2628 [00:06<2:11:37,  3.01s/it]  0%|          | 3/2628 [00:08<1:38:33,  2.25s/it]  0%|          | 4/2628 [00:09<1:21:45,  1.87s/it]  0%|          | 5/2628 [00:10<1:12:14,  1.65s/it]  0%|          | 6/2628 [00:11<1:06:28,  1.52s/it]  0%|          | 7/2628 [00:13<1:02:31,  1.43s/it]  0%|          | 8/2628 [00:14<1:00:31,  1.39s/it]  0%|          | 9/2628 [00:15<59:16,  1.36s/it]    0%|          | 10/2628 [00:16<58:06,  1.33s/it]                                                   0%|          | 10/2628 [00:16<58:06,  1.33s/it]  0%|          | 11/2628 [00:18<57:00,  1.31s/it]  0%|          | 12/2628 [00:19<56:46,  1.30s/it]  0%|          | 13/2628 [00:20<56:00,  1.28s/it]  1%|          | 14/2628 [00:22<56:03,  1.29s/it]  1%|          | 15/2628 [00:23<55:21,  1.27s/it]  1%|          | 16/2628 [00:24<55:17,  1.27s/it]  1%|          | 17/2628 [00:25<55:08,  1.27s/it]  1%|          | 18/2628 [00:27<55:28,  1.28s/it]  1%|          | 19/2628 [00:28<55:27,  1.28s/it]  1%|          | 20/2628 [00:29<55:17,  1.27s/it]                                                   1%|          | 20/2628 [00:29<55:17,  1.27s/it]  1%|          | 21/2628 [00:30<55:08,  1.27s/it]  1%|          | 22/2628 [00:32<55:13,  1.27s/it]  1%|          | 23/2628 [00:33<54:57,  1.27s/it]  1%|          | 24/2628 [00:34<55:04,  1.27s/it]  1%|          | 25/2628 [00:35<54:41,  1.26s/it]  1%|          | 26/2628 [00:37<55:01,  1.27s/it]  1%|          | 27/2628 [00:38<55:22,  1.28s/it]  1%|          | 28/2628 [00:39<54:53,  1.27s/it]  1%|          | 29/2628 [00:41<54:35,  1.26s/it]  1%|          | 30/2628 [00:42<55:08,  1.27s/it]                                                   1%|          | 30/2628 [00:42<55:08,  1.27s/it]  1%|          | 31/2628 [00:43<54:51,  1.27s/it]  1%|          | 32/2628 [00:44<54:27,  1.26s/it]  1%|▏         | 33/2628 [00:46<54:16,  1.25s/it]  1%|▏         | 34/2628 [00:47<54:27,  1.26s/it]  1%|▏         | 35/2628 [00:48<54:39,  1.26s/it]  1%|▏         | 36/2628 [00:49<54:21,  1.26s/it]  1%|▏         | 37/2628 [00:51<54:33,  1.26s/it]  1%|▏         | 38/2628 [00:52<54:10,  1.26s/it]  1%|▏         | 39/2628 [00:53<54:40,  1.27s/it]  2%|▏         | 40/2628 [00:54<54:16,  1.26s/it]                                                   2%|▏         | 40/2628 [00:54<54:16,  1.26s/it]  2%|▏         | 41/2628 [00:56<54:41,  1.27s/it]  2%|▏         | 42/2628 [00:57<54:20,  1.26s/it]  2%|▏         | 43/2628 [00:58<54:20,  1.26s/it]  2%|▏         | 44/2628 [00:59<54:03,  1.26s/it]  2%|▏         | 45/2628 [01:01<54:17,  1.26s/it]  2%|▏         | 46/2628 [01:02<54:08,  1.26s/it]  2%|▏         | 47/2628 [01:03<54:03,  1.26s/it]  2%|▏         | 48/2628 [01:04<53:46,  1.25s/it]  2%|▏         | 49/2628 [01:06<54:02,  1.26s/it]  2%|▏         | 50/2628 [01:07<53:48,  1.25s/it]                                                   2%|▏         | 50/2628 [01:07<53:48,  1.25s/it]  2%|▏         | 51/2628 [01:08<54:11,  1.26s/it]  2%|▏         | 52/2628 [01:10<53:50,  1.25s/it]  2%|▏         | 53/2628 [01:11<54:23,  1.27s/it]  2%|▏         | 54/2628 [01:12<54:36,  1.27s/it]  2%|▏         | 55/2628 [01:13<54:16,  1.27s/it]  2%|▏         | 56/2628 [01:15<53:52,  1.26s/it]  2%|▏         | 57/2628 [01:16<53:38,  1.25s/it]  2%|▏         | 58/2628 [01:17<53:43,  1.25s/it]  2%|▏         | 59/2628 [01:18<54:00,  1.26s/it]  2%|▏         | 60/2628 [01:20<53:41,  1.25s/it]                                                   2%|▏         | 60/2628 [01:20<53:41,  1.25s/it]  2%|▏         | 61/2628 [01:21<53:42,  1.26s/it]  2%|▏         | 62/2628 [01:22<53:56,  1.26s/it]  2%|▏         | 63/2628 [01:23<53:35,  1.25s/it]  2%|▏         | 64/2628 [01:25<53:29,  1.25s/it]  2%|▏         | 65/2628 [01:26<53:44,  1.26s/it]  3%|▎         | 66/2628 [01:27<53:37,  1.26s/it]  3%|▎         | 67/2628 [01:28<53:36,  1.26s/it]  3%|▎         | 68/2628 [01:30<53:22,  1.25s/it]  3%|▎         | 69/2628 [01:31<53:13,  1.25s/it]  3%|▎         | 70/2628 [01:32<53:34,  1.26s/it]                                                   3%|▎         | 70/2628 [01:32<53:34,  1.26s/it]  3%|▎         | 71/2628 [01:33<53:21,  1.25s/it]  3%|▎         | 72/2628 [01:35<53:10,  1.25s/it]  3%|▎         | 73/2628 [01:36<53:00,  1.24s/it]  3%|▎         | 74/2628 [01:37<53:09,  1.25s/it]  3%|▎         | 75/2628 [01:38<53:09,  1.25s/it]  3%|▎         | 76/2628 [01:40<53:28,  1.26s/it]  3%|▎         | 77/2628 [01:41<53:18,  1.25s/it]  3%|▎         | 78/2628 [01:42<53:08,  1.25s/it]  3%|▎         | 79/2628 [01:43<53:31,  1.26s/it]  3%|▎         | 80/2628 [01:45<53:43,  1.27s/it]                                                   3%|▎         | 80/2628 [01:45<53:43,  1.27s/it]  3%|▎         | 81/2628 [01:46<53:55,  1.27s/it]  3%|▎         | 82/2628 [01:47<54:04,  1.27s/it]  3%|▎         | 83/2628 [01:49<54:10,  1.28s/it]  3%|▎         | 84/2628 [01:50<54:15,  1.28s/it]  3%|▎         | 85/2628 [01:51<53:51,  1.27s/it]  3%|▎         | 86/2628 [01:52<53:28,  1.26s/it]  3%|▎         | 87/2628 [01:54<53:23,  1.26s/it]  3%|▎         | 88/2628 [01:55<53:22,  1.26s/it]  3%|▎         | 89/2628 [01:56<53:32,  1.27s/it]  3%|▎         | 90/2628 [01:57<53:13,  1.26s/it]                                                   3%|▎         | 90/2628 [01:57<53:13,  1.26s/it]  3%|▎         | 91/2628 [01:59<53:29,  1.27s/it]  4%|▎         | 92/2628 [02:00<53:06,  1.26s/it]  4%|▎         | 93/2628 [02:01<53:09,  1.26s/it]  4%|▎         | 94/2628 [02:02<53:20,  1.26s/it]  4%|▎         | 95/2628 [02:04<53:31,  1.27s/it]  4%|▎         | 96/2628 [02:05<53:05,  1.26s/it]  4%|▎         | 97/2628 [02:06<53:02,  1.26s/it]  4%|▎         | 98/2628 [02:07<53:23,  1.27s/it]  4%|▍         | 99/2628 [02:09<53:13,  1.26s/it]  4%|▍         | 100/2628 [02:10<52:57,  1.26s/it]                                                    4%|▍         | 100/2628 [02:10<52:57,  1.26s/it][INFO|trainer.py:4643] 2025-10-21 23:12:40,993 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-10-21 23:12:40,994 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-21 23:12:40,994 >>   Batch size = 8
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank1]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank1]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank1]:     output = eval_loop(
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank1]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank1]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank1]:     loss, generated_tokens, _ = super().prediction_step(
[rank1]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank1]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank1]:     self._validate_model_kwargs(model_kwargs.copy())
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank1]:     raise ValueError(
[rank1]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank3]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank3]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank3]:     output = eval_loop(
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank3]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank3]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank3]:     loss, generated_tokens, _ = super().prediction_step(
[rank3]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank3]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank3]:     self._validate_model_kwargs(model_kwargs.copy())
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank3]:     raise ValueError(
[rank3]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank2]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank2]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank2]:     output = eval_loop(
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank2]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank2]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank2]:     loss, generated_tokens, _ = super().prediction_step(
[rank2]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank2]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank2]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank2]:     self._validate_model_kwargs(model_kwargs.copy())
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank2]:     raise ValueError(
[rank2]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
    loss, generated_tokens, _ = super().prediction_step(
                                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
    self._validate_model_kwargs(model_kwargs.copy())
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
    raise ValueError(
ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank0]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank0]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank0]:     output = eval_loop(
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank0]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank0]:     loss, generated_tokens, _ = super().prediction_step(
[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank0]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank0]:     self._validate_model_kwargs(model_kwargs.copy())
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank0]:     raise ValueError(
[rank0]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
W1021 23:12:46.611000 2477074 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2477078 closing signal SIGTERM
W1021 23:12:46.616000 2477074 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2477080 closing signal SIGTERM
E1021 23:12:47.783000 2477074 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 2477079) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-21_23:12:46
  host      : hkn0916.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2477081)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-21_23:12:46
  host      : hkn0916.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2477079)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '49231', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen.yaml']' returned non-zero exit status 1.
srun: error: hkn0916: task 0: Exited with exit code 1

GpuFreq=control_disabled
[W1021 17:59:49.368838468 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 17:59:49.369005763 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 17:59:49.369006213 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 17:59:49.369012598 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:50,936 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 17:59:51,166 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-21 17:59:51,668 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-21 17:59:51,920 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-21 17:59:51,954 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,198 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 17:59:52,199 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 17:59:52,418 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-21 17:59:52,797 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-21 17:59:52,803 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-10-21 17:59:53,406 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-10-21 17:59:53,795 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-4B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1021 17:59:53.321333909 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1021 17:59:53.418163199 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1021 17:59:53.443237950 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Converting format of dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 5/56022 [00:00<33:44, 27.67 examples/s]Converting format of dataset (num_proc=128):   1%|          | 341/56022 [00:00<00:37, 1495.84 examples/s]Converting format of dataset (num_proc=128):   1%|          | 542/56022 [00:00<00:40, 1372.14 examples/s]Converting format of dataset (num_proc=128):   1%|▏         | 809/56022 [00:00<00:31, 1772.55 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1265/56022 [00:00<00:21, 2584.35 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1554/56022 [00:00<00:22, 2421.60 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1817/56022 [00:00<00:23, 2312.21 examples/s]Converting format of dataset (num_proc=128):   4%|▎         | 2063/56022 [00:01<00:25, 2142.02 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2288/56022 [00:01<00:24, 2170.21 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 2569/56022 [00:01<00:22, 2343.41 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 2812/56022 [00:01<00:23, 2272.15 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 3046/56022 [00:01<00:24, 2151.69 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3296/56022 [00:01<00:23, 2238.09 examples/s]Converting format of dataset (num_proc=128):   6%|▋         | 3566/56022 [00:01<00:22, 2364.44 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 3807/56022 [00:01<00:21, 2375.98 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 4049/56022 [00:01<00:22, 2362.26 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4300/56022 [00:01<00:21, 2401.18 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4544/56022 [00:02<00:21, 2388.49 examples/s]Converting format of dataset (num_proc=128):   9%|▊         | 4784/56022 [00:02<00:23, 2147.17 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5038/56022 [00:02<00:22, 2228.01 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5266/56022 [00:02<00:22, 2211.76 examples/s]Converting format of dataset (num_proc=128):  10%|▉         | 5517/56022 [00:02<00:22, 2287.62 examples/s]Converting format of dataset (num_proc=128):  10%|█         | 5790/56022 [00:02<00:20, 2409.60 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 6034/56022 [00:02<00:21, 2340.91 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 6290/56022 [00:02<00:20, 2397.20 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6545/56022 [00:02<00:20, 2439.03 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6791/56022 [00:03<00:21, 2269.48 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7058/56022 [00:03<00:20, 2380.14 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7369/56022 [00:03<00:18, 2587.89 examples/s]Converting format of dataset (num_proc=128):  14%|█▎        | 7651/56022 [00:03<00:18, 2644.26 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 7919/56022 [00:03<00:19, 2443.26 examples/s]Converting format of dataset (num_proc=128):  15%|█▍        | 8168/56022 [00:03<00:21, 2234.16 examples/s]Converting format of dataset (num_proc=128):  15%|█▍        | 8398/56022 [00:03<00:21, 2240.86 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8677/56022 [00:03<00:20, 2366.12 examples/s]Converting format of dataset (num_proc=128):  16%|█▌        | 8920/56022 [00:04<00:22, 2072.80 examples/s]Converting format of dataset (num_proc=128):  16%|█▋        | 9202/56022 [00:04<00:20, 2254.05 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9537/56022 [00:04<00:18, 2527.95 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9801/56022 [00:04<00:18, 2475.71 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 10056/56022 [00:04<00:19, 2367.85 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 10298/56022 [00:04<00:19, 2336.24 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10573/56022 [00:04<00:18, 2437.17 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10821/56022 [00:04<00:19, 2325.95 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 11126/56022 [00:04<00:17, 2519.12 examples/s]Converting format of dataset (num_proc=128):  20%|██        | 11383/56022 [00:04<00:18, 2438.32 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11632/56022 [00:05<00:18, 2408.71 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11903/56022 [00:05<00:17, 2491.20 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12192/56022 [00:05<00:16, 2604.67 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12456/56022 [00:05<00:17, 2468.65 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 12707/56022 [00:05<00:19, 2255.87 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 13080/56022 [00:05<00:16, 2647.60 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 13353/56022 [00:05<00:16, 2579.88 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 13617/56022 [00:05<00:16, 2563.81 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 13908/56022 [00:05<00:15, 2660.09 examples/s]Converting format of dataset (num_proc=128):  25%|██▌       | 14178/56022 [00:06<00:16, 2554.18 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14461/56022 [00:06<00:15, 2630.84 examples/s]Converting format of dataset (num_proc=128):  26%|██▋       | 14737/56022 [00:06<00:15, 2664.99 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15006/56022 [00:06<00:17, 2342.25 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15250/56022 [00:06<00:23, 1754.71 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15456/56022 [00:06<00:22, 1819.52 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15659/56022 [00:06<00:26, 1518.86 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15950/56022 [00:07<00:22, 1804.72 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16179/56022 [00:07<00:21, 1894.02 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 16529/56022 [00:07<00:17, 2293.35 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 16780/56022 [00:07<00:17, 2277.52 examples/s]Converting format of dataset (num_proc=128):  30%|███       | 17039/56022 [00:07<00:16, 2358.99 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 17287/56022 [00:07<00:16, 2380.03 examples/s]Converting format of dataset (num_proc=128):  31%|███▏      | 17550/56022 [00:07<00:15, 2447.69 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17834/56022 [00:07<00:14, 2558.01 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 18101/56022 [00:07<00:14, 2586.23 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18364/56022 [00:08<00:15, 2394.06 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18661/56022 [00:08<00:14, 2550.37 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 18922/56022 [00:08<00:15, 2370.02 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19165/56022 [00:08<00:15, 2372.79 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 19455/56022 [00:08<00:14, 2519.47 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 19711/56022 [00:08<00:15, 2301.30 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 19947/56022 [00:08<00:15, 2290.00 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 20205/56022 [00:08<00:15, 2369.36 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20454/56022 [00:08<00:14, 2403.18 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20753/56022 [00:08<00:13, 2567.79 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21014/56022 [00:09<00:14, 2480.62 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21301/56022 [00:09<00:13, 2570.53 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21562/56022 [00:09<00:14, 2445.48 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 21810/56022 [00:09<00:14, 2440.37 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 22056/56022 [00:09<00:14, 2400.95 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 22336/56022 [00:09<00:13, 2513.90 examples/s]Converting format of dataset (num_proc=128):  40%|████      | 22589/56022 [00:09<00:13, 2389.48 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 22831/56022 [00:09<00:14, 2343.97 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 23068/56022 [00:09<00:14, 2239.68 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23396/56022 [00:10<00:12, 2526.93 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23685/56022 [00:10<00:12, 2624.46 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 23959/56022 [00:10<00:12, 2655.82 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24228/56022 [00:10<00:12, 2477.26 examples/s]Converting format of dataset (num_proc=128):  44%|████▎     | 24482/56022 [00:10<00:12, 2491.75 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24756/56022 [00:10<00:12, 2561.06 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 25030/56022 [00:10<00:11, 2605.01 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 25293/56022 [00:10<00:12, 2504.27 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25551/56022 [00:10<00:12, 2435.21 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25797/56022 [00:11<00:13, 2233.68 examples/s]Converting format of dataset (num_proc=128):  46%|████▋     | 26025/56022 [00:11<00:15, 1954.18 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26244/56022 [00:11<00:14, 2012.88 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26474/56022 [00:11<00:14, 2083.48 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 26763/56022 [00:11<00:12, 2295.88 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 27018/56022 [00:11<00:12, 2364.29 examples/s]Converting format of dataset (num_proc=128):  49%|████▊     | 27294/56022 [00:11<00:11, 2475.24 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27547/56022 [00:11<00:11, 2390.38 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 27829/56022 [00:11<00:11, 2510.61 examples/s]Converting format of dataset (num_proc=128):  50%|█████     | 28132/56022 [00:12<00:10, 2658.39 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28404/56022 [00:12<00:10, 2618.56 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28668/56022 [00:12<00:10, 2523.88 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 28924/56022 [00:12<00:10, 2500.41 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 29272/56022 [00:12<00:09, 2777.71 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29552/56022 [00:12<00:10, 2429.77 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29848/56022 [00:12<00:10, 2558.49 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30121/56022 [00:12<00:09, 2596.21 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30387/56022 [00:12<00:10, 2512.53 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 30701/56022 [00:13<00:09, 2677.49 examples/s]Converting format of dataset (num_proc=128):  55%|█████▌    | 30974/56022 [00:13<00:09, 2516.66 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31314/56022 [00:13<00:08, 2757.96 examples/s]Converting format of dataset (num_proc=128):  56%|█████▋    | 31597/56022 [00:13<00:09, 2447.99 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 31886/56022 [00:13<00:09, 2562.21 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 32152/56022 [00:13<00:09, 2475.85 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32406/56022 [00:13<00:09, 2446.77 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32730/56022 [00:13<00:08, 2663.19 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 33001/56022 [00:13<00:08, 2649.99 examples/s]Converting format of dataset (num_proc=128):  60%|█████▉    | 33363/56022 [00:14<00:07, 2924.39 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 33661/56022 [00:14<00:07, 2844.11 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 33950/56022 [00:14<00:08, 2701.05 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 34241/56022 [00:14<00:07, 2758.40 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34523/56022 [00:14<00:08, 2492.47 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34780/56022 [00:14<00:08, 2495.10 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35035/56022 [00:14<00:08, 2486.21 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35287/56022 [00:14<00:08, 2441.24 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35535/56022 [00:15<00:10, 1999.34 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35749/56022 [00:15<00:10, 1870.72 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 36068/56022 [00:15<00:09, 2187.77 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 36312/56022 [00:15<00:08, 2248.77 examples/s]Converting format of dataset (num_proc=128):  65%|██████▌   | 36548/56022 [00:15<00:08, 2271.80 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 36786/56022 [00:15<00:09, 2124.82 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 37084/56022 [00:15<00:08, 2290.11 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37319/56022 [00:15<00:08, 2303.93 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37607/56022 [00:15<00:07, 2463.68 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 37859/56022 [00:16<00:07, 2299.43 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 38147/56022 [00:16<00:07, 2457.26 examples/s]Converting format of dataset (num_proc=128):  69%|██████▊   | 38398/56022 [00:16<00:07, 2376.29 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38654/56022 [00:16<00:07, 2422.43 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38900/56022 [00:16<00:07, 2254.17 examples/s]Converting format of dataset (num_proc=128):  70%|██████▉   | 39170/56022 [00:16<00:07, 2374.19 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 39412/56022 [00:16<00:07, 2181.50 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39712/56022 [00:16<00:06, 2330.95 examples/s]Converting format of dataset (num_proc=128):  71%|███████▏  | 39969/56022 [00:16<00:06, 2377.27 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40210/56022 [00:17<00:07, 2120.57 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40561/56022 [00:17<00:06, 2481.25 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40845/56022 [00:17<00:05, 2577.56 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 41133/56022 [00:17<00:05, 2660.71 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41432/56022 [00:17<00:05, 2751.35 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41712/56022 [00:17<00:05, 2745.22 examples/s]Converting format of dataset (num_proc=128):  75%|███████▍  | 41991/56022 [00:17<00:05, 2706.72 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42266/56022 [00:17<00:05, 2667.40 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42535/56022 [00:17<00:05, 2441.25 examples/s]Converting format of dataset (num_proc=128):  76%|███████▋  | 42838/56022 [00:17<00:05, 2598.37 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43103/56022 [00:18<00:04, 2606.54 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43367/56022 [00:18<00:05, 2357.85 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43609/56022 [00:18<00:05, 2311.54 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43884/56022 [00:18<00:05, 2410.01 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44129/56022 [00:18<00:04, 2403.53 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44372/56022 [00:18<00:05, 2200.87 examples/s]Converting format of dataset (num_proc=128):  80%|███████▉  | 44662/56022 [00:18<00:04, 2387.90 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 44907/56022 [00:18<00:04, 2350.28 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 45248/56022 [00:18<00:04, 2644.34 examples/s]Converting format of dataset (num_proc=128):  81%|████████▏ | 45519/56022 [00:19<00:04, 2248.01 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 45757/56022 [00:19<00:05, 2001.39 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 45999/56022 [00:19<00:04, 2102.45 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46222/56022 [00:19<00:04, 2047.16 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46600/56022 [00:19<00:03, 2496.26 examples/s]Converting format of dataset (num_proc=128):  84%|████████▎ | 46862/56022 [00:19<00:03, 2396.47 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47193/56022 [00:19<00:03, 2640.67 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 47474/56022 [00:19<00:03, 2680.97 examples/s]Converting format of dataset (num_proc=128):  85%|████████▌ | 47752/56022 [00:20<00:03, 2706.48 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48028/56022 [00:20<00:03, 2279.64 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48273/56022 [00:20<00:03, 2296.44 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48585/56022 [00:20<00:02, 2513.85 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48920/56022 [00:20<00:02, 2727.45 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49204/56022 [00:20<00:02, 2452.34 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49516/56022 [00:20<00:02, 2602.67 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 49786/56022 [00:20<00:02, 2562.51 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 50050/56022 [00:20<00:02, 2559.51 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 50367/56022 [00:21<00:02, 2728.66 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 50646/56022 [00:21<00:02, 2653.47 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 50915/56022 [00:21<00:02, 2190.28 examples/s]Converting format of dataset (num_proc=128):  91%|█████████▏| 51167/56022 [00:21<00:02, 2269.94 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51518/56022 [00:21<00:01, 2586.92 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51816/56022 [00:21<00:01, 2689.58 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 52140/56022 [00:21<00:01, 2834.79 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▎| 52483/56022 [00:21<00:01, 2999.18 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52790/56022 [00:21<00:01, 2977.14 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▍| 53093/56022 [00:22<00:00, 2975.57 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 53396/56022 [00:22<00:00, 2930.40 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 53696/56022 [00:22<00:00, 2815.58 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▋| 53985/56022 [00:22<00:00, 2749.41 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54266/56022 [00:22<00:00, 2676.28 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54540/56022 [00:22<00:00, 2083.92 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54771/56022 [00:22<00:00, 2118.61 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 55021/56022 [00:22<00:00, 2190.94 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▊| 55274/56022 [00:23<00:00, 2278.25 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55515/56022 [00:23<00:00, 2270.37 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 55754/56022 [00:23<00:00, 2168.97 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 55978/56022 [00:23<00:00, 1796.93 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56022/56022 [00:23<00:00, 2367.54 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):   1%|          | 1/83 [00:00<00:14,  5.47 examples/s]Converting format of dataset (num_proc=83):  30%|███       | 25/83 [00:00<00:00, 108.36 examples/s]Converting format of dataset (num_proc=83):  73%|███████▎  | 61/83 [00:00<00:00, 201.83 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 92.25 examples/s] 
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1021 18:00:22.431516251 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56022 [02:27<5:12:46,  2.96 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56022 [02:44<2:28:39,  6.18 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56022 [02:46<1:21:42, 11.16 examples/s]Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56022 [02:50<52:03, 17.38 examples/s]  Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56022 [02:52<34:57, 25.67 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56022 [02:53<23:10, 38.40 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 3504/56022 [02:55<12:44, 68.69 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 3942/56022 [02:56<09:50, 88.23 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4380/56022 [02:58<08:08, 105.82 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 5256/56022 [03:01<05:53, 143.43 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 5694/56022 [03:05<06:04, 137.91 examples/s]Running tokenizer on dataset (num_proc=128):  11%|█         | 6132/56022 [03:06<05:06, 162.59 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6570/56022 [03:09<05:04, 162.54 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7446/56022 [03:14<04:43, 171.39 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8322/56022 [03:14<03:11, 248.96 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 8760/56022 [03:15<02:34, 305.96 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9198/56022 [03:18<03:27, 225.86 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9636/56022 [03:19<02:45, 279.76 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 10074/56022 [03:21<02:59, 255.39 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10512/56022 [03:25<04:11, 180.82 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10950/56022 [03:26<03:17, 227.98 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 11388/56022 [03:27<02:53, 256.82 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 11826/56022 [03:28<02:41, 274.15 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12264/56022 [03:30<02:40, 272.45 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12702/56022 [03:32<02:50, 253.89 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 13139/56022 [03:33<02:19, 308.45 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13577/56022 [03:34<02:29, 284.64 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14015/56022 [03:40<04:11, 167.18 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14453/56022 [03:40<03:00, 229.91 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 14890/56022 [03:40<02:16, 300.42 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15328/56022 [03:42<02:29, 271.70 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 15766/56022 [03:43<01:53, 355.13 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16204/56022 [03:43<01:36, 410.58 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16642/56022 [03:44<01:16, 515.15 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 17080/56022 [03:45<01:23, 469.14 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17956/56022 [03:46<00:59, 643.64 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 18394/56022 [03:46<00:58, 643.76 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 18832/56022 [03:49<01:36, 386.67 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 19269/56022 [03:49<01:14, 494.18 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19707/56022 [03:52<02:07, 285.12 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20145/56022 [03:53<01:57, 305.74 examples/s]Running tokenizer on dataset (num_proc=128):  37%|███▋      | 20583/56022 [03:54<01:43, 340.92 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21021/56022 [03:55<01:33, 376.01 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21459/56022 [03:56<01:25, 406.17 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21896/56022 [03:57<01:32, 367.00 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22334/56022 [03:58<01:12, 461.80 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 22772/56022 [03:58<00:56, 591.27 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23210/56022 [03:58<00:44, 731.02 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23647/56022 [03:58<00:34, 933.81 examples/s]Running tokenizer on dataset (num_proc=128):  44%|████▍     | 24522/56022 [03:59<00:22, 1425.64 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24959/56022 [03:59<00:20, 1494.28 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▌     | 25396/56022 [04:00<00:27, 1118.82 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25834/56022 [04:00<00:27, 1091.20 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26710/56022 [04:01<00:26, 1110.82 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27148/56022 [04:01<00:26, 1071.41 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27586/56022 [04:01<00:23, 1193.42 examples/s]Running tokenizer on dataset (num_proc=128):  50%|█████     | 28023/56022 [04:03<00:45, 618.26 examples/s] Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 28897/56022 [04:03<00:27, 996.13 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 29335/56022 [04:04<00:26, 994.43 examples/s]Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30210/56022 [04:04<00:20, 1249.79 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▍    | 30648/56022 [04:06<00:34, 744.67 examples/s] Running tokenizer on dataset (num_proc=128):  55%|█████▌    | 31086/56022 [04:06<00:31, 784.92 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 31524/56022 [04:06<00:28, 848.62 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 31961/56022 [04:08<00:36, 653.68 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32398/56022 [04:08<00:30, 762.96 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 32836/56022 [04:09<00:34, 662.83 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 33274/56022 [04:09<00:33, 672.10 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33711/56022 [04:10<00:33, 664.10 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 34148/56022 [04:10<00:29, 731.88 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 34586/56022 [04:11<00:28, 741.49 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35023/56022 [04:12<00:35, 598.95 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35461/56022 [04:12<00:26, 784.52 examples/s]Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 35898/56022 [04:12<00:21, 938.80 examples/s]Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 36335/56022 [04:13<00:22, 859.84 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 36773/56022 [04:14<00:32, 596.03 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 37211/56022 [04:15<00:29, 648.04 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 37648/56022 [04:16<00:30, 608.64 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38086/56022 [04:16<00:24, 742.44 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 38523/56022 [04:18<00:39, 442.26 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38960/56022 [04:19<00:35, 482.50 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 39398/56022 [04:19<00:26, 616.73 examples/s]Running tokenizer on dataset (num_proc=128):  72%|███████▏  | 40274/56022 [04:20<00:20, 781.83 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 40711/56022 [04:20<00:19, 773.17 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 41148/56022 [04:20<00:15, 966.03 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41585/56022 [04:21<00:13, 1096.74 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 42023/56022 [04:21<00:15, 875.31 examples/s] Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42460/56022 [04:22<00:14, 930.29 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42897/56022 [04:22<00:14, 911.05 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44210/56022 [04:23<00:07, 1642.02 examples/s]Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 44648/56022 [04:23<00:08, 1315.64 examples/s]Running tokenizer on dataset (num_proc=128):  80%|████████  | 45086/56022 [04:23<00:07, 1515.39 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 45962/56022 [04:23<00:04, 2127.82 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46400/56022 [04:24<00:04, 2356.66 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▎ | 46838/56022 [04:28<00:24, 374.62 examples/s] Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 47275/56022 [04:29<00:23, 366.74 examples/s]Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 47712/56022 [04:29<00:18, 456.93 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 48588/56022 [04:32<00:17, 428.45 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49463/56022 [04:33<00:13, 489.65 examples/s]Running tokenizer on dataset (num_proc=128):  89%|████████▉ | 49900/56022 [04:34<00:11, 529.45 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50337/56022 [04:37<00:18, 303.42 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50774/56022 [04:43<00:29, 175.82 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 51212/56022 [04:43<00:22, 214.60 examples/s]Running tokenizer on dataset (num_proc=128):  92%|█████████▏| 51649/56022 [04:44<00:16, 262.01 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 52086/56022 [04:46<00:16, 240.99 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 52523/56022 [04:47<00:12, 283.93 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▌| 53398/56022 [04:52<00:11, 219.39 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 53835/56022 [04:53<00:07, 273.88 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 54710/56022 [04:53<00:03, 421.25 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55147/56022 [04:54<00:02, 418.28 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 55585/56022 [04:54<00:00, 507.63 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [04:55<00:00, 551.89 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [04:55<00:00, 189.48 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<01:02,  1.30 examples/s]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:18,  4.23 examples/s]Running tokenizer on dataset (num_proc=83):   7%|▋         | 6/83 [00:01<00:10,  7.10 examples/s]Running tokenizer on dataset (num_proc=83):  10%|▉         | 8/83 [00:01<00:09,  8.29 examples/s]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:01<00:07,  9.58 examples/s]Running tokenizer on dataset (num_proc=83):  17%|█▋        | 14/83 [00:01<00:06, 10.06 examples/s]Running tokenizer on dataset (num_proc=83):  20%|██        | 17/83 [00:01<00:05, 11.53 examples/s]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:02<00:05, 11.04 examples/s]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:02<00:06,  9.97 examples/s]Running tokenizer on dataset (num_proc=83):  29%|██▉       | 24/83 [00:02<00:04, 11.84 examples/s]Running tokenizer on dataset (num_proc=83):  31%|███▏      | 26/83 [00:02<00:04, 11.96 examples/s]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:03<00:05, 10.56 examples/s]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:03<00:04, 10.67 examples/s]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:03<00:04, 10.95 examples/s]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:03<00:04, 11.11 examples/s]Running tokenizer on dataset (num_proc=83):  45%|████▍     | 37/83 [00:03<00:03, 12.72 examples/s]Running tokenizer on dataset (num_proc=83):  47%|████▋     | 39/83 [00:03<00:03, 11.07 examples/s]Running tokenizer on dataset (num_proc=83):  49%|████▉     | 41/83 [00:04<00:03, 12.58 examples/s]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:04<00:03, 11.03 examples/s]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:04<00:03, 11.34 examples/s]Running tokenizer on dataset (num_proc=83):  58%|█████▊    | 48/83 [00:04<00:03, 11.55 examples/s]Running tokenizer on dataset (num_proc=83):  60%|██████    | 50/83 [00:04<00:02, 11.38 examples/s]Running tokenizer on dataset (num_proc=83):  63%|██████▎   | 52/83 [00:05<00:02, 12.86 examples/s]Running tokenizer on dataset (num_proc=83):  65%|██████▌   | 54/83 [00:05<00:02, 12.45 examples/s]Running tokenizer on dataset (num_proc=83):  67%|██████▋   | 56/83 [00:05<00:02, 10.82 examples/s]Running tokenizer on dataset (num_proc=83):  70%|██████▉   | 58/83 [00:05<00:02, 12.48 examples/s]Running tokenizer on dataset (num_proc=83):  72%|███████▏  | 60/83 [00:05<00:01, 12.58 examples/s]Running tokenizer on dataset (num_proc=83):  75%|███████▍  | 62/83 [00:05<00:01, 10.65 examples/s]Running tokenizer on dataset (num_proc=83):  77%|███████▋  | 64/83 [00:06<00:01, 12.07 examples/s]Running tokenizer on dataset (num_proc=83):  80%|███████▉  | 66/83 [00:06<00:01, 11.70 examples/s]Running tokenizer on dataset (num_proc=83):  82%|████████▏ | 68/83 [00:06<00:01, 11.85 examples/s]Running tokenizer on dataset (num_proc=83):  84%|████████▍ | 70/83 [00:06<00:01, 11.37 examples/s]Running tokenizer on dataset (num_proc=83):  87%|████████▋ | 72/83 [00:06<00:00, 11.50 examples/s]Running tokenizer on dataset (num_proc=83):  89%|████████▉ | 74/83 [00:06<00:00, 11.30 examples/s]Running tokenizer on dataset (num_proc=83):  92%|█████████▏| 76/83 [00:07<00:00, 11.11 examples/s]Running tokenizer on dataset (num_proc=83):  94%|█████████▍| 78/83 [00:07<00:00, 11.88 examples/s]Running tokenizer on dataset (num_proc=83):  96%|█████████▋| 80/83 [00:07<00:00, 10.36 examples/s]Running tokenizer on dataset (num_proc=83):  99%|█████████▉| 82/83 [00:07<00:00, 11.24 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:07<00:00, 10.62 examples/s]
[INFO|configuration_utils.py:765] 2025-10-21 18:05:31,839 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/config.json
[INFO|configuration_utils.py:839] 2025-10-21 18:05:31,861 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[WARNING|logging.py:328] 2025-10-21 18:05:33,000 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-21 18:05:33,013 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-21 18:05:33,031 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-21 18:05:33,049 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-10-21 18:05:33,053 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-21 18:05:33,105 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.95s/it]
[INFO|configuration_utils.py:941] 2025-10-21 18:05:47,232 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-21 18:05:47,233 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-21 18:05:47,357 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-4B-Instruct.
[WARNING|trainer.py:906] 2025-10-21 18:05:47,421 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-21 18:05:47,475 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-21 18:05:47,845 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:1335] 2025-10-21 18:05:59,055 >> skipped Embedding(2304, 1024): 2.25M params
[INFO|trainer.py:1335] 2025-10-21 18:05:59,056 >> skipped Embedding(151936, 2560): 373.1875M params
[INFO|trainer.py:1338] 2025-10-21 18:05:59,056 >> skipped: 373.1875M params
[INFO|trainer.py:2519] 2025-10-21 18:05:59,392 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-21 18:05:59,392 >>   Num examples = 56,022
[INFO|trainer.py:2521] 2025-10-21 18:05:59,392 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-21 18:05:59,392 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2525] 2025-10-21 18:05:59,392 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-10-21 18:05:59,392 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2527] 2025-10-21 18:05:59,392 >>   Total optimization steps = 2,628
[INFO|trainer.py:2528] 2025-10-21 18:05:59,394 >>   Number of trainable parameters = 4,106,660,864
[INFO|integration_utils.py:867] 2025-10-21 18:05:59,395 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251021_180559-tmyklm8w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen/Qwen3-VL-4B-Instruct_roboG_qwen3_vl_temporal_grounding_box_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/tmyklm8w
  0%|          | 0/2628 [00:00<?, ?it/s]  0%|          | 1/2628 [00:06<5:02:31,  6.91s/it]Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
    loss.backward(**kwargs)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 2 has a total capacity of 39.49 GiB of which 40.00 MiB is free. Including non-PyTorch memory, this process has 39.42 GiB memory in use. Of the allocated memory 37.91 GiB is allocated by PyTorch, and 419.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 1 has a total capacity of 39.49 GiB of which 20.00 MiB is free. Including non-PyTorch memory, this process has 39.44 GiB memory in use. Of the allocated memory 38.05 GiB is allocated by PyTorch, and 290.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank3]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 3 has a total capacity of 39.49 GiB of which 40.00 MiB is free. Including non-PyTorch memory, this process has 39.42 GiB memory in use. Of the allocated memory 37.99 GiB is allocated by PyTorch, and 335.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 59.62 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.01 GiB is allocated by PyTorch, and 293.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 59.62 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.01 GiB is allocated by PyTorch, and 293.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1021 18:06:13.666000 411564 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 411568 closing signal SIGTERM
W1021 18:06:13.672000 411564 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 411570 closing signal SIGTERM
W1021 18:06:13.673000 411564 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 411571 closing signal SIGTERM
E1021 18:06:14.088000 411564 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 411569) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-21_18:06:13
  host      : hkn0710.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 411569)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '41741', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen.yaml']' returned non-zero exit status 1.
srun: error: hkn0710: task 0: Exited with exit code 1

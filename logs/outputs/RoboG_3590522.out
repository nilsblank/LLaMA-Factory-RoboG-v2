Using config: examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen.yaml
[INFO|2025-10-22 11:48:36] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:34217
[WARNING|2025-10-22 11:48:47] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-22 11:48:50,527] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:48:50,527] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:48:50,527] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:48:50,527] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.

[INFO|2025-10-22 11:49:18] llamafactory.hparams.parser:426 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-22 11:49:18] llamafactory.hparams.parser:426 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-22 11:49:19] llamafactory.hparams.parser:426 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-22 11:49:19] llamafactory.hparams.parser:426 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-22 11:49:20] llamafactory.data.loader:143 >> Loading dataset /home/hk-project-sustainebot/bm3844/datasets/datasets/robogrounder/qwen3-vl/roboG_stagepoc_temporal_grounding_box_qwen_train.jsonl...
[INFO|2025-10-22 11:49:41] llamafactory.data.loader:143 >> Loading dataset /home/hk-project-sustainebot/bm3844/datasets/datasets/robogrounder/qwen3-vl/roboG_stagepoc_temporal_grounding_box_qwen_eval.jsonl...
training example:
input_ids:
[151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 27, 15, 13, 17, 6486, 29, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 5542, 279, 2766, 11, 1477, 700, 892, 1633, 279, 12305, 476, 3738, 16282, 291, 448, 13, 3411, 279, 2383, 323, 13934, 315, 279, 16282, 291, 1633, 304, 4718, 3561, 369, 279, 3897, 1156, 4034, 13, 151645, 198, 151644, 77091, 198, 785, 12305, 16282, 291, 448, 279, 15097, 624, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 21, 15, 11, 220, 17, 17, 20, 11, 220, 19, 19, 19, 11, 220, 19, 18, 20, 1125, 330, 1502, 788, 330, 32578, 6375, 16707, 921, 73594, 151645, 198]
inputs:
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><0.2 seconds><|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|> From the video, find out which object the robot or human interacted with. Return the label and coordinates of the interacted object in JSON format for the provided first frame.<|im_end|>
<|im_start|>assistant
The robot interacted with the clothes.
```json
[
{"bbox_2d": [260, 225, 444, 435], "label": "clothes"}
]
```<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 12305, 16282, 291, 448, 279, 15097, 624, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 21, 15, 11, 220, 17, 17, 20, 11, 220, 19, 19, 19, 11, 220, 19, 18, 20, 1125, 330, 1502, 788, 330, 32578, 6375, 16707, 921, 73594, 151645, 198]
labels:
The robot interacted with the clothes.
```json
[
{"bbox_2d": [260, 225, 444, 435], "label": "clothes"}
]
```<|im_end|>

eval example:
input_ids:
[151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 27, 15, 13, 17, 6486, 29, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 10294, 279, 2766, 323, 24523, 279, 1633, 429, 572, 16634, 553, 279, 12305, 476, 3738, 13, 9258, 279, 2383, 323, 30618, 3745, 315, 279, 16282, 291, 1633, 304, 4718, 3561, 369, 279, 2661, 1156, 4034, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><0.2 seconds><|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|> Review the video and locate the object that was engaged by the robot or human. Output the label and bounding box of the interacted object in JSON format for the given first frame.<|im_end|>
<|im_start|>assistant

label_ids:
[785, 12305, 16282, 291, 448, 279, 14961, 26334, 624, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 21, 17, 11, 220, 17, 19, 19, 11, 220, 18, 20, 20, 11, 220, 18, 15, 19, 1125, 330, 1502, 788, 330, 81914, 26334, 16707, 921, 73594, 151645, 198]
labels:
The robot interacted with the silver lid.
```json
[
{"bbox_2d": [262, 244, 355, 304], "label": "silver lid"}
]
```<|im_end|>

[INFO|2025-10-22 11:55:12] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|2025-10-22 11:55:28] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-10-22 11:55:28] llamafactory.model.loader:143 >> all params: 4,437,815,808
[WARNING|2025-10-22 11:55:28] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
gen_kwargs in prediction_step: {}
inputs keys in prediction_step: gen_kwargs in prediction_step: {}
inputs keys in prediction_step: gen_kwargs in prediction_step: {}
inputs keys in prediction_step: gen_kwargs in prediction_step: {}
inputs keys in prediction_step: KeysView({'input_ids': tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151644,    872,  ..., 151644,  77091,    198],
        ...,
        [151644,    872,    198,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198]],
       device='cuda:1'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:1'), 'pixel_values': tensor([[0.0588, 0.0903, 0.0588,  ..., 0.1216, 0.1455, 0.1533],
        [0.2002, 0.2236, 0.2471,  ..., 0.1924, 0.2002, 0.2080],
        [0.3105, 0.2871, 0.2236,  ..., 0.0903, 0.1060, 0.1216],
        ...,
        [0.2002, 0.1924, 0.2002,  ..., 0.1768, 0.1768, 0.1846],
        [0.3262, 0.3418, 0.3340,  ..., 0.3496, 0.3496, 0.3574],
        [0.2793, 0.3184, 0.3418,  ..., 0.0747, 0.1377, 0.1689]],
       device='cuda:1', dtype=torch.bfloat16), 'image_grid_thw': tensor([[ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36]], device='cuda:1'), 'pixel_values_videos': tensor([[ 0.0432,  0.0275,  0.0588,  ...,  0.3184,  0.2158,  0.1846],
        [ 0.0669,  0.0588,  0.0510,  ..., -0.0903, -0.0669, -0.0669],
        [ 0.3809,  0.3730,  0.3418,  ..., -0.0669, -0.0825, -0.0510],
        ...,
        [ 0.6172,  0.6484,  0.6406,  ...,  0.2314,  0.2158,  0.2158],
        [ 0.2637,  0.2471,  0.2559,  ...,  0.3809,  0.3652,  0.3496],
        [ 0.4668,  0.4902,  0.4590,  ...,  0.2158,  0.0981,  0.1533]],
       device='cuda:1', dtype=torch.bfloat16), 'video_grid_thw': tensor([[ 1,  6, 10],
        [ 1,  6, 10],
        [ 2,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 2,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10]], device='cuda:1'), 'video_metadata': [VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 4, 'fps': 2.0, 'width': None, 'height': None, 'duration': 4, 'video_backend': None, 'frames_indices': array([0, 1, 2, 3])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 4, 'fps': 2.0, 'width': None, 'height': None, 'duration': 4, 'video_backend': None, 'frames_indices': array([0, 1, 2, 3])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None)]})
KeysView({'input_ids': tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151644,    872,    198,  ..., 151644,  77091,    198],
        ...,
        [151643, 151644,    872,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198]],
       device='cuda:3'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [0, 1, 1,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:3'), 'pixel_values': tensor([[0.0275, 0.0669, 0.0432,  ..., 0.0510, 0.0432, 0.0669],
        [0.1846, 0.2002, 0.2158,  ..., 0.0981, 0.1138, 0.1138],
        [0.2559, 0.2236, 0.1768,  ..., 0.0354, 0.0275, 0.0197],
        ...,
        [0.1689, 0.1846, 0.1924,  ..., 0.2002, 0.2080, 0.2080],
        [0.4277, 0.4199, 0.4199,  ..., 0.3027, 0.3027, 0.2949],
        [0.2559, 0.2471, 0.2314,  ..., 0.2002, 0.2393, 0.2637]],
       device='cuda:3', dtype=torch.bfloat16), 'image_grid_thw': tensor([[ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36]], device='cuda:3'), 'pixel_values_videos': tensor([[ 0.0039, -0.0039,  0.0432,  ...,  0.2393,  0.1924,  0.1924],
        [ 0.0197,  0.0039, -0.0354,  ..., -0.1533, -0.1299, -0.1455],
        [ 0.3730,  0.3574,  0.3262,  ..., -0.1138, -0.1455, -0.0903],
        ...,
        [ 0.6250,  0.6094,  0.5625,  ...,  0.2236,  0.2314,  0.2314],
        [ 0.2559,  0.2559,  0.2393,  ...,  0.2949,  0.2871,  0.2637],
        [ 0.4434,  0.3887,  0.3105,  ...,  0.0747,  0.1611,  0.2236]],
       device='cuda:3', dtype=torch.bfloat16), 'video_grid_thw': tensor([[ 1,  6, 10],
        [ 1,  6, 10],
        [ 2,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 2,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10]], device='cuda:3'), 'video_metadata': [VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 4, 'fps': 2.0, 'width': None, 'height': None, 'duration': 4, 'video_backend': None, 'frames_indices': array([0, 1, 2, 3])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 4, 'fps': 2.0, 'width': None, 'height': None, 'duration': 4, 'video_backend': None, 'frames_indices': array([0, 1, 2, 3])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None)]})
KeysView({'input_ids': tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        ...,
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198],
        [151643, 151643, 151643,  ..., 151644,  77091,    198]],
       device='cuda:2'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        ...,
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:2'), 'pixel_values': tensor([[-0.5781, -0.6094, -0.6484,  ...,  0.1216,  0.1299,  0.1216],
        [ 0.2002,  0.2393,  0.2559,  ...,  0.2080,  0.1924,  0.1846],
        [-0.5781, -0.6016, -0.6328,  ...,  0.0588,  0.0588,  0.0669],
        ...,
        [ 0.2314,  0.2236,  0.2236,  ...,  0.1846,  0.1846,  0.1924],
        [ 0.4902,  0.4746,  0.4512,  ...,  0.2158,  0.2158,  0.2158],
        [ 0.2236,  0.2314,  0.2314,  ...,  0.2559,  0.2559,  0.2559]],
       device='cuda:2', dtype=torch.bfloat16), 'image_grid_thw': tensor([[ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36]], device='cuda:2'), 'pixel_values_videos': tensor([[-0.6484, -0.6797, -0.6875,  ...,  0.3262,  0.2158,  0.2158],
        [ 0.0432,  0.0275,  0.0039,  ..., -0.0981, -0.0747, -0.0981],
        [ 0.4121,  0.4043,  0.3887,  ..., -0.0432, -0.0825, -0.0510],
        ...,
        [ 0.5391,  0.4746,  0.3027,  ...,  0.2158,  0.2393,  0.2314],
        [ 0.2793,  0.2715,  0.2559,  ...,  0.2949,  0.2793,  0.2793],
        [ 0.2236,  0.1924,  0.1768,  ...,  0.1768,  0.2236,  0.2471]],
       device='cuda:2', dtype=torch.bfloat16), 'video_grid_thw': tensor([[ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 2,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10]], device='cuda:2'), 'video_metadata': [VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 4, 'fps': 2.0, 'width': None, 'height': None, 'duration': 4, 'video_backend': None, 'frames_indices': array([0, 1, 2, 3])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None)]})
KeysView({'input_ids': tensor([[151643, 151644,    872,  ..., 151644,  77091,    198],
        [151643, 151643, 151644,  ..., 151644,  77091,    198],
        [151644,    872,    198,  ..., 151644,  77091,    198],
        ...,
        [151643, 151643, 151644,  ..., 151644,  77091,    198],
        [151644,    872,    198,  ..., 151644,  77091,    198],
        [151643, 151643, 151644,  ..., 151644,  77091,    198]],
       device='cuda:0'), 'attention_mask': tensor([[0, 1, 1,  ..., 1, 1, 1],
        [0, 0, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [0, 0, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [0, 0, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.1377,  0.1611,  0.2080,  ...,  0.1924,  0.1768,  0.1689],
        [ 0.0510,  0.0354,  0.0039,  ...,  0.2393,  0.2393,  0.2559],
        [ 0.1138,  0.1216,  0.1216,  ..., -0.1611, -0.2471, -0.2949],
        ...,
        [ 0.1689,  0.1689,  0.1689,  ...,  0.1533,  0.1533,  0.1533],
        [ 0.2715,  0.2637,  0.2715,  ...,  0.2559,  0.2637,  0.2793],
        [ 0.2793,  0.2871,  0.2871,  ...,  0.0588,  0.1299,  0.1533]],
       device='cuda:0', dtype=torch.bfloat16), 'image_grid_thw': tensor([[ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36],
        [ 1, 28, 36]], device='cuda:0'), 'pixel_values_videos': tensor([[ 0.1846,  0.1689,  0.0669,  ...,  0.0118, -0.0510, -0.0981],
        [-0.0118, -0.0354, -0.0275,  ..., -0.1138, -0.1377, -0.1060],
        [ 0.1455,  0.0432,  0.0197,  ..., -0.1846, -0.1924, -0.1377],
        ...,
        [ 0.5469,  0.5703,  0.5781,  ...,  0.1924,  0.1768,  0.1768],
        [ 0.2236,  0.2080,  0.2002,  ...,  0.3105,  0.2949,  0.2949],
        [ 0.3965,  0.4199,  0.3965,  ...,  0.1689,  0.0432,  0.1060]],
       device='cuda:0', dtype=torch.bfloat16), 'video_grid_thw': tensor([[ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10],
        [ 1,  6, 10]], device='cuda:0'), 'video_metadata': [VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None), VideoMetadata(total_num_frames={'total_num_frames': 2, 'fps': 2.0, 'width': None, 'height': None, 'duration': 2, 'video_backend': None, 'frames_indices': array([0, 1])}, fps=None, width=None, height=None, duration=None, video_backend=None, frames_indices=None)]})

============================= JOB FEEDBACK =============================

Job ID: 3590522
Cluster: hk
User/Group: bm3844/hk-project-sustainebot
Account: hk-project-p0024638
State: FAILED (exit code 1)
Partition: accelerated
Nodes: 1
Cores per node: 64
Nodelist: hkn0814
CPU Utilized: 05:03:47
CPU Efficiency: 56.85% of 08:54:24 core-walltime
Job Wall-clock time: 00:08:21
Starttime: Wed Oct 22 11:47:15 2025
Endtime: Wed Oct 22 11:55:36 2025
Memory Utilized: 42.58 GB
Memory Efficiency: 8.76% of 485.84 GB (485.84 GB/node)
Energy Consumed: 381221 Joule / 105.894722222222 Watthours
Average node power draw: 760.920159680639 Watt

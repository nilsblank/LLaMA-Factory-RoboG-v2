GpuFreq=control_disabled
[W1021 22:46:53.066402603 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:46:53.068143821 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:46:53.068736593 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:46:53.073129054 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,188 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,188 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,188 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,189 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,189 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,189 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,189 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-10-21 22:46:54,522 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:381] 2025-10-21 22:46:54,524 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/preprocessor_config.json
[INFO|image_processing_base.py:381] 2025-10-21 22:46:54,532 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-21 22:46:54,556 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-10-21 22:46:54,557 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-10-21 22:46:54,923 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:724] 2025-10-21 22:46:54,924 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-21 22:46:54,925 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1114] 2025-10-21 22:46:54,925 >> loading configuration file None
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1021 22:46:55.090764484 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W1021 22:46:55.090766160 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1021 22:46:55.097249473 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-10-21 22:46:55,349 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

Converting format of dataset (num_proc=128):   0%|          | 0/56026 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 1/56026 [00:00<1:35:08,  9.81 examples/s]Converting format of dataset (num_proc=128):   0%|          | 122/56026 [00:00<01:19, 707.45 examples/s]Converting format of dataset (num_proc=128):   0%|          | 242/56026 [00:00<01:00, 929.18 examples/s]Converting format of dataset (num_proc=128):   1%|          | 351/56026 [00:00<00:56, 985.71 examples/s]Converting format of dataset (num_proc=128):   1%|          | 479/56026 [00:00<00:51, 1079.83 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1010/56026 [00:00<00:22, 2499.68 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1263/56026 [00:00<00:25, 2112.62 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1485/56026 [00:00<00:25, 2123.15 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1714/56026 [00:00<00:25, 2167.70 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1938/56026 [00:01<00:26, 2042.08 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2151/56026 [00:01<00:26, 2022.05 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2358/56026 [00:01<00:27, 1970.76 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 2614/56026 [00:01<00:25, 2133.56 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 2832/56026 [00:01<00:25, 2089.36 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 3043/56026 [00:01<00:26, 2004.11 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3377/56026 [00:01<00:22, 2373.59 examples/s]Converting format of dataset (num_proc=128):   6%|▋         | 3619/56026 [00:01<00:24, 2153.06 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 3842/56026 [00:02<00:26, 1941.23 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 4068/56026 [00:02<00:25, 2021.30 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4279/56026 [00:02<00:27, 1906.05 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4573/56026 [00:02<00:23, 2176.41 examples/s]Converting format of dataset (num_proc=128):   9%|▊         | 4874/56026 [00:02<00:21, 2388.00 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5127/56026 [00:02<00:21, 2407.22 examples/s]Converting format of dataset (num_proc=128):  10%|▉         | 5374/56026 [00:02<00:21, 2311.87 examples/s]Converting format of dataset (num_proc=128):  10%|█         | 5610/56026 [00:02<00:22, 2272.70 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 5920/56026 [00:02<00:20, 2494.34 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 6174/56026 [00:02<00:20, 2426.66 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6454/56026 [00:03<00:19, 2524.09 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6709/56026 [00:03<00:19, 2524.29 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6964/56026 [00:03<00:24, 2032.71 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7184/56026 [00:03<00:24, 2002.77 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7463/56026 [00:03<00:22, 2200.48 examples/s]Converting format of dataset (num_proc=128):  14%|█▎        | 7695/56026 [00:03<00:22, 2184.65 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 7923/56026 [00:03<00:22, 2123.48 examples/s]Converting format of dataset (num_proc=128):  15%|█▍        | 8178/56026 [00:03<00:21, 2236.67 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8407/56026 [00:04<00:21, 2203.91 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8650/56026 [00:04<00:21, 2226.78 examples/s]Converting format of dataset (num_proc=128):  16%|█▌        | 8888/56026 [00:04<00:20, 2264.56 examples/s]Converting format of dataset (num_proc=128):  16%|█▋        | 9198/56026 [00:04<00:18, 2484.62 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9518/56026 [00:04<00:17, 2663.57 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 9810/56026 [00:04<00:16, 2735.57 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 10085/56026 [00:04<00:18, 2461.69 examples/s]Converting format of dataset (num_proc=128):  18%|█▊        | 10337/56026 [00:04<00:18, 2468.01 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10588/56026 [00:04<00:18, 2453.74 examples/s]Converting format of dataset (num_proc=128):  19%|█▉        | 10838/56026 [00:05<00:19, 2302.22 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 11075/56026 [00:05<00:20, 2151.58 examples/s]Converting format of dataset (num_proc=128):  20%|██        | 11315/56026 [00:05<00:20, 2214.13 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11541/56026 [00:05<00:21, 2079.76 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11800/56026 [00:05<00:20, 2208.31 examples/s]Converting format of dataset (num_proc=128):  21%|██▏       | 12025/56026 [00:05<00:20, 2139.36 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12259/56026 [00:05<00:19, 2194.45 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12504/56026 [00:05<00:19, 2263.92 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 12781/56026 [00:05<00:17, 2409.16 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 13025/56026 [00:05<00:18, 2325.59 examples/s]Converting format of dataset (num_proc=128):  24%|██▎       | 13260/56026 [00:06<00:19, 2218.52 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 13487/56026 [00:06<00:19, 2213.36 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 13771/56026 [00:06<00:17, 2389.42 examples/s]Converting format of dataset (num_proc=128):  25%|██▌       | 14013/56026 [00:06<00:17, 2351.70 examples/s]Converting format of dataset (num_proc=128):  25%|██▌       | 14258/56026 [00:06<00:17, 2352.43 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14496/56026 [00:06<00:18, 2298.57 examples/s]Converting format of dataset (num_proc=128):  26%|██▋       | 14756/56026 [00:06<00:17, 2380.61 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15007/56026 [00:06<00:16, 2417.35 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 15316/56026 [00:06<00:15, 2604.91 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15658/56026 [00:07<00:14, 2841.53 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15944/56026 [00:07<00:15, 2629.01 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16213/56026 [00:07<00:16, 2434.29 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16523/56026 [00:07<00:15, 2607.44 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 16791/56026 [00:07<00:15, 2575.78 examples/s]Converting format of dataset (num_proc=128):  30%|███       | 17053/56026 [00:07<00:16, 2298.43 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 17291/56026 [00:07<00:17, 2266.67 examples/s]Converting format of dataset (num_proc=128):  31%|███▏      | 17524/56026 [00:07<00:17, 2150.80 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17744/56026 [00:08<00:18, 2023.34 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17962/56026 [00:08<00:18, 2064.04 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18223/56026 [00:08<00:17, 2211.95 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18465/56026 [00:08<00:16, 2232.18 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18691/56026 [00:08<00:18, 2022.30 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19010/56026 [00:08<00:15, 2332.24 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19251/56026 [00:08<00:16, 2209.56 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 19479/56026 [00:08<00:16, 2222.51 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 19715/56026 [00:08<00:16, 2156.60 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 19954/56026 [00:08<00:16, 2218.03 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 20191/56026 [00:09<00:15, 2260.00 examples/s]Converting format of dataset (num_proc=128):  36%|███▋      | 20420/56026 [00:09<00:16, 2189.02 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20708/56026 [00:09<00:14, 2362.30 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20978/56026 [00:09<00:14, 2404.61 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21220/56026 [00:09<00:15, 2292.32 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21452/56026 [00:09<00:15, 2215.27 examples/s]Converting format of dataset (num_proc=128):  39%|███▊      | 21675/56026 [00:09<00:16, 2134.65 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 21960/56026 [00:09<00:14, 2331.86 examples/s]Converting format of dataset (num_proc=128):  40%|███▉      | 22269/56026 [00:09<00:13, 2543.11 examples/s]Converting format of dataset (num_proc=128):  40%|████      | 22527/56026 [00:10<00:13, 2425.35 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 22844/56026 [00:10<00:12, 2593.03 examples/s]Converting format of dataset (num_proc=128):  41%|████▏     | 23132/56026 [00:10<00:12, 2673.65 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23403/56026 [00:10<00:12, 2602.28 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23665/56026 [00:10<00:12, 2600.62 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 23927/56026 [00:10<00:12, 2542.74 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24199/56026 [00:10<00:12, 2591.62 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24515/56026 [00:10<00:11, 2753.65 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24792/56026 [00:10<00:12, 2504.33 examples/s]Converting format of dataset (num_proc=128):  45%|████▍     | 25048/56026 [00:11<00:13, 2353.36 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 25302/56026 [00:11<00:12, 2400.54 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25546/56026 [00:11<00:13, 2233.04 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25775/56026 [00:11<00:13, 2199.00 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26077/56026 [00:11<00:12, 2419.74 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26324/56026 [00:11<00:13, 2275.54 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26558/56026 [00:11<00:12, 2268.88 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 26797/56026 [00:11<00:12, 2288.84 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 27029/56026 [00:11<00:14, 2033.44 examples/s]Converting format of dataset (num_proc=128):  49%|████▊     | 27240/56026 [00:12<00:14, 2018.44 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27487/56026 [00:12<00:13, 2134.46 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27732/56026 [00:12<00:12, 2220.20 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 28003/56026 [00:12<00:11, 2346.97 examples/s]Converting format of dataset (num_proc=128):  50%|█████     | 28241/56026 [00:12<00:12, 2248.91 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28539/56026 [00:12<00:11, 2450.51 examples/s]Converting format of dataset (num_proc=128):  51%|█████▏    | 28819/56026 [00:12<00:10, 2532.58 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 29149/56026 [00:12<00:09, 2750.95 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29427/56026 [00:12<00:09, 2686.39 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29698/56026 [00:13<00:11, 2258.23 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29937/56026 [00:13<00:11, 2234.68 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30170/56026 [00:13<00:12, 2046.13 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30384/56026 [00:13<00:13, 1949.92 examples/s]Converting format of dataset (num_proc=128):  55%|█████▍    | 30643/56026 [00:13<00:12, 2107.71 examples/s]Converting format of dataset (num_proc=128):  55%|█████▌    | 30957/56026 [00:13<00:10, 2378.98 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31206/56026 [00:13<00:11, 2241.01 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31476/56026 [00:13<00:10, 2356.99 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 31721/56026 [00:14<00:10, 2240.35 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 31951/56026 [00:14<00:10, 2214.21 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32266/56026 [00:14<00:09, 2466.30 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32519/56026 [00:14<00:09, 2416.55 examples/s]Converting format of dataset (num_proc=128):  59%|█████▊    | 32830/56026 [00:14<00:08, 2609.13 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 33096/56026 [00:14<00:08, 2595.79 examples/s]Converting format of dataset (num_proc=128):  60%|█████▉    | 33358/56026 [00:14<00:09, 2440.06 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 33641/56026 [00:14<00:08, 2547.76 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 33953/56026 [00:14<00:08, 2708.52 examples/s]Converting format of dataset (num_proc=128):  61%|██████▏   | 34347/56026 [00:14<00:07, 3061.56 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34658/56026 [00:15<00:07, 2956.34 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34957/56026 [00:15<00:07, 2673.85 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35232/56026 [00:15<00:07, 2622.41 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35527/56026 [00:15<00:07, 2709.69 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35802/56026 [00:15<00:08, 2499.07 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 36064/56026 [00:15<00:07, 2529.49 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 36321/56026 [00:15<00:08, 2262.62 examples/s]Converting format of dataset (num_proc=128):  65%|██████▌   | 36652/56026 [00:15<00:07, 2533.18 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 36916/56026 [00:16<00:08, 2384.58 examples/s]Converting format of dataset (num_proc=128):  66%|██████▋   | 37181/56026 [00:16<00:07, 2419.04 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37443/56026 [00:16<00:07, 2448.86 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37693/56026 [00:16<00:08, 2277.80 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 37926/56026 [00:16<00:07, 2288.30 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 38179/56026 [00:16<00:07, 2352.13 examples/s]Converting format of dataset (num_proc=128):  69%|██████▊   | 38434/56026 [00:16<00:07, 2398.80 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38691/56026 [00:16<00:07, 2397.56 examples/s]Converting format of dataset (num_proc=128):  70%|██████▉   | 38945/56026 [00:16<00:07, 2438.15 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 39255/56026 [00:16<00:06, 2622.02 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39519/56026 [00:17<00:07, 2316.37 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39780/56026 [00:17<00:06, 2394.85 examples/s]Converting format of dataset (num_proc=128):  71%|███████▏  | 40031/56026 [00:17<00:06, 2422.98 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40352/56026 [00:17<00:05, 2631.53 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40620/56026 [00:17<00:05, 2595.95 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40899/56026 [00:17<00:05, 2650.80 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 41168/56026 [00:17<00:05, 2546.26 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41427/56026 [00:17<00:06, 2390.67 examples/s]Converting format of dataset (num_proc=128):  75%|███████▍  | 41754/56026 [00:17<00:05, 2628.53 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42023/56026 [00:18<00:05, 2596.84 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42287/56026 [00:18<00:05, 2603.77 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42550/56026 [00:18<00:05, 2536.57 examples/s]Converting format of dataset (num_proc=128):  76%|███████▋  | 42807/56026 [00:18<00:05, 2513.74 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43072/56026 [00:18<00:05, 2549.92 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43330/56026 [00:18<00:05, 2463.15 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43579/56026 [00:18<00:05, 2245.98 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43810/56026 [00:18<00:05, 2163.50 examples/s]Converting format of dataset (num_proc=128):  79%|███████▊  | 44054/56026 [00:18<00:05, 2234.07 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44281/56026 [00:19<00:05, 1959.66 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44528/56026 [00:19<00:05, 2090.83 examples/s]Converting format of dataset (num_proc=128):  80%|███████▉  | 44798/56026 [00:19<00:04, 2251.39 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 45056/56026 [00:19<00:04, 2316.44 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 45382/56026 [00:19<00:04, 2579.82 examples/s]Converting format of dataset (num_proc=128):  81%|████████▏ | 45648/56026 [00:19<00:04, 2557.62 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 45911/56026 [00:19<00:04, 2460.26 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 46213/56026 [00:19<00:03, 2595.52 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46486/56026 [00:19<00:03, 2632.46 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46752/56026 [00:20<00:03, 2632.32 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47017/56026 [00:20<00:03, 2538.56 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47334/56026 [00:20<00:03, 2713.18 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 47608/56026 [00:20<00:03, 2536.54 examples/s]Converting format of dataset (num_proc=128):  85%|████████▌ | 47893/56026 [00:20<00:03, 2621.08 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48176/56026 [00:20<00:02, 2676.75 examples/s]Converting format of dataset (num_proc=128):  86%|████████▋ | 48448/56026 [00:20<00:02, 2670.59 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48717/56026 [00:20<00:02, 2605.31 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48981/56026 [00:20<00:03, 2285.37 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49232/56026 [00:21<00:02, 2343.21 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49525/56026 [00:21<00:02, 2501.15 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 49784/56026 [00:21<00:02, 2487.51 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 50101/56026 [00:21<00:02, 2661.46 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 50375/56026 [00:21<00:02, 2658.37 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 50644/56026 [00:21<00:02, 2524.23 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 50900/56026 [00:21<00:02, 2458.06 examples/s]Converting format of dataset (num_proc=128):  91%|█████████▏| 51170/56026 [00:21<00:01, 2520.33 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51431/56026 [00:21<00:01, 2534.26 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51693/56026 [00:22<00:01, 2551.12 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 51951/56026 [00:22<00:01, 2516.29 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 52250/56026 [00:22<00:01, 2635.67 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52527/56026 [00:22<00:01, 2674.03 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52796/56026 [00:22<00:01, 2620.71 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▍| 53073/56026 [00:22<00:01, 2621.34 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 53369/56026 [00:22<00:00, 2691.35 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 53640/56026 [00:22<00:00, 2626.17 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 53917/56026 [00:22<00:00, 2659.78 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54184/56026 [00:22<00:00, 2421.71 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54435/56026 [00:23<00:00, 2379.10 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54678/56026 [00:23<00:00, 2388.00 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54937/56026 [00:23<00:00, 2438.57 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▊| 55204/56026 [00:23<00:00, 2463.31 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55474/56026 [00:23<00:00, 2477.57 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55745/56026 [00:23<00:00, 2543.29 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 56007/56026 [00:23<00:00, 1737.90 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56026/56026 [00:24<00:00, 2331.36 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):   1%|          | 1/83 [00:00<00:10,  8.05 examples/s]Converting format of dataset (num_proc=83):  25%|██▌       | 21/83 [00:00<00:00, 71.32 examples/s]Converting format of dataset (num_proc=83):  45%|████▍     | 37/83 [00:00<00:00, 98.31 examples/s]Converting format of dataset (num_proc=83):  76%|███████▌  | 63/83 [00:00<00:00, 148.67 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 104.33 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1021 22:47:24.140207707 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56026 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56026 [01:33<3:18:41,  4.66 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56026 [01:34<1:21:26, 11.29 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56026 [01:37<47:08, 19.35 examples/s] Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56026 [01:38<28:45, 31.45 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56026 [01:38<18:53, 47.51 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56026 [01:40<13:12, 67.38 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 3066/56026 [01:40<09:02, 97.63 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 3504/56026 [01:41<06:45, 129.65 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 3942/56026 [01:42<05:17, 164.12 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4380/56026 [01:42<03:45, 229.26 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 4818/56026 [01:43<02:49, 301.58 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6570/56026 [01:43<01:07, 727.33 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7008/56026 [01:44<01:12, 676.68 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7446/56026 [01:45<01:11, 679.59 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 7884/56026 [01:45<01:05, 738.72 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8322/56026 [01:46<00:58, 810.56 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 8760/56026 [01:46<00:51, 923.99 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9198/56026 [01:46<00:48, 957.85 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9636/56026 [01:46<00:37, 1225.28 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10512/56026 [01:47<00:24, 1869.86 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10950/56026 [01:47<00:39, 1141.62 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 11388/56026 [01:50<01:44, 428.93 examples/s] Running tokenizer on dataset (num_proc=128):  21%|██        | 11826/56026 [01:52<01:56, 380.77 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12264/56026 [01:52<01:34, 463.09 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12702/56026 [01:52<01:16, 563.78 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13578/56026 [01:53<00:56, 756.60 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14016/56026 [01:54<00:50, 831.09 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14454/56026 [01:55<01:08, 608.64 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 14892/56026 [01:55<00:59, 686.25 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15330/56026 [01:56<01:00, 672.12 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16206/56026 [01:57<00:49, 797.01 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16644/56026 [01:58<00:54, 718.24 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 17520/56026 [01:58<00:38, 991.26 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17958/56026 [01:58<00:38, 986.30 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 18396/56026 [01:59<00:42, 878.45 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 19272/56026 [02:01<01:01, 601.32 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19710/56026 [02:03<01:26, 419.30 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20148/56026 [02:04<01:09, 513.50 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21024/56026 [02:04<00:44, 789.76 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21900/56026 [02:05<00:43, 776.35 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22338/56026 [02:05<00:38, 865.82 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23214/56026 [02:05<00:25, 1285.68 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23652/56026 [02:06<00:27, 1170.64 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 24090/56026 [02:06<00:23, 1338.52 examples/s]Running tokenizer on dataset (num_proc=128):  44%|████▍     | 24528/56026 [02:07<00:29, 1070.55 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24966/56026 [02:08<00:43, 713.90 examples/s] Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25842/56026 [02:09<00:35, 859.00 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26718/56026 [02:09<00:27, 1051.74 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27156/56026 [02:09<00:24, 1162.35 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27594/56026 [02:10<00:31, 890.34 examples/s] Running tokenizer on dataset (num_proc=128):  51%|█████     | 28470/56026 [02:11<00:22, 1231.63 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 28908/56026 [02:11<00:19, 1410.23 examples/s]Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30222/56026 [02:11<00:11, 2191.25 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▌    | 31098/56026 [02:11<00:08, 2868.08 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 31974/56026 [02:12<00:13, 1735.55 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32412/56026 [02:12<00:13, 1778.91 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 32850/56026 [02:12<00:12, 1898.09 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 33288/56026 [02:13<00:14, 1615.03 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33726/56026 [02:13<00:14, 1536.37 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 34163/56026 [02:13<00:11, 1841.05 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 34600/56026 [02:13<00:10, 1993.02 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35038/56026 [02:14<00:09, 2169.57 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35476/56026 [02:14<00:10, 2040.10 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 37227/56026 [02:14<00:07, 2652.76 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38103/56026 [02:14<00:05, 3318.61 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 38541/56026 [02:15<00:05, 2986.66 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38979/56026 [02:15<00:05, 3055.47 examples/s]Running tokenizer on dataset (num_proc=128):  70%|███████   | 39417/56026 [02:15<00:05, 3033.58 examples/s]Running tokenizer on dataset (num_proc=128):  72%|███████▏  | 40292/56026 [02:16<00:09, 1672.54 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 40729/56026 [02:16<00:08, 1838.28 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 41167/56026 [02:16<00:07, 2029.82 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41604/56026 [02:16<00:07, 2055.20 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 42042/56026 [02:17<00:10, 1316.54 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42479/56026 [02:17<00:10, 1264.01 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42916/56026 [02:18<00:08, 1533.81 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44227/56026 [02:18<00:06, 1790.17 examples/s]Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 44664/56026 [02:18<00:05, 1991.73 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 45538/56026 [02:19<00:05, 1995.02 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46412/56026 [02:19<00:05, 1906.90 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▎ | 46849/56026 [02:19<00:04, 2108.32 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 47286/56026 [02:20<00:04, 1864.49 examples/s]Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 47723/56026 [02:20<00:03, 2158.62 examples/s]Running tokenizer on dataset (num_proc=128):  86%|████████▌ | 48160/56026 [02:20<00:03, 2359.63 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 48597/56026 [02:20<00:02, 2684.84 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49034/56026 [02:20<00:02, 2551.79 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49471/56026 [02:20<00:02, 2499.56 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50345/56026 [02:21<00:01, 3222.07 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50782/56026 [02:21<00:01, 3438.69 examples/s]Running tokenizer on dataset (num_proc=128):  92%|█████████▏| 51656/56026 [02:21<00:01, 4272.59 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 52530/56026 [02:21<00:00, 4023.01 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 53841/56026 [02:21<00:00, 3931.40 examples/s]Running tokenizer on dataset (num_proc=128):  97%|█████████▋| 54278/56026 [02:22<00:00, 3228.62 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55152/56026 [02:22<00:00, 3812.81 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56026/56026 [02:22<00:00, 3223.68 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56026/56026 [02:22<00:00, 392.53 examples/s] 
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<00:47,  1.71 examples/s]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:17,  4.52 examples/s]Running tokenizer on dataset (num_proc=83):   6%|▌         | 5/83 [00:00<00:10,  7.30 examples/s]Running tokenizer on dataset (num_proc=83):   8%|▊         | 7/83 [00:01<00:08,  8.56 examples/s]Running tokenizer on dataset (num_proc=83):  11%|█         | 9/83 [00:01<00:08,  8.29 examples/s]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:01<00:06, 10.43 examples/s]Running tokenizer on dataset (num_proc=83):  16%|█▌        | 13/83 [00:01<00:07,  9.34 examples/s]Running tokenizer on dataset (num_proc=83):  18%|█▊        | 15/83 [00:01<00:06, 11.14 examples/s]Running tokenizer on dataset (num_proc=83):  20%|██        | 17/83 [00:01<00:05, 11.17 examples/s]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:02<00:05, 10.96 examples/s]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:02<00:05, 10.65 examples/s]Running tokenizer on dataset (num_proc=83):  28%|██▊       | 23/83 [00:02<00:06,  9.77 examples/s]Running tokenizer on dataset (num_proc=83):  30%|███       | 25/83 [00:02<00:05,  9.79 examples/s]Running tokenizer on dataset (num_proc=83):  33%|███▎      | 27/83 [00:02<00:05, 10.22 examples/s]Running tokenizer on dataset (num_proc=83):  35%|███▍      | 29/83 [00:03<00:05, 10.18 examples/s]Running tokenizer on dataset (num_proc=83):  37%|███▋      | 31/83 [00:03<00:05,  8.84 examples/s]Running tokenizer on dataset (num_proc=83):  40%|███▉      | 33/83 [00:03<00:04, 10.27 examples/s]Running tokenizer on dataset (num_proc=83):  42%|████▏     | 35/83 [00:03<00:04, 10.76 examples/s]Running tokenizer on dataset (num_proc=83):  45%|████▍     | 37/83 [00:04<00:05,  8.91 examples/s]Running tokenizer on dataset (num_proc=83):  47%|████▋     | 39/83 [00:04<00:04, 10.63 examples/s]Running tokenizer on dataset (num_proc=83):  49%|████▉     | 41/83 [00:04<00:03, 10.68 examples/s]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:04<00:03, 10.39 examples/s]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:04<00:03, 10.70 examples/s]Running tokenizer on dataset (num_proc=83):  57%|█████▋    | 47/83 [00:04<00:03, 10.59 examples/s]Running tokenizer on dataset (num_proc=83):  59%|█████▉    | 49/83 [00:05<00:03, 10.44 examples/s]Running tokenizer on dataset (num_proc=83):  61%|██████▏   | 51/83 [00:05<00:03,  9.69 examples/s]Running tokenizer on dataset (num_proc=83):  64%|██████▍   | 53/83 [00:05<00:02, 10.12 examples/s]Running tokenizer on dataset (num_proc=83):  66%|██████▋   | 55/83 [00:05<00:02, 10.25 examples/s]Running tokenizer on dataset (num_proc=83):  69%|██████▊   | 57/83 [00:06<00:02,  9.23 examples/s]Running tokenizer on dataset (num_proc=83):  71%|███████   | 59/83 [00:06<00:02,  9.88 examples/s]Running tokenizer on dataset (num_proc=83):  73%|███████▎  | 61/83 [00:06<00:02, 10.17 examples/s]Running tokenizer on dataset (num_proc=83):  76%|███████▌  | 63/83 [00:06<00:02,  9.20 examples/s]Running tokenizer on dataset (num_proc=83):  80%|███████▉  | 66/83 [00:06<00:01, 10.98 examples/s]Running tokenizer on dataset (num_proc=83):  82%|████████▏ | 68/83 [00:06<00:01, 12.37 examples/s]Running tokenizer on dataset (num_proc=83):  84%|████████▍ | 70/83 [00:07<00:01, 10.52 examples/s]Running tokenizer on dataset (num_proc=83):  87%|████████▋ | 72/83 [00:07<00:01, 10.72 examples/s]Running tokenizer on dataset (num_proc=83):  89%|████████▉ | 74/83 [00:07<00:00, 10.91 examples/s]Running tokenizer on dataset (num_proc=83):  92%|█████████▏| 76/83 [00:07<00:00, 12.22 examples/s]Running tokenizer on dataset (num_proc=83):  94%|█████████▍| 78/83 [00:07<00:00,  9.72 examples/s]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [00:08<00:00, 12.28 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:08<00:00, 12.15 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:08<00:00,  9.94 examples/s]
[INFO|configuration_utils.py:763] 2025-10-21 22:50:00,218 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/config.json
[INFO|configuration_utils.py:839] 2025-10-21 22:50:00,248 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_size": 2560,
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "pad_token_id": 151643,
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": false,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": false,
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "dtype": "bfloat16",
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[WARNING|logging.py:328] 2025-10-21 22:50:01,998 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1169] 2025-10-21 22:50:01,999 >> loading weights file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-21 22:50:02,004 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-21 22:50:02,020 >> Generate config GenerationConfig {
  "eos_token_id": 151645,
  "pad_token_id": 151643
}

[INFO|modeling_utils.py:2341] 2025-10-21 22:50:02,024 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-21 22:50:02,102 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.02s/it]
[INFO|configuration_utils.py:939] 2025-10-21 22:50:16,253 >> loading configuration file /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-21 22:50:16,253 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-21 22:50:16,254 >> Could not locate the custom_generate/generate.py inside /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train.
[WARNING|trainer.py:906] 2025-10-21 22:50:16,332 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-21 22:50:16,362 >> Using auto half precision backend
[INFO|trainer.py:4643] 2025-10-21 22:50:16,803 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-10-21 22:50:16,803 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-21 22:50:16,803 >>   Batch size = 8
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:03<00:01,  1.68s/it]100%|██████████| 3/3 [00:06<00:00,  2.44s/it]Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Dumping model to file cache /scratch/slurm_tmpdir/job_3589755/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589755/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589755/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589755/jieba.cache
Loading model cost 0.483 seconds.
Prefix dict has been built successfully.
Loading model cost 0.484 seconds.
Prefix dict has been built successfully.
Loading model cost 0.485 seconds.
Prefix dict has been built successfully.
Loading model cost 0.486 seconds.
Prefix dict has been built successfully.
[INFO|integration_utils.py:867] 2025-10-21 22:50:30,318 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251021_225030-p8oqt76t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/saves/qwen3_5vl-4b/full/sft/roboG_qwen3_vl_ablation_two_frames_train_roboG_qwen3_vl_ablation_two_frames_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/p8oqt76t
100%|██████████| 3/3 [00:11<00:00,  3.72s/it]
[INFO|trainer.py:4643] 2025-10-21 22:50:34,060 >> 
***** Running Prediction *****
[INFO|trainer.py:4645] 2025-10-21 22:50:34,061 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-21 22:50:34,061 >>   Batch size = 8
  0%|          | 0/3 [00:00<?, ?it/s] 67%|██████▋   | 2/3 [00:03<00:01,  1.69s/it]wandb: WARNING Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
100%|██████████| 3/3 [00:06<00:00,  2.47s/it]100%|██████████| 3/3 [00:07<00:00,  2.37s/it]

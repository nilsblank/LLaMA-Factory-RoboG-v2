GpuFreq=control_disabled
[W1021 22:42:05.408022806 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:42:05.408063598 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:42:05.408084608 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 22:42:05.408111270 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:07,023 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 22:42:07,148 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-21 22:42:07,690 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-21 22:42:07,925 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-21 22:42:07,929 >> Image processor Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "max_pixels": 1003520,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 1003520,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 22:42:08,172 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 22:42:08,289 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-21 22:42:08,657 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-21 22:42:08,657 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-10-21 22:42:09,241 >> loading configuration file processor_config.json from cache at None
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1021 22:42:09.191381842 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-10-21 22:42:09,532 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "max_pixels": 1003520,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 1003520,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2Tokenizer(name_or_path='Qwen/Qwen3-VL-4B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1021 22:42:09.211457382 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1021 22:42:09.217648709 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Converting format of dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 10/56022 [00:00<09:51, 94.76 examples/s]Converting format of dataset (num_proc=128):   1%|          | 627/56022 [00:00<00:15, 3578.26 examples/s]Converting format of dataset (num_proc=128):   2%|▏         | 1154/56022 [00:00<00:12, 4335.00 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1728/56022 [00:00<00:11, 4882.99 examples/s]Converting format of dataset (num_proc=128):   4%|▍         | 2296/56022 [00:00<00:10, 5126.64 examples/s]Converting format of dataset (num_proc=128):   5%|▌         | 2862/56022 [00:00<00:10, 5303.44 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3452/56022 [00:00<00:09, 5463.28 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 4025/56022 [00:00<00:09, 5525.58 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4611/56022 [00:00<00:09, 5627.75 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5177/56022 [00:01<00:09, 5610.20 examples/s]Converting format of dataset (num_proc=128):  10%|█         | 5759/56022 [00:01<00:08, 5671.75 examples/s]Converting format of dataset (num_proc=128):  11%|█▏        | 6345/56022 [00:01<00:08, 5722.77 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6919/56022 [00:01<00:08, 5712.57 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7493/56022 [00:01<00:08, 5668.78 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 8071/56022 [00:01<00:08, 5699.16 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8651/56022 [00:01<00:08, 5727.60 examples/s]Converting format of dataset (num_proc=128):  16%|█▋        | 9225/56022 [00:01<00:08, 5707.49 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9798/56022 [00:01<00:08, 5707.74 examples/s]Converting format of dataset (num_proc=128):  19%|█▊        | 10369/56022 [00:01<00:08, 5699.83 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 10944/56022 [00:02<00:08, 5586.37 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11541/56022 [00:02<00:07, 5696.78 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12113/56022 [00:02<00:07, 5652.19 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 12696/56022 [00:02<00:07, 5700.84 examples/s]Converting format of dataset (num_proc=128):  24%|██▎       | 13268/56022 [00:02<00:07, 5691.10 examples/s]Converting format of dataset (num_proc=128):  25%|██▍       | 13840/56022 [00:02<00:07, 5638.28 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14412/56022 [00:02<00:07, 5657.98 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 14986/56022 [00:02<00:07, 5678.98 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15563/56022 [00:02<00:07, 5698.55 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16146/56022 [00:02<00:06, 5715.03 examples/s]Converting format of dataset (num_proc=128):  30%|██▉       | 16724/56022 [00:03<00:06, 5734.15 examples/s]Converting format of dataset (num_proc=128):  31%|███       | 17298/56022 [00:03<00:06, 5713.39 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 17880/56022 [00:03<00:06, 5723.29 examples/s]Converting format of dataset (num_proc=128):  33%|███▎      | 18454/56022 [00:03<00:06, 5645.18 examples/s]Converting format of dataset (num_proc=128):  34%|███▍      | 19032/56022 [00:03<00:06, 5674.71 examples/s]Converting format of dataset (num_proc=128):  35%|███▌      | 19616/56022 [00:03<00:06, 5721.75 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 20191/56022 [00:03<00:06, 5675.01 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20760/56022 [00:03<00:06, 5636.16 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21324/56022 [00:03<00:06, 5618.54 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 21913/56022 [00:03<00:05, 5698.91 examples/s]Converting format of dataset (num_proc=128):  40%|████      | 22485/56022 [00:04<00:06, 5578.88 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 23070/56022 [00:04<00:05, 5657.16 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23638/56022 [00:04<00:05, 5580.15 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24200/56022 [00:04<00:05, 5572.81 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24759/56022 [00:04<00:05, 5572.91 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 25340/56022 [00:04<00:05, 5642.47 examples/s]Converting format of dataset (num_proc=128):  46%|████▌     | 25905/56022 [00:04<00:05, 5634.85 examples/s]Converting format of dataset (num_proc=128):  47%|████▋     | 26474/56022 [00:04<00:05, 5647.66 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 27040/56022 [00:04<00:05, 5638.16 examples/s]Converting format of dataset (num_proc=128):  49%|████▉     | 27605/56022 [00:04<00:05, 5631.84 examples/s]Converting format of dataset (num_proc=128):  50%|█████     | 28184/56022 [00:05<00:04, 5662.68 examples/s]Converting format of dataset (num_proc=128):  51%|█████▏    | 28754/56022 [00:05<00:04, 5659.04 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 29331/56022 [00:05<00:04, 5683.48 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29901/56022 [00:05<00:04, 5650.32 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30490/56022 [00:05<00:04, 5718.03 examples/s]Converting format of dataset (num_proc=128):  55%|█████▌    | 31065/56022 [00:05<00:04, 5674.11 examples/s]Converting format of dataset (num_proc=128):  56%|█████▋    | 31643/56022 [00:05<00:04, 5689.51 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32230/56022 [00:05<00:04, 5734.59 examples/s]Converting format of dataset (num_proc=128):  59%|█████▊    | 32810/56022 [00:05<00:04, 5736.46 examples/s]Converting format of dataset (num_proc=128):  60%|█████▉    | 33385/56022 [00:05<00:03, 5731.61 examples/s]Converting format of dataset (num_proc=128):  61%|██████    | 33962/56022 [00:06<00:03, 5729.03 examples/s]Converting format of dataset (num_proc=128):  62%|██████▏   | 34584/56022 [00:06<00:03, 5864.28 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35171/56022 [00:06<00:03, 5727.08 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35746/56022 [00:06<00:03, 5733.70 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 36331/56022 [00:06<00:03, 5767.43 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 36909/56022 [00:06<00:03, 5714.42 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37483/56022 [00:06<00:03, 5702.34 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 38062/56022 [00:06<00:03, 5694.75 examples/s]Converting format of dataset (num_proc=128):  69%|██████▉   | 38646/56022 [00:06<00:03, 5732.95 examples/s]Converting format of dataset (num_proc=128):  70%|███████   | 39221/56022 [00:07<00:02, 5680.61 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39793/56022 [00:07<00:02, 5690.65 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40364/56022 [00:07<00:02, 5688.39 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40940/56022 [00:07<00:02, 5703.70 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41518/56022 [00:07<00:02, 5699.46 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42099/56022 [00:07<00:02, 5730.81 examples/s]Converting format of dataset (num_proc=128):  76%|███████▌  | 42675/56022 [00:07<00:02, 5676.06 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 43259/56022 [00:07<00:02, 5722.70 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43838/56022 [00:07<00:02, 5741.12 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44414/56022 [00:07<00:02, 5714.37 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 45002/56022 [00:08<00:01, 5752.17 examples/s]Converting format of dataset (num_proc=128):  81%|████████▏ | 45579/56022 [00:08<00:01, 5754.98 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 46160/56022 [00:08<00:01, 5767.69 examples/s]Converting format of dataset (num_proc=128):  83%|████████▎ | 46738/56022 [00:08<00:01, 5750.91 examples/s]Converting format of dataset (num_proc=128):  84%|████████▍ | 47334/56022 [00:08<00:01, 5804.47 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 47916/56022 [00:08<00:01, 5778.27 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48495/56022 [00:08<00:01, 5731.25 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49084/56022 [00:08<00:01, 5772.40 examples/s]Converting format of dataset (num_proc=128):  89%|████████▊ | 49664/56022 [00:08<00:01, 5779.65 examples/s]Converting format of dataset (num_proc=128):  90%|████████▉ | 50243/56022 [00:08<00:01, 5733.68 examples/s]Converting format of dataset (num_proc=128):  91%|█████████ | 50847/56022 [00:09<00:00, 5795.69 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51440/56022 [00:09<00:00, 5833.75 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 52026/56022 [00:09<00:00, 5825.93 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52613/56022 [00:09<00:00, 5822.66 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▍| 53204/56022 [00:09<00:00, 5810.76 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▌| 53787/56022 [00:09<00:00, 5763.11 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54368/56022 [00:09<00:00, 5602.19 examples/s]Converting format of dataset (num_proc=128):  98%|█████████▊| 54978/56022 [00:09<00:00, 5677.19 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▉| 55633/56022 [00:09<00:00, 5822.17 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56022/56022 [00:10<00:00, 5496.45 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):  23%|██▎       | 19/83 [00:00<00:00, 188.64 examples/s]Converting format of dataset (num_proc=83):  94%|█████████▍| 78/83 [00:00<00:00, 422.31 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 167.51 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1021 22:42:25.067364542 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56022 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56022 [01:01<2:10:59,  7.07 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56022 [01:09<1:03:18, 14.52 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56022 [01:16<40:12, 22.67 examples/s] Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56022 [01:19<26:36, 34.00 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56022 [01:19<16:58, 52.84 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56022 [01:24<14:51, 59.86 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 3066/56022 [01:26<10:59, 80.26 examples/s]Running tokenizer on dataset (num_proc=128):   7%|▋         | 3942/56022 [01:27<05:54, 146.92 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4380/56022 [01:28<04:53, 176.06 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 4818/56022 [01:28<03:54, 218.52 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 5256/56022 [01:29<02:53, 292.10 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 5694/56022 [01:31<03:37, 231.88 examples/s]Running tokenizer on dataset (num_proc=128):  11%|█         | 6132/56022 [01:33<03:19, 250.17 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6570/56022 [01:34<02:51, 287.96 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7008/56022 [01:41<06:02, 135.17 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 7884/56022 [01:47<05:42, 140.71 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8322/56022 [01:48<04:45, 167.36 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 8760/56022 [01:49<03:53, 202.67 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9198/56022 [01:57<06:28, 120.46 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9636/56022 [01:57<04:59, 154.90 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 10074/56022 [01:59<04:06, 186.53 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10512/56022 [01:59<02:57, 256.25 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10950/56022 [02:00<02:33, 293.64 examples/s]Running tokenizer on dataset (num_proc=128):  20%|██        | 11388/56022 [02:05<04:41, 158.78 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 11826/56022 [02:07<04:12, 174.81 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12264/56022 [02:09<03:47, 192.12 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12702/56022 [02:11<03:38, 198.06 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 13140/56022 [02:12<02:51, 249.34 examples/s]Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13578/56022 [02:12<02:07, 333.72 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14016/56022 [02:12<01:32, 454.52 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14454/56022 [02:14<01:53, 366.92 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 14892/56022 [02:14<01:26, 477.83 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15330/56022 [02:18<02:52, 235.37 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 15768/56022 [02:20<02:35, 259.42 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16206/56022 [02:21<02:12, 301.26 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16644/56022 [02:23<02:25, 270.79 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 17082/56022 [02:23<01:54, 341.31 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 17520/56022 [02:26<02:33, 250.14 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17958/56022 [02:32<04:17, 147.90 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17958/56022 [02:48<04:17, 147.90 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 18396/56022 [02:48<10:09, 61.77 examples/s] Running tokenizer on dataset (num_proc=128):  34%|███▎      | 18834/56022 [02:56<10:12, 60.71 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 19272/56022 [03:05<11:03, 55.36 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▍      | 19272/56022 [03:18<11:03, 55.36 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19710/56022 [03:19<13:13, 45.74 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20148/56022 [03:24<11:22, 52.59 examples/s]Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20148/56022 [03:38<11:22, 52.59 examples/s]Running tokenizer on dataset (num_proc=128):  37%|███▋      | 20586/56022 [03:41<14:28, 40.82 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21024/56022 [03:41<10:04, 57.92 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21462/56022 [03:47<09:22, 61.49 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21900/56022 [03:47<06:31, 87.15 examples/s]Running tokenizer on dataset (num_proc=128):  39%|███▉      | 21900/56022 [03:58<06:31, 87.15 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22338/56022 [03:59<08:58, 62.59 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 22776/56022 [04:02<07:20, 75.46 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23214/56022 [04:03<05:41, 95.94 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23652/56022 [04:08<05:32, 97.44 examples/s]Running tokenizer on dataset (num_proc=128):  43%|████▎     | 24090/56022 [04:10<04:36, 115.53 examples/s]Running tokenizer on dataset (num_proc=128):  44%|████▍     | 24528/56022 [04:12<03:58, 132.04 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24966/56022 [04:18<04:57, 104.23 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▌     | 25404/56022 [04:20<03:54, 130.64 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25842/56022 [04:23<03:48, 132.00 examples/s]Running tokenizer on dataset (num_proc=128):  47%|████▋     | 26280/56022 [04:24<02:51, 173.66 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26718/56022 [04:26<02:55, 166.69 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27156/56022 [04:35<04:43, 101.87 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27594/56022 [04:35<03:27, 137.03 examples/s]Running tokenizer on dataset (num_proc=128):  50%|█████     | 28032/56022 [04:36<02:34, 181.49 examples/s]Running tokenizer on dataset (num_proc=128):  51%|█████     | 28470/56022 [04:37<02:09, 213.09 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 28908/56022 [04:37<01:31, 296.05 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 29346/56022 [04:38<01:13, 363.27 examples/s]Running tokenizer on dataset (num_proc=128):  53%|█████▎    | 29784/56022 [04:39<01:16, 343.72 examples/s]Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30222/56022 [04:42<01:39, 260.32 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▍    | 30660/56022 [04:44<01:50, 229.96 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▌    | 31098/56022 [04:46<01:40, 247.77 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 31536/56022 [04:46<01:14, 327.29 examples/s]Running tokenizer on dataset (num_proc=128):  57%|█████▋    | 31974/56022 [04:47<01:03, 377.26 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32412/56022 [04:48<00:55, 427.23 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 32850/56022 [04:49<01:03, 363.45 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 33288/56022 [04:53<01:45, 214.83 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33726/56022 [04:54<01:31, 243.40 examples/s]Running tokenizer on dataset (num_proc=128):  61%|██████    | 34164/56022 [04:55<01:07, 323.26 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 34602/56022 [04:56<00:58, 364.58 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35040/56022 [04:57<01:00, 344.12 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35478/56022 [05:01<01:45, 194.64 examples/s]Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 35915/56022 [05:11<03:28, 96.37 examples/s] Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 36352/56022 [05:13<02:49, 115.91 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 36789/56022 [05:22<03:54, 81.93 examples/s] Running tokenizer on dataset (num_proc=128):  66%|██████▋   | 37227/56022 [05:23<02:47, 112.05 examples/s]Running tokenizer on dataset (num_proc=128):  67%|██████▋   | 37665/56022 [05:24<02:04, 147.62 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38102/56022 [05:24<01:32, 194.31 examples/s]Running tokenizer on dataset (num_proc=128):  69%|██████▉   | 38540/56022 [05:26<01:25, 204.22 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38978/56022 [05:31<01:53, 150.50 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 39852/56022 [05:33<01:16, 211.44 examples/s]Running tokenizer on dataset (num_proc=128):  72%|███████▏  | 40289/56022 [05:34<01:00, 260.34 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 40726/56022 [05:38<01:28, 172.42 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 41163/56022 [05:40<01:20, 184.81 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41600/56022 [05:41<01:05, 219.29 examples/s]Running tokenizer on dataset (num_proc=128):  75%|███████▌  | 42037/56022 [05:43<01:01, 228.32 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42474/56022 [05:44<00:48, 277.50 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42911/56022 [05:46<00:53, 244.25 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 43348/56022 [05:51<01:17, 164.44 examples/s]Running tokenizer on dataset (num_proc=128):  78%|███████▊  | 43785/56022 [05:52<00:58, 208.46 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44222/56022 [05:53<00:48, 241.70 examples/s]Running tokenizer on dataset (num_proc=128):  80%|███████▉  | 44660/56022 [05:54<00:40, 279.16 examples/s]Running tokenizer on dataset (num_proc=128):  80%|████████  | 45097/56022 [05:58<01:00, 181.26 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 45534/56022 [05:59<00:43, 243.23 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 45971/56022 [06:08<01:31, 109.38 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46408/56022 [06:10<01:18, 122.97 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▎ | 46845/56022 [06:12<01:02, 145.67 examples/s]Running tokenizer on dataset (num_proc=128):  84%|████████▍ | 47282/56022 [06:12<00:42, 203.98 examples/s]Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 47719/56022 [06:13<00:31, 266.35 examples/s]Running tokenizer on dataset (num_proc=128):  86%|████████▌ | 48156/56022 [06:14<00:27, 281.49 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 48593/56022 [06:14<00:19, 380.38 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49030/56022 [06:15<00:15, 448.52 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49467/56022 [06:16<00:14, 459.17 examples/s]Running tokenizer on dataset (num_proc=128):  89%|████████▉ | 49904/56022 [06:16<00:12, 486.85 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50341/56022 [06:20<00:20, 272.73 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50778/56022 [06:21<00:20, 261.92 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████▏| 51215/56022 [06:23<00:19, 252.36 examples/s]Running tokenizer on dataset (num_proc=128):  92%|█████████▏| 51652/56022 [06:25<00:17, 252.33 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 52089/56022 [06:26<00:12, 310.57 examples/s]Running tokenizer on dataset (num_proc=128):  94%|█████████▍| 52526/56022 [06:26<00:08, 430.14 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 52963/56022 [06:28<00:09, 323.27 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▌| 53400/56022 [06:30<00:09, 275.89 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 53837/56022 [06:32<00:08, 255.34 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 54711/56022 [06:34<00:03, 340.46 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55148/56022 [06:37<00:03, 239.27 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 55585/56022 [06:42<00:02, 165.36 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [06:48<00:00, 127.69 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56022/56022 [06:48<00:00, 137.27 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:01<02:07,  1.56s/ examples]Running tokenizer on dataset (num_proc=83):   2%|▏         | 2/83 [00:02<01:46,  1.31s/ examples]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:03<01:38,  1.23s/ examples]Running tokenizer on dataset (num_proc=83):   5%|▍         | 4/83 [00:04<01:32,  1.17s/ examples]Running tokenizer on dataset (num_proc=83):   6%|▌         | 5/83 [00:05<01:28,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):   7%|▋         | 6/83 [00:07<01:29,  1.16s/ examples]Running tokenizer on dataset (num_proc=83):   8%|▊         | 7/83 [00:08<01:26,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  10%|▉         | 8/83 [00:09<01:25,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  11%|█         | 9/83 [00:10<01:25,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  12%|█▏        | 10/83 [00:11<01:22,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:12<01:22,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  14%|█▍        | 12/83 [00:13<01:20,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  16%|█▌        | 13/83 [00:15<01:18,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  17%|█▋        | 14/83 [00:16<01:19,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  18%|█▊        | 15/83 [00:17<01:17,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  19%|█▉        | 16/83 [00:18<01:16,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  20%|██        | 17/83 [00:19<01:15,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  22%|██▏       | 18/83 [00:20<01:12,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  23%|██▎       | 19/83 [00:21<01:13,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  24%|██▍       | 20/83 [00:23<01:11,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  25%|██▌       | 21/83 [00:24<01:12,  1.17s/ examples]Running tokenizer on dataset (num_proc=83):  27%|██▋       | 22/83 [00:25<01:08,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  28%|██▊       | 23/83 [00:26<01:07,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  29%|██▉       | 24/83 [00:27<01:06,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  30%|███       | 25/83 [00:28<01:05,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  31%|███▏      | 26/83 [00:29<01:04,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  33%|███▎      | 27/83 [00:31<01:03,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:32<01:02,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  35%|███▍      | 29/83 [00:33<01:00,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:34<01:00,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  37%|███▋      | 31/83 [00:35<00:58,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:36<00:57,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  40%|███▉      | 33/83 [00:37<00:57,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:38<00:55,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  42%|████▏     | 35/83 [00:40<00:53,  1.11s/ examples]Running tokenizer on dataset (num_proc=83):  43%|████▎     | 36/83 [00:41<00:52,  1.11s/ examples]Running tokenizer on dataset (num_proc=83):  45%|████▍     | 37/83 [00:42<00:51,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  46%|████▌     | 38/83 [00:43<00:50,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  47%|████▋     | 39/83 [00:44<00:49,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  48%|████▊     | 40/83 [00:45<00:48,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  49%|████▉     | 41/83 [00:46<00:47,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  51%|█████     | 42/83 [00:47<00:45,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:49<00:44,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  53%|█████▎    | 44/83 [00:50<00:44,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:51<00:43,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  55%|█████▌    | 46/83 [00:52<00:41,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  57%|█████▋    | 47/83 [00:53<00:40,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  58%|█████▊    | 48/83 [00:54<00:39,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  59%|█████▉    | 49/83 [00:55<00:38,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  60%|██████    | 50/83 [00:57<00:38,  1.16s/ examples]Running tokenizer on dataset (num_proc=83):  61%|██████▏   | 51/83 [00:58<00:36,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  63%|██████▎   | 52/83 [00:59<00:35,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  64%|██████▍   | 53/83 [01:00<00:34,  1.16s/ examples]Running tokenizer on dataset (num_proc=83):  65%|██████▌   | 54/83 [01:01<00:33,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  66%|██████▋   | 55/83 [01:02<00:32,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  67%|██████▋   | 56/83 [01:03<00:31,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  69%|██████▊   | 57/83 [01:05<00:29,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  70%|██████▉   | 58/83 [01:06<00:28,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  71%|███████   | 59/83 [01:07<00:27,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  72%|███████▏  | 60/83 [01:08<00:25,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  73%|███████▎  | 61/83 [01:09<00:24,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  75%|███████▍  | 62/83 [01:10<00:24,  1.16s/ examples]Running tokenizer on dataset (num_proc=83):  76%|███████▌  | 63/83 [01:11<00:22,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  77%|███████▋  | 64/83 [01:12<00:21,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  78%|███████▊  | 65/83 [01:14<00:20,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  80%|███████▉  | 66/83 [01:15<00:19,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  81%|████████  | 67/83 [01:16<00:18,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  82%|████████▏ | 68/83 [01:17<00:17,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  83%|████████▎ | 69/83 [01:18<00:15,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  84%|████████▍ | 70/83 [01:19<00:14,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  86%|████████▌ | 71/83 [01:20<00:13,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  87%|████████▋ | 72/83 [01:22<00:12,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  88%|████████▊ | 73/83 [01:23<00:11,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  89%|████████▉ | 74/83 [01:24<00:10,  1.15s/ examples]Running tokenizer on dataset (num_proc=83):  90%|█████████ | 75/83 [01:25<00:09,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  92%|█████████▏| 76/83 [01:26<00:07,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  93%|█████████▎| 77/83 [01:27<00:06,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  94%|█████████▍| 78/83 [01:28<00:05,  1.17s/ examples]Running tokenizer on dataset (num_proc=83):  95%|█████████▌| 79/83 [01:29<00:04,  1.12s/ examples]Running tokenizer on dataset (num_proc=83):  96%|█████████▋| 80/83 [01:31<00:03,  1.14s/ examples]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [01:32<00:02,  1.13s/ examples]Running tokenizer on dataset (num_proc=83):  99%|█████████▉| 82/83 [01:33<00:01,  1.10s/ examples]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [01:34<00:00,  1.12s/ examples]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [01:34<00:00,  1.14s/ examples]
[INFO|configuration_utils.py:765] 2025-10-21 22:50:59,157 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/config.json
[INFO|configuration_utils.py:839] 2025-10-21 22:50:59,165 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

[WARNING|logging.py:328] 2025-10-21 22:50:59,580 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-21 22:50:59,582 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-21 22:50:59,586 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-21 22:50:59,594 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-10-21 22:50:59,598 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-21 22:50:59,638 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  9.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  9.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  9.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.36s/it]
[INFO|configuration_utils.py:941] 2025-10-21 22:51:20,570 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-21 22:51:20,570 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-10-21 22:51:20,685 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-4B-Instruct.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[WARNING|trainer.py:906] 2025-10-21 22:51:20,704 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-21 22:51:20,708 >> Using auto half precision backend
The model is already on multiple devices. Skipping the move to device specified in `args`.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-21 22:51:21,057 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:1335] 2025-10-21 22:51:27,800 >> skipped Embedding(2304, 1024): 2.25M params
[INFO|trainer.py:1335] 2025-10-21 22:51:27,800 >> skipped Embedding(151936, 2560): 373.1875M params
[INFO|trainer.py:1338] 2025-10-21 22:51:27,801 >> skipped: 373.1875M params
[INFO|trainer.py:2519] 2025-10-21 22:51:28,023 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-21 22:51:28,023 >>   Num examples = 56,022
[INFO|trainer.py:2521] 2025-10-21 22:51:28,023 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-21 22:51:28,023 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:2525] 2025-10-21 22:51:28,023 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-10-21 22:51:28,023 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2527] 2025-10-21 22:51:28,023 >>   Total optimization steps = 2,628
[INFO|trainer.py:2528] 2025-10-21 22:51:28,025 >>   Number of trainable parameters = 4,106,660,864
[INFO|integration_utils.py:867] 2025-10-21 22:51:28,026 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251021_225128-yv6e5ull
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen/Qwen3-VL-4B-Instruct_roboG_qwen3_vl_temporal_grounding_box_train_sft
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/yv6e5ull
  0%|          | 0/2628 [00:00<?, ?it/s]  0%|          | 1/2628 [00:05<3:57:51,  5.43s/it]  0%|          | 2/2628 [00:06<2:14:07,  3.06s/it]  0%|          | 3/2628 [00:08<1:39:02,  2.26s/it]  0%|          | 4/2628 [00:09<1:22:26,  1.89s/it]  0%|          | 5/2628 [00:10<1:13:16,  1.68s/it]  0%|          | 6/2628 [00:12<1:06:57,  1.53s/it]  0%|          | 7/2628 [00:13<1:02:51,  1.44s/it]  0%|          | 8/2628 [00:14<1:00:47,  1.39s/it]  0%|          | 9/2628 [00:15<59:22,  1.36s/it]    0%|          | 10/2628 [00:17<58:13,  1.33s/it]                                                   0%|          | 10/2628 [00:17<58:13,  1.33s/it]  0%|          | 11/2628 [00:18<57:00,  1.31s/it]  0%|          | 12/2628 [00:19<56:30,  1.30s/it]  0%|          | 13/2628 [00:20<55:49,  1.28s/it]  1%|          | 14/2628 [00:22<56:09,  1.29s/it]  1%|          | 15/2628 [00:23<55:27,  1.27s/it]  1%|          | 16/2628 [00:24<55:08,  1.27s/it]  1%|          | 17/2628 [00:25<55:00,  1.26s/it]  1%|          | 18/2628 [00:27<55:25,  1.27s/it]  1%|          | 19/2628 [00:28<55:25,  1.27s/it]  1%|          | 20/2628 [00:29<55:10,  1.27s/it]                                                   1%|          | 20/2628 [00:29<55:10,  1.27s/it]  1%|          | 21/2628 [00:31<54:57,  1.26s/it]  1%|          | 22/2628 [00:32<55:12,  1.27s/it]  1%|          | 23/2628 [00:33<55:00,  1.27s/it]  1%|          | 24/2628 [00:34<55:03,  1.27s/it]  1%|          | 25/2628 [00:36<54:41,  1.26s/it]  1%|          | 26/2628 [00:37<54:51,  1.27s/it]  1%|          | 27/2628 [00:38<55:13,  1.27s/it]  1%|          | 28/2628 [00:39<54:43,  1.26s/it]  1%|          | 29/2628 [00:41<54:29,  1.26s/it]  1%|          | 30/2628 [00:42<54:43,  1.26s/it]                                                   1%|          | 30/2628 [00:42<54:43,  1.26s/it]  1%|          | 31/2628 [00:43<54:26,  1.26s/it]  1%|          | 32/2628 [00:44<54:13,  1.25s/it]  1%|▏         | 33/2628 [00:46<54:03,  1.25s/it]  1%|▏         | 34/2628 [00:47<53:56,  1.25s/it]  1%|▏         | 35/2628 [00:48<54:17,  1.26s/it]  1%|▏         | 36/2628 [00:49<54:16,  1.26s/it]  1%|▏         | 37/2628 [00:51<54:32,  1.26s/it]  1%|▏         | 38/2628 [00:52<54:21,  1.26s/it]  1%|▏         | 39/2628 [00:53<54:52,  1.27s/it]  2%|▏         | 40/2628 [00:55<54:53,  1.27s/it]                                                   2%|▏         | 40/2628 [00:55<54:53,  1.27s/it]  2%|▏         | 41/2628 [00:56<55:14,  1.28s/it]  2%|▏         | 42/2628 [00:57<54:48,  1.27s/it]  2%|▏         | 43/2628 [00:58<54:35,  1.27s/it]  2%|▏         | 44/2628 [01:00<54:11,  1.26s/it]  2%|▏         | 45/2628 [01:01<54:22,  1.26s/it]  2%|▏         | 46/2628 [01:02<54:13,  1.26s/it]  2%|▏         | 47/2628 [01:03<54:13,  1.26s/it]  2%|▏         | 48/2628 [01:05<54:06,  1.26s/it]  2%|▏         | 49/2628 [01:06<54:17,  1.26s/it]  2%|▏         | 50/2628 [01:07<54:05,  1.26s/it]                                                   2%|▏         | 50/2628 [01:07<54:05,  1.26s/it]  2%|▏         | 51/2628 [01:08<54:33,  1.27s/it]  2%|▏         | 52/2628 [01:10<54:08,  1.26s/it]  2%|▏         | 53/2628 [01:11<54:35,  1.27s/it]  2%|▏         | 54/2628 [01:12<54:45,  1.28s/it]  2%|▏         | 55/2628 [01:13<54:15,  1.27s/it]  2%|▏         | 56/2628 [01:15<53:54,  1.26s/it]  2%|▏         | 57/2628 [01:16<53:40,  1.25s/it]  2%|▏         | 58/2628 [01:17<53:41,  1.25s/it]  2%|▏         | 59/2628 [01:18<53:56,  1.26s/it]  2%|▏         | 60/2628 [01:20<53:56,  1.26s/it]                                                   2%|▏         | 60/2628 [01:20<53:56,  1.26s/it]  2%|▏         | 61/2628 [01:21<54:02,  1.26s/it]  2%|▏         | 62/2628 [01:22<54:12,  1.27s/it]  2%|▏         | 63/2628 [01:24<53:45,  1.26s/it]  2%|▏         | 64/2628 [01:25<53:47,  1.26s/it]  2%|▏         | 65/2628 [01:26<54:30,  1.28s/it]  3%|▎         | 66/2628 [01:27<54:23,  1.27s/it]  3%|▎         | 67/2628 [01:29<54:13,  1.27s/it]  3%|▎         | 68/2628 [01:30<54:04,  1.27s/it]  3%|▎         | 69/2628 [01:31<53:52,  1.26s/it]  3%|▎         | 70/2628 [01:32<53:57,  1.27s/it]                                                   3%|▎         | 70/2628 [01:32<53:57,  1.27s/it]  3%|▎         | 71/2628 [01:34<53:38,  1.26s/it]  3%|▎         | 72/2628 [01:35<53:33,  1.26s/it]  3%|▎         | 73/2628 [01:36<53:25,  1.25s/it]  3%|▎         | 74/2628 [01:37<53:31,  1.26s/it]  3%|▎         | 75/2628 [01:39<53:34,  1.26s/it]  3%|▎         | 76/2628 [01:40<53:45,  1.26s/it]  3%|▎         | 77/2628 [01:41<53:27,  1.26s/it]  3%|▎         | 78/2628 [01:42<53:15,  1.25s/it]  3%|▎         | 79/2628 [01:44<53:36,  1.26s/it]  3%|▎         | 80/2628 [01:45<53:44,  1.27s/it]                                                   3%|▎         | 80/2628 [01:45<53:44,  1.27s/it]  3%|▎         | 81/2628 [01:46<53:51,  1.27s/it]  3%|▎         | 82/2628 [01:48<54:01,  1.27s/it]  3%|▎         | 83/2628 [01:49<54:04,  1.28s/it]  3%|▎         | 84/2628 [01:50<54:09,  1.28s/it]  3%|▎         | 85/2628 [01:51<54:01,  1.27s/it]  3%|▎         | 86/2628 [01:53<53:43,  1.27s/it]  3%|▎         | 87/2628 [01:54<53:27,  1.26s/it]  3%|▎         | 88/2628 [01:55<53:20,  1.26s/it]  3%|▎         | 89/2628 [01:56<53:30,  1.26s/it]  3%|▎         | 90/2628 [01:58<53:05,  1.26s/it]                                                   3%|▎         | 90/2628 [01:58<53:05,  1.26s/it]  3%|▎         | 91/2628 [01:59<53:19,  1.26s/it]  4%|▎         | 92/2628 [02:00<52:57,  1.25s/it]  4%|▎         | 93/2628 [02:01<52:56,  1.25s/it]  4%|▎         | 94/2628 [02:03<53:16,  1.26s/it]  4%|▎         | 95/2628 [02:04<53:30,  1.27s/it]  4%|▎         | 96/2628 [02:05<53:11,  1.26s/it]  4%|▎         | 97/2628 [02:07<53:16,  1.26s/it]  4%|▎         | 98/2628 [02:08<53:32,  1.27s/it]  4%|▍         | 99/2628 [02:09<53:21,  1.27s/it]  4%|▍         | 100/2628 [02:10<53:07,  1.26s/it]                                                    4%|▍         | 100/2628 [02:10<53:07,  1.26s/it][INFO|trainer.py:4643] 2025-10-21 22:53:40,249 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-10-21 22:53:40,249 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-21 22:53:40,249 >>   Batch size = 8
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
    loss, generated_tokens, _ = super().prediction_step(
                                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
    self._validate_model_kwargs(model_kwargs.copy())
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
    raise ValueError(
ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank1]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank1]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank1]:     output = eval_loop(
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank1]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank1]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank1]:     loss, generated_tokens, _ = super().prediction_step(
[rank1]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank1]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank1]:     self._validate_model_kwargs(model_kwargs.copy())
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank1]:     raise ValueError(
[rank1]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank2]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank2]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank2]:     output = eval_loop(
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank2]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank2]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank2]:     loss, generated_tokens, _ = super().prediction_step(
[rank2]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank2]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank2]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank2]:     self._validate_model_kwargs(model_kwargs.copy())
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank2]:     raise ValueError(
[rank2]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank0]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank0]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank0]:     output = eval_loop(
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank0]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank0]:     loss, generated_tokens, _ = super().prediction_step(
[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank0]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank0]:     self._validate_model_kwargs(model_kwargs.copy())
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank0]:     raise ValueError(
[rank0]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 142, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 82, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank3]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank3]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 206, in evaluate
[rank3]:     output = eval_loop(
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4685, in evaluation_loop
[rank3]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank3]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/trainer.py", line 264, in prediction_step
[rank3]:     loss, generated_tokens, _ = super().prediction_step(
[rank3]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 327, in prediction_step
[rank3]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 2388, in generate
[rank3]:     self._validate_model_kwargs(model_kwargs.copy())
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/generation/utils.py", line 1599, in _validate_model_kwargs
[rank3]:     raise ValueError(
[rank3]: ValueError: The following `model_kwargs` are not used by the model: ['video_metadata'] (note: typos in the generate arguments will also show up in this list)
W1021 22:53:46.120000 1514140 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1514142 closing signal SIGTERM
W1021 22:53:46.125000 1514140 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1514143 closing signal SIGTERM
W1021 22:53:46.126000 1514140 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1514145 closing signal SIGTERM
E1021 22:53:46.942000 1514140 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 1514144) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-21_22:53:46
  host      : hkn0916.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1514144)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '36179', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen.yaml']' returned non-zero exit status 1.
srun: error: hkn0916: task 0: Exited with exit code 1

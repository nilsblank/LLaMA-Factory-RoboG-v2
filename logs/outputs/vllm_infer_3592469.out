[WARNING|2025-10-23 14:09:14] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 10-23 14:09:15 [__init__.py:216] Automatically detected platform cuda.
INFO 10-23 14:09:30 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_model_len': 3072, 'pipeline_parallel_size': 4, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 4, 'video': 2, 'audio': 2}, 'model': '/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train'}
INFO 10-23 14:09:50 [model.py:547] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 10-23 14:09:50 [model.py:1510] Using max model len 3072
INFO 10-23 14:09:53 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 10-23 14:09:54 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2025-10-23 14:10:00] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 10-23 14:10:00 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:10:01 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:10:01 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train', speculative_config=None, tokenizer='/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=4, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:10:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_522abff4'), local_subscribe_addr='ipc:///scratch/slurm_tmpdir/job_3592469/c367254b-1cfc-48e5-afa1-3402eb62c42a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[WARNING|2025-10-23 14:10:07] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 10-23 14:10:07 [__init__.py:216] Automatically detected platform cuda.
[WARNING|2025-10-23 14:10:07] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2025-10-23 14:10:07] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2025-10-23 14:10:07] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 10-23 14:10:07 [__init__.py:216] Automatically detected platform cuda.
INFO 10-23 14:10:07 [__init__.py:216] Automatically detected platform cuda.
INFO 10-23 14:10:07 [__init__.py:216] Automatically detected platform cuda.
INFO 10-23 14:10:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e7d3a14b'), local_subscribe_addr='ipc:///scratch/slurm_tmpdir/job_3592469/c083a409-a24f-45d5-8ad5-fd2d17890734', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 10-23 14:10:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_07ae061e'), local_subscribe_addr='ipc:///scratch/slurm_tmpdir/job_3592469/3078bcd6-2fff-4ca0-9427-d48bc9f98d54', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 10-23 14:10:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4053ed9e'), local_subscribe_addr='ipc:///scratch/slurm_tmpdir/job_3592469/532758c6-3603-495b-9ee0-2ea50a03f3db', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 10-23 14:10:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_54e7d9a1'), local_subscribe_addr='ipc:///scratch/slurm_tmpdir/job_3592469/daae46de-0718-4173-950e-49e331181628', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 10-23 14:10:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-23 14:10:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-23 14:10:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-23 14:10:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 10-23 14:10:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-23 14:10:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-23 14:10:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 10-23 14:10:18 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 10-23 14:10:18 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 3, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 10-23 14:10:18 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 10-23 14:10:18 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 10-23 14:10:18 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 10-23 14:10:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 10-23 14:10:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 10-23 14:10:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 10-23 14:10:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2602] Starting to load model /home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train...
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2602] Starting to load model /home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train...
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2602] Starting to load model /home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train...
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2602] Starting to load model /home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train...
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:31 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:31 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:31 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:31 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:36 [default_loader.py:267] Loading weights took 4.62 seconds
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:36 [default_loader.py:267] Loading weights took 4.62 seconds
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:36 [default_loader.py:267] Loading weights took 4.64 seconds
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:36 [default_loader.py:267] Loading weights took 4.65 seconds
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:36 [gpu_model_runner.py:2653] Model loading took 2.6727 GiB and 15.103805 seconds
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:36 [gpu_model_runner.py:2653] Model loading took 2.6727 GiB and 15.136199 seconds
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:36 [gpu_model_runner.py:2653] Model loading took 3.2528 GiB and 15.137912 seconds
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:36 [gpu_model_runner.py:2653] Model loading took 3.2528 GiB and 15.131161 seconds
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:10:37 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:10:37 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:10:37 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:10:37 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:02 [backends.py:548] Using cache directory: /home/hk-project-sustainebot/bm3844/.cache/vllm/torch_compile_cache/8bde67748f/rank_2_0/backbone for vLLM's torch.compile
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:02 [backends.py:548] Using cache directory: /home/hk-project-sustainebot/bm3844/.cache/vllm/torch_compile_cache/8bde67748f/rank_0_0/backbone for vLLM's torch.compile
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:02 [backends.py:548] Using cache directory: /home/hk-project-sustainebot/bm3844/.cache/vllm/torch_compile_cache/8bde67748f/rank_3_0/backbone for vLLM's torch.compile
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:02 [backends.py:559] Dynamo bytecode transform time: 19.51 s
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:02 [backends.py:559] Dynamo bytecode transform time: 19.63 s
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:02 [backends.py:559] Dynamo bytecode transform time: 19.98 s
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:02 [backends.py:548] Using cache directory: /home/hk-project-sustainebot/bm3844/.cache/vllm/torch_compile_cache/8bde67748f/rank_1_0/backbone for vLLM's torch.compile
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:02 [backends.py:559] Dynamo bytecode transform time: 19.96 s
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:09 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:09 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:09 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:09 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:15 [backends.py:218] Compiling a graph for dynamic shape takes 12.98 s
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:15 [backends.py:218] Compiling a graph for dynamic shape takes 13.12 s
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:16 [backends.py:218] Compiling a graph for dynamic shape takes 13.30 s
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:16 [backends.py:218] Compiling a graph for dynamic shape takes 13.32 s
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:17 [monitor.py:34] torch.compile takes 33.28 s in total
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:17 [monitor.py:34] torch.compile takes 32.81 s in total
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:17 [monitor.py:34] torch.compile takes 32.75 s in total
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:17 [monitor.py:34] torch.compile takes 32.97 s in total
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:17 [gpu_worker.py:298] Available KV cache memory: 29.53 GiB
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:18 [gpu_worker.py:298] Available KV cache memory: 29.53 GiB
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:18 [gpu_worker.py:298] Available KV cache memory: 28.95 GiB
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:18 [gpu_worker.py:298] Available KV cache memory: 28.94 GiB
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1087] GPU KV cache size: 3,372,464 tokens
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1091] Maximum concurrency for 3,072 tokens per request: 1097.81x
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1087] GPU KV cache size: 3,440,048 tokens
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1091] Maximum concurrency for 3,072 tokens per request: 1119.81x
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1087] GPU KV cache size: 3,440,048 tokens
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1091] Maximum concurrency for 3,072 tokens per request: 1119.81x
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1087] GPU KV cache size: 3,372,240 tokens
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:18 [kv_cache_utils.py:1091] Maximum concurrency for 3,072 tokens per request: 1097.73x
[1;36m(Worker_PP0 pid=3708424)[0;0m INFO 10-23 14:11:20 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.29 GiB
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:11:20 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.29 GiB
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:11:20 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.29 GiB
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:11:20 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.29 GiB
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:20 [core.py:210] init engine (profile, create kv cache, warmup model) took 43.70 seconds
[1;36m(EngineCore_DP0 pid=3708410)[0;0m INFO 10-23 14:11:24 [core.py:149] Batch queue is enabled with size 4
INFO 10-23 14:11:25 [llm.py:306] Supported_tasks: ['generate']
[INFO|2025-10-23 14:11:25] llamafactory.data.loader:143 >> Loading dataset /home/hk-project-sustainebot/bm3844/datasets/datasets/robogrounder/roboG_stagepoc_ablation_two_frames_train.jsonl...
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 151652, 151655, 151653, 37427, 2986, 279, 1378, 5335, 323, 10542, 279, 1633, 429, 572, 54215, 553, 279, 12305, 476, 3738, 13, 39565, 279, 2383, 323, 30618, 3745, 315, 279, 16282, 291, 1633, 304, 4718, 3561, 369, 279, 2661, 1156, 4034, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|> Analyze the two images and identify the object that was manipulated by the robot or human. Provide the label and bounding box of the interacted object in JSON format for the given first frame.<|im_end|>
<|im_start|>assistant

label_ids:
[785, 12305, 16282, 291, 448, 279, 15097, 624, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 21, 17, 11, 220, 17, 17, 18, 11, 220, 19, 19, 21, 11, 220, 19, 18, 17, 1125, 330, 1502, 788, 330, 32578, 6375, 16707, 921, 73594, 151645, 198]
labels:
The robot interacted with the clothes.
```json
[
{"bbox_2d": [262, 223, 446, 432], "label": "clothes"}
]
```<|im_end|>

[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:48 [multiproc_executor.py:154] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[1;36m(Worker_PP1 pid=3708425)[0;0m INFO 10-23 14:12:48 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=3708426)[0;0m INFO 10-23 14:12:48 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP3 pid=3708427)[0;0m INFO 10-23 14:12:48 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:69] Dumping input data for V1 LLM engine (v0.11.0) with config: model='/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train', speculative_config=None, tokenizer='/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=4, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/hk-project-sustainebot/bm3844/code/LLaMA-FactoryRoboG/saves/qwen2_5vl-3b/full/sft/roboG_stagepoc_ablation_two_frames_train, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}, 
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=305,prompt_token_ids_len=734,mm_features=[MultiModalFeatureSpec(data={'pixel_values': MultiModalFieldElem(modality='image', key='pixel_values', data=tensor([[-1.5000, -1.5000, -1.5000,  ..., -1.3828, -1.3828, -1.2969],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [-1.5469, -1.5625, -1.6016,  ..., -1.2109, -1.2422, -1.2500],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [-1.5000, -1.5000, -1.5156,  ..., -1.0391, -1.2266, -1.4062],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         ...,
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.4609,  1.4453,  1.4609,  ...,  0.6367,  0.6250,  0.5977],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.5391,  1.5234,  1.5078,  ...,  0.9102,  0.9102,  0.9375],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.4766,  1.4453,  1.4453,  ...,  0.8359,  0.8242,  0.7969]],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]        dtype=torch.bfloat16), field=MultiModalFlatField(slices=[[slice(0, 1344, None)], [slice(1344, 2688, None)]], dim=0)), 'image_grid_thw': MultiModalFieldElem(modality='image', key='image_grid_thw', data=tensor([ 1, 32, 42]), field=MultiModalBatchedField())}, modality='image', identifier='ef8cb79179f55875020bd0dd4775a8ddba17f51a2cc7c1696688ba35cb1d238d', mm_position=PlaceholderRange(offset=15, length=336, is_embed=None)), MultiModalFeatureSpec(data={'pixel_values': MultiModalFieldElem(modality='image', key='pixel_values', data=tensor([[-1.5000, -1.5000, -1.5000,  ..., -1.3828, -1.3828, -1.2969],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [-1.5469, -1.5625, -1.6016,  ..., -1.2109, -1.2422, -1.2500],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [-1.5000, -1.5000, -1.5156,  ..., -1.0391, -1.2266, -1.4062],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         ...,
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.4609,  1.4453,  1.4609,  ...,  0.6367,  0.6250,  0.5977],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.5391,  1.5234,  1.5078,  ...,  0.9102,  0.9102,  0.9375],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]         [ 1.4766,  1.4453,  1.4453,  ...,  0.8359,  0.8242,  0.7969]],
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [dump_input.py:76]        dtype=torch.bfloat16), field=MultiModalFlatField(slices=[[slice(0, 1344, None)], [slice(1344, 2688, None)]], dim=0)), 'image_grid_thw': MultiModalFieldElem(modality='image', key='image_grid_thw', data=tensor([ 1, 32, 42]), field=MultiModalBatchedField())}, modality='image', identifier='ef8cb79179f55875020bd0dd4775a8ddba17f51a2cc7c1696688ba35cb1d238d', mm_position=PlaceholderRange(offset=353, length=336, is_embed=None))],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.95, top_p=0.7, top_k=50, min_p=0.0, seed=None, stop=[], stop_token_ids=[151643, 151645], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8615, 8616, 8617],),num_computed_tokens=688,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=['41', '42', '44', '91', '92', '93', '94', '95', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '109', '169', '170', '172', '173', '174', '176', '177', '178', '180', '184', '185', '241', '242', '244', '246', '248', '249', '250', '252', '255', '256', '257', '258', '261', '263', '264', '265', '269', '270', '271', '273', '274', '275', '276', '281', '282', '283', '284', '285', '286', '288'], resumed_from_preemption=[false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], new_token_ids=[[11], [1311], [11], [21], [20], [220], [15097], [11], [17], [330], [58456], [73594], [1633], [220], [330], [508], [9099], [504], [17], [18], [20], [279], [330], [11], [11], [279], [67], [11], [374], [15097], [13], [73594], [374], [624], [374], [374], [374], [12305], [518], [12305], [15097], [16282], [279], [374], [374], [374], [448], [624], [16282], [553], [553], [553], [553], [374], [315], [304], [374], [304], [304], [1633]], new_block_ids=[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, [[8598]], null, null, [[8599]], [[8600]], null, [[8601]], [[8602]], null, [[8603]], [[8604]], [[8605]], null, null, [[8606]], null, [[8607]], null, null, null, null, null, null, null, null, [[8608]], null, null, null, null, [[8609]], null, [[8610]], null, [[8611]], [[8612]], [[8613]], [[8614]], null], num_computed_tokens=[756, 756, 756, 755, 755, 754, 753, 755, 754, 754, 755, 753, 754, 755, 755, 753, 754, 755, 755, 754, 754, 752, 754, 754, 752, 752, 754, 752, 752, 754, 752, 752, 752, 753, 753, 752, 751, 752, 753, 743, 740, 742, 739, 738, 739, 739, 736, 737, 738, 737, 735, 736, 737, 736, 734, 736, 736, 736, 736, 733]), num_scheduled_tokens={285: 1, 256: 1, 106: 1, 288: 1, 180: 1, 255: 1, 275: 1, 91: 1, 41: 1, 263: 1, 249: 1, 276: 1, 99: 1, 274: 1, 92: 1, 246: 1, 178: 1, 265: 1, 100: 1, 104: 1, 273: 1, 269: 1, 261: 1, 173: 1, 281: 1, 101: 1, 270: 1, 283: 1, 172: 1, 95: 1, 264: 1, 103: 1, 176: 1, 177: 1, 44: 1, 244: 1, 305: 46, 241: 1, 258: 1, 286: 1, 282: 1, 174: 1, 107: 1, 252: 1, 42: 1, 169: 1, 284: 1, 185: 1, 257: 1, 105: 1, 271: 1, 102: 1, 93: 1, 94: 1, 248: 1, 242: 1, 109: 1, 170: 1, 98: 1, 184: 1, 250: 1}, total_num_scheduled_tokens=106, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0], finished_req_ids=['262'], free_encoder_mm_hashes=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710] EngineCore encountered a fatal error.
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 701, in run_engine_core
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     engine_core.run_busy_loop()
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 728, in run_busy_loop
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     self._process_engine_step()
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 754, in _process_engine_step
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     outputs, model_executed = self.step_fn()
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]                               ^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 346, in step_with_batch_queue
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     model_output = self.execute_model_with_error_logging(
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 270, in execute_model_with_error_logging
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     raise err
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 261, in execute_model_with_error_logging
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     return model_fn(scheduler_output)
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 347, in <lambda>
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     lambda _: future.result(), scheduler_output)
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]               ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     return self.__get_result()
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     raise self._exception
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     result = self.fn(*self.args, **self.kwargs)
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 244, in get_response
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     status, result = w.worker_response_mq.dequeue(
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 511, in dequeue
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     with self.acquire_read(timeout, cancel, indefinite) as buf:
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/contextlib.py", line 137, in __enter__
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     return next(self.gen)
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]            ^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 455, in acquire_read
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710]     raise RuntimeError("cancelled")
[1;36m(EngineCore_DP0 pid=3708410)[0;0m ERROR 10-23 14:12:52 [core.py:710] RuntimeError: cancelled

============================= JOB FEEDBACK =============================

Job ID: 3592469
Cluster: hk
User/Group: bm3844/hk-project-sustainebot
Account: hk-project-p0024638
State: FAILED (exit code 1)
Partition: accelerated
Nodes: 1
Cores per node: 64
Nodelist: hkn0425
CPU Utilized: 00:11:15
CPU Efficiency: 3.88% of 04:50:08 core-walltime
Job Wall-clock time: 00:04:32
Starttime: Thu Oct 23 14:08:23 2025
Endtime: Thu Oct 23 14:12:55 2025
Memory Utilized: 17.61 GB
Memory Efficiency: 3.63% of 485.84 GB (485.84 GB/node)
Energy Consumed: 146966 Joule / 40.8238888888889 Watthours
Average node power draw: 540.316176470588 Watt

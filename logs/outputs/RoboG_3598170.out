Using config: examples/train_full/qwen3vl/qwen3vl_roboG_poc_box_qwen_only_video.yaml
[INFO|2025-10-25 01:58:30] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:35859
[WARNING|2025-10-25 01:58:44] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-25 01:58:52,488] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-25 01:58:52,489] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-25 01:58:52,489] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-25 01:58:52,489] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.
Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.Warning: sentence-transformers not installed, falling back to simple string similarity for LabelEvaluator. Install with `pip install sentence-transformers` for better results.

[INFO|2025-10-25 01:59:40] llamafactory.hparams.parser:426 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-25 01:59:40] llamafactory.hparams.parser:426 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-25 01:59:40] llamafactory.hparams.parser:426 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-25 01:59:40] llamafactory.hparams.parser:426 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-25 01:59:43] llamafactory.data.loader:143 >> Loading dataset /home/hk-project-sustainebot/bm3844/datasets/datasets/robogrounder/qwen3-vl/roboG_stagepoc_grounding_only_video_train.jsonl...
[INFO|2025-10-25 02:00:03] llamafactory.data.loader:143 >> Loading dataset /home/hk-project-sustainebot/bm3844/datasets/datasets/robogrounder/qwen3-vl/roboG_stagepoc_grounding_only_video_eval.jsonl...
training example:
input_ids:
[151644, 872, 198, 27, 15, 13, 17, 6486, 29, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 758, 279, 2766, 11, 264, 12305, 54215, 458, 1633, 13, 9258, 279, 2383, 323, 13934, 315, 279, 2856, 3728, 315, 279, 16282, 291, 1633, 304, 4718, 3561, 13, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 23, 24, 11, 220, 16, 19, 15, 11, 220, 19, 23, 24, 11, 220, 17, 24, 23, 1125, 330, 1502, 788, 330, 32578, 6375, 16707, 921, 73594, 151645, 198]
inputs:
<|im_start|>user
<0.2 seconds><|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|> In the video, a robot manipulated an object. Output the label and coordinates of the initial location of the interacted object in JSON format.<|im_end|>
<|im_start|>assistant
```json
[
{"bbox_2d": [289, 140, 489, 298], "label": "clothes"}
]
```<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 17, 23, 24, 11, 220, 16, 19, 15, 11, 220, 19, 23, 24, 11, 220, 17, 24, 23, 1125, 330, 1502, 788, 330, 32578, 6375, 16707, 921, 73594, 151645, 198]
labels:
```json
[
{"bbox_2d": [289, 140, 489, 298], "label": "clothes"}
]
```<|im_end|>

eval example:
input_ids:
[151644, 872, 198, 27, 15, 13, 17, 6486, 29, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 758, 279, 2766, 11, 264, 12305, 54215, 458, 1633, 13, 9258, 279, 2383, 323, 13934, 315, 279, 2856, 3728, 315, 279, 16282, 291, 1633, 304, 4718, 3561, 13, 151645, 198, 151644, 77091, 198, 785, 1633, 54215, 553, 279, 12305, 374, 279, 18575, 1633, 624, 11445, 2856, 3728, 374, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 16, 22, 21, 11, 220, 17, 18, 15, 11, 220, 17, 16, 24, 11, 220, 17, 22, 15, 1125, 330, 1502, 788, 330, 34164, 1633, 16707, 921, 73594, 151645, 198]
inputs:
<|im_start|>user
<0.2 seconds><|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|> In the video, a robot manipulated an object. Output the label and coordinates of the initial location of the interacted object in JSON format.<|im_end|>
<|im_start|>assistant
The object manipulated by the robot is the orange object.
 Its initial location is:
```json
[
{"bbox_2d": [176, 230, 219, 270], "label": "orange object"}
]
```<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 1633, 54215, 553, 279, 12305, 374, 279, 18575, 1633, 624, 11445, 2856, 3728, 374, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 16, 22, 21, 11, 220, 17, 18, 15, 11, 220, 17, 16, 24, 11, 220, 17, 22, 15, 1125, 330, 1502, 788, 330, 34164, 1633, 16707, 921, 73594, 151645, 198]
labels:
The object manipulated by the robot is the orange object.
 Its initial location is:
```json
[
{"bbox_2d": [176, 230, 219, 270], "label": "orange object"}
]
```<|im_end|>

[INFO|2025-10-25 02:03:21] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-10-25 02:03:36] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-10-25 02:03:36] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-10-25 02:03:36] llamafactory.model.adapter:143 >> Pure bf16 / BAdam detected, remaining trainable params in half precision.
[INFO|2025-10-25 02:03:36] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2025-10-25 02:03:36] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-10-25 02:03:36] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
[INFO|2025-10-25 02:03:36] llamafactory.model.loader:143 >> trainable params: 4,106,660,864 || all params: 4,437,815,808 || trainable%: 92.5379
[WARNING|2025-10-25 02:03:36] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mQwen/Qwen3-VL-4B-Instruct_roboG_stagepoc_grounding_only_video_train_sft[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251025_020338-cxkqtmjw/logs[0m

============================= JOB FEEDBACK =============================

Job ID: 3598170
Cluster: hk
User/Group: bm3844/hk-project-sustainebot
Account: hk-project-p0024638
State: FAILED (exit code 1)
Partition: accelerated
Nodes: 1
Cores per node: 64
Nodelist: hkn0405
CPU Utilized: 02:23:33
CPU Efficiency: 30.94% of 07:44:00 core-walltime
Job Wall-clock time: 00:07:15
Starttime: Sat Oct 25 01:56:46 2025
Endtime: Sat Oct 25 02:04:01 2025
Memory Utilized: 34.05 GB
Memory Efficiency: 7.01% of 485.84 GB (485.84 GB/node)
Energy Consumed: 274296 Joule / 76.1933333333333 Watthours
Average node power draw: 630.565517241379 Watt

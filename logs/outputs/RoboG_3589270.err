GpuFreq=control_disabled
[W1021 16:30:36.243907344 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 16:30:36.245569579 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 16:30:36.249739213 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1021 16:30:36.250966022 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,191 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:38,192 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 16:30:38,346 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-10-21 16:30:38,824 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-10-21 16:30:39,062 >> loading configuration file preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-10-21 16:30:39,069 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file vocab.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file merges.txt from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file tokenizer.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file tokenizer_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-10-21 16:30:39,462 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-10-21 16:30:39,614 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-10-21 16:30:39,987 >> loading configuration file video_preprocessor_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-10-21 16:30:39,988 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1021 16:30:40.090804602 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1021 16:30:40.104681688 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1021 16:30:40.323746213 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1116] 2025-10-21 16:30:41,057 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-10-21 16:30:41,344 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-4B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

Converting format of dataset (num_proc=128):   0%|          | 0/56026 [00:00<?, ? examples/s]Converting format of dataset (num_proc=128):   0%|          | 19/56026 [00:00<05:05, 183.42 examples/s]Converting format of dataset (num_proc=128):   1%|▏         | 770/56026 [00:00<00:12, 4426.06 examples/s]Converting format of dataset (num_proc=128):   3%|▎         | 1422/56026 [00:00<00:10, 5357.32 examples/s]Converting format of dataset (num_proc=128):   4%|▎         | 2067/56026 [00:00<00:09, 5776.71 examples/s]Converting format of dataset (num_proc=128):   5%|▍         | 2690/56026 [00:00<00:08, 5930.80 examples/s]Converting format of dataset (num_proc=128):   6%|▌         | 3328/56026 [00:00<00:08, 6077.67 examples/s]Converting format of dataset (num_proc=128):   7%|▋         | 3969/56026 [00:00<00:08, 6176.64 examples/s]Converting format of dataset (num_proc=128):   8%|▊         | 4622/56026 [00:00<00:08, 6287.50 examples/s]Converting format of dataset (num_proc=128):   9%|▉         | 5254/56026 [00:00<00:08, 6262.61 examples/s]Converting format of dataset (num_proc=128):  11%|█         | 5885/56026 [00:01<00:08, 6254.04 examples/s]Converting format of dataset (num_proc=128):  12%|█▏        | 6533/56026 [00:01<00:07, 6314.93 examples/s]Converting format of dataset (num_proc=128):  13%|█▎        | 7193/56026 [00:01<00:07, 6399.59 examples/s]Converting format of dataset (num_proc=128):  14%|█▍        | 7836/56026 [00:01<00:07, 6373.28 examples/s]Converting format of dataset (num_proc=128):  15%|█▌        | 8475/56026 [00:01<00:07, 6339.60 examples/s]Converting format of dataset (num_proc=128):  16%|█▋        | 9122/56026 [00:01<00:07, 6371.14 examples/s]Converting format of dataset (num_proc=128):  17%|█▋        | 9762/56026 [00:01<00:07, 6281.60 examples/s]Converting format of dataset (num_proc=128):  19%|█▊        | 10406/56026 [00:01<00:07, 6326.12 examples/s]Converting format of dataset (num_proc=128):  20%|█▉        | 11061/56026 [00:01<00:07, 6392.13 examples/s]Converting format of dataset (num_proc=128):  21%|██        | 11720/56026 [00:01<00:06, 6439.46 examples/s]Converting format of dataset (num_proc=128):  22%|██▏       | 12366/56026 [00:02<00:06, 6384.18 examples/s]Converting format of dataset (num_proc=128):  23%|██▎       | 13007/56026 [00:02<00:06, 6367.84 examples/s]Converting format of dataset (num_proc=128):  24%|██▍       | 13645/56026 [00:02<00:06, 6330.31 examples/s]Converting format of dataset (num_proc=128):  26%|██▌       | 14288/56026 [00:02<00:06, 6358.03 examples/s]Converting format of dataset (num_proc=128):  27%|██▋       | 14925/56026 [00:02<00:06, 6334.79 examples/s]Converting format of dataset (num_proc=128):  28%|██▊       | 15559/56026 [00:02<00:06, 6327.50 examples/s]Converting format of dataset (num_proc=128):  29%|██▉       | 16201/56026 [00:02<00:06, 6351.38 examples/s]Converting format of dataset (num_proc=128):  30%|███       | 16860/56026 [00:02<00:06, 6405.59 examples/s]Converting format of dataset (num_proc=128):  31%|███▏      | 17516/56026 [00:02<00:05, 6439.68 examples/s]Converting format of dataset (num_proc=128):  32%|███▏      | 18161/56026 [00:02<00:05, 6330.07 examples/s]Converting format of dataset (num_proc=128):  34%|███▎      | 18837/56026 [00:03<00:05, 6455.67 examples/s]Converting format of dataset (num_proc=128):  35%|███▍      | 19486/56026 [00:03<00:05, 6382.50 examples/s]Converting format of dataset (num_proc=128):  36%|███▌      | 20125/56026 [00:03<00:05, 6369.89 examples/s]Converting format of dataset (num_proc=128):  37%|███▋      | 20799/56026 [00:03<00:05, 6469.84 examples/s]Converting format of dataset (num_proc=128):  38%|███▊      | 21447/56026 [00:03<00:05, 6424.06 examples/s]Converting format of dataset (num_proc=128):  39%|███▉      | 22093/56026 [00:03<00:05, 6433.15 examples/s]Converting format of dataset (num_proc=128):  41%|████      | 22737/56026 [00:03<00:05, 6407.39 examples/s]Converting format of dataset (num_proc=128):  42%|████▏     | 23379/56026 [00:03<00:05, 6363.94 examples/s]Converting format of dataset (num_proc=128):  43%|████▎     | 24029/56026 [00:03<00:05, 6388.90 examples/s]Converting format of dataset (num_proc=128):  44%|████▍     | 24671/56026 [00:03<00:04, 6387.65 examples/s]Converting format of dataset (num_proc=128):  45%|████▌     | 25317/56026 [00:04<00:04, 6398.66 examples/s]Converting format of dataset (num_proc=128):  46%|████▋     | 25969/56026 [00:04<00:04, 6425.29 examples/s]Converting format of dataset (num_proc=128):  48%|████▊     | 26620/56026 [00:04<00:04, 6432.97 examples/s]Converting format of dataset (num_proc=128):  49%|████▊     | 27266/56026 [00:04<00:04, 6425.86 examples/s]Converting format of dataset (num_proc=128):  50%|████▉     | 27909/56026 [00:04<00:04, 6336.69 examples/s]Converting format of dataset (num_proc=128):  51%|█████     | 28559/56026 [00:04<00:04, 6375.74 examples/s]Converting format of dataset (num_proc=128):  52%|█████▏    | 29209/56026 [00:04<00:04, 6412.55 examples/s]Converting format of dataset (num_proc=128):  53%|█████▎    | 29853/56026 [00:04<00:04, 6419.55 examples/s]Converting format of dataset (num_proc=128):  54%|█████▍    | 30510/56026 [00:04<00:03, 6461.18 examples/s]Converting format of dataset (num_proc=128):  56%|█████▌    | 31158/56026 [00:04<00:03, 6382.46 examples/s]Converting format of dataset (num_proc=128):  57%|█████▋    | 31818/56026 [00:05<00:03, 6439.86 examples/s]Converting format of dataset (num_proc=128):  58%|█████▊    | 32466/56026 [00:05<00:03, 6400.31 examples/s]Converting format of dataset (num_proc=128):  59%|█████▉    | 33107/56026 [00:05<00:03, 6397.71 examples/s]Converting format of dataset (num_proc=128):  60%|██████    | 33749/56026 [00:05<00:03, 6392.86 examples/s]Converting format of dataset (num_proc=128):  61%|██████▏   | 34407/56026 [00:05<00:03, 6443.90 examples/s]Converting format of dataset (num_proc=128):  63%|██████▎   | 35052/56026 [00:05<00:03, 6277.67 examples/s]Converting format of dataset (num_proc=128):  64%|██████▍   | 35743/56026 [00:05<00:03, 6459.71 examples/s]Converting format of dataset (num_proc=128):  65%|██████▍   | 36391/56026 [00:05<00:03, 6363.80 examples/s]Converting format of dataset (num_proc=128):  66%|██████▌   | 37030/56026 [00:05<00:03, 6207.55 examples/s]Converting format of dataset (num_proc=128):  67%|██████▋   | 37661/56026 [00:05<00:02, 6223.43 examples/s]Converting format of dataset (num_proc=128):  68%|██████▊   | 38359/56026 [00:06<00:02, 6443.39 examples/s]Converting format of dataset (num_proc=128):  70%|██████▉   | 39008/56026 [00:06<00:02, 6326.30 examples/s]Converting format of dataset (num_proc=128):  71%|███████   | 39647/56026 [00:06<00:02, 6318.41 examples/s]Converting format of dataset (num_proc=128):  72%|███████▏  | 40287/56026 [00:06<00:02, 6340.57 examples/s]Converting format of dataset (num_proc=128):  73%|███████▎  | 40924/56026 [00:06<00:02, 6339.90 examples/s]Converting format of dataset (num_proc=128):  74%|███████▍  | 41581/56026 [00:06<00:02, 6398.41 examples/s]Converting format of dataset (num_proc=128):  75%|███████▌  | 42224/56026 [00:06<00:02, 6402.51 examples/s]Converting format of dataset (num_proc=128):  77%|███████▋  | 42868/56026 [00:06<00:02, 6382.31 examples/s]Converting format of dataset (num_proc=128):  78%|███████▊  | 43508/56026 [00:06<00:02, 6198.27 examples/s]Converting format of dataset (num_proc=128):  79%|███████▉  | 44165/56026 [00:07<00:01, 6295.42 examples/s]Converting format of dataset (num_proc=128):  80%|████████  | 44837/56026 [00:07<00:01, 6416.96 examples/s]Converting format of dataset (num_proc=128):  81%|████████  | 45480/56026 [00:07<00:01, 6365.87 examples/s]Converting format of dataset (num_proc=128):  82%|████████▏ | 46132/56026 [00:07<00:01, 6399.86 examples/s]Converting format of dataset (num_proc=128):  84%|████████▎ | 46789/56026 [00:07<00:01, 6442.14 examples/s]Converting format of dataset (num_proc=128):  85%|████████▍ | 47434/56026 [00:07<00:01, 6362.82 examples/s]Converting format of dataset (num_proc=128):  86%|████████▌ | 48092/56026 [00:07<00:01, 6424.50 examples/s]Converting format of dataset (num_proc=128):  87%|████████▋ | 48736/56026 [00:07<00:01, 6421.08 examples/s]Converting format of dataset (num_proc=128):  88%|████████▊ | 49382/56026 [00:07<00:01, 6424.63 examples/s]Converting format of dataset (num_proc=128):  89%|████████▉ | 50037/56026 [00:07<00:00, 6442.65 examples/s]Converting format of dataset (num_proc=128):  90%|█████████ | 50684/56026 [00:08<00:00, 6352.44 examples/s]Converting format of dataset (num_proc=128):  92%|█████████▏| 51333/56026 [00:08<00:00, 6388.15 examples/s]Converting format of dataset (num_proc=128):  93%|█████████▎| 51985/56026 [00:08<00:00, 6426.21 examples/s]Converting format of dataset (num_proc=128):  94%|█████████▍| 52630/56026 [00:08<00:00, 6384.32 examples/s]Converting format of dataset (num_proc=128):  95%|█████████▌| 53289/56026 [00:08<00:00, 6443.48 examples/s]Converting format of dataset (num_proc=128):  96%|█████████▋| 53937/56026 [00:08<00:00, 6433.39 examples/s]Converting format of dataset (num_proc=128):  97%|█████████▋| 54583/56026 [00:08<00:00, 6419.80 examples/s]Converting format of dataset (num_proc=128):  99%|█████████▊| 55289/56026 [00:08<00:00, 6586.19 examples/s]Converting format of dataset (num_proc=128): 100%|█████████▉| 55956/56026 [00:08<00:00, 6585.98 examples/s]Converting format of dataset (num_proc=128): 100%|██████████| 56026/56026 [00:08<00:00, 6266.37 examples/s]
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Converting format of dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Converting format of dataset (num_proc=83):  11%|█         | 9/83 [00:00<00:00, 86.40 examples/s]Converting format of dataset (num_proc=83): 100%|██████████| 83/83 [00:00<00:00, 295.85 examples/s]
/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1021 16:30:55.988825621 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=128):   0%|          | 0/56026 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=128):   1%|          | 438/56026 [00:21<45:24, 20.41 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 876/56026 [00:28<26:48, 34.29 examples/s]Running tokenizer on dataset (num_proc=128):   2%|▏         | 1314/56026 [00:28<15:04, 60.50 examples/s]Running tokenizer on dataset (num_proc=128):   3%|▎         | 1752/56026 [00:30<10:41, 84.60 examples/s]Running tokenizer on dataset (num_proc=128):   4%|▍         | 2190/56026 [00:38<12:45, 70.36 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▍         | 2628/56026 [00:39<08:26, 105.48 examples/s]Running tokenizer on dataset (num_proc=128):   5%|▌         | 3066/56026 [00:41<07:13, 122.14 examples/s]Running tokenizer on dataset (num_proc=128):   6%|▋         | 3504/56026 [00:48<09:33, 91.57 examples/s] Running tokenizer on dataset (num_proc=128):   7%|▋         | 3941/56026 [00:49<07:09, 121.13 examples/s]Running tokenizer on dataset (num_proc=128):   8%|▊         | 4379/56026 [00:50<05:20, 161.17 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▊         | 4817/56026 [00:51<04:04, 209.83 examples/s]Running tokenizer on dataset (num_proc=128):   9%|▉         | 5255/56026 [00:52<03:36, 234.56 examples/s]Running tokenizer on dataset (num_proc=128):  10%|█         | 5693/56026 [00:52<02:36, 320.69 examples/s]Running tokenizer on dataset (num_proc=128):  11%|█         | 6131/56026 [00:54<02:39, 312.94 examples/s]Running tokenizer on dataset (num_proc=128):  12%|█▏        | 6569/56026 [00:55<02:22, 346.62 examples/s]Running tokenizer on dataset (num_proc=128):  13%|█▎        | 7006/56026 [00:55<01:52, 436.27 examples/s]Running tokenizer on dataset (num_proc=128):  14%|█▍        | 7882/56026 [00:56<01:10, 684.79 examples/s]Running tokenizer on dataset (num_proc=128):  15%|█▍        | 8320/56026 [00:57<01:21, 583.16 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▌        | 8758/56026 [00:57<01:16, 621.93 examples/s]Running tokenizer on dataset (num_proc=128):  16%|█▋        | 9195/56026 [00:58<01:22, 569.82 examples/s]Running tokenizer on dataset (num_proc=128):  17%|█▋        | 9633/56026 [00:59<01:13, 631.15 examples/s]Running tokenizer on dataset (num_proc=128):  18%|█▊        | 10071/56026 [00:59<01:10, 653.24 examples/s]Running tokenizer on dataset (num_proc=128):  19%|█▉        | 10509/56026 [00:59<00:54, 836.84 examples/s]Running tokenizer on dataset (num_proc=128):  20%|█▉        | 10946/56026 [01:00<00:58, 764.33 examples/s]Running tokenizer on dataset (num_proc=128):  21%|██        | 11822/56026 [01:00<00:39, 1128.85 examples/s]Running tokenizer on dataset (num_proc=128):  22%|██▏       | 12260/56026 [01:01<00:36, 1189.25 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 12698/56026 [01:01<00:32, 1330.05 examples/s]Running tokenizer on dataset (num_proc=128):  23%|██▎       | 13135/56026 [01:03<01:10, 604.31 examples/s] Running tokenizer on dataset (num_proc=128):  24%|██▍       | 13573/56026 [01:04<01:12, 584.82 examples/s]Running tokenizer on dataset (num_proc=128):  25%|██▌       | 14011/56026 [01:05<01:18, 532.57 examples/s]Running tokenizer on dataset (num_proc=128):  26%|██▌       | 14449/56026 [01:05<01:02, 663.42 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 14887/56026 [01:08<01:59, 344.55 examples/s]Running tokenizer on dataset (num_proc=128):  27%|██▋       | 15325/56026 [01:08<01:32, 437.71 examples/s]Running tokenizer on dataset (num_proc=128):  28%|██▊       | 15762/56026 [01:08<01:09, 583.27 examples/s]Running tokenizer on dataset (num_proc=128):  29%|██▉       | 16200/56026 [01:09<01:00, 653.33 examples/s]Running tokenizer on dataset (num_proc=128):  30%|██▉       | 16638/56026 [01:09<00:58, 670.81 examples/s]Running tokenizer on dataset (num_proc=128):  30%|███       | 17076/56026 [01:10<01:09, 559.58 examples/s]Running tokenizer on dataset (num_proc=128):  31%|███▏      | 17514/56026 [01:10<00:51, 742.39 examples/s]Running tokenizer on dataset (num_proc=128):  32%|███▏      | 17952/56026 [01:11<00:45, 845.18 examples/s]Running tokenizer on dataset (num_proc=128):  33%|███▎      | 18390/56026 [01:11<00:40, 930.34 examples/s]Running tokenizer on dataset (num_proc=128):  34%|███▎      | 18828/56026 [01:11<00:32, 1159.98 examples/s]Running tokenizer on dataset (num_proc=128):  35%|███▌      | 19704/56026 [01:12<00:36, 991.09 examples/s] Running tokenizer on dataset (num_proc=128):  36%|███▌      | 20142/56026 [01:13<00:33, 1072.18 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21018/56026 [01:13<00:22, 1555.48 examples/s]Running tokenizer on dataset (num_proc=128):  38%|███▊      | 21456/56026 [01:13<00:21, 1599.69 examples/s]Running tokenizer on dataset (num_proc=128):  40%|███▉      | 22331/56026 [01:14<00:27, 1219.98 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████      | 22768/56026 [01:14<00:26, 1232.43 examples/s]Running tokenizer on dataset (num_proc=128):  41%|████▏     | 23206/56026 [01:15<00:23, 1372.48 examples/s]Running tokenizer on dataset (num_proc=128):  42%|████▏     | 23644/56026 [01:15<00:33, 964.39 examples/s] Running tokenizer on dataset (num_proc=128):  43%|████▎     | 24082/56026 [01:16<00:28, 1118.43 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▍     | 24958/56026 [01:17<00:29, 1046.18 examples/s]Running tokenizer on dataset (num_proc=128):  45%|████▌     | 25396/56026 [01:17<00:24, 1272.52 examples/s]Running tokenizer on dataset (num_proc=128):  46%|████▌     | 25833/56026 [01:17<00:19, 1523.87 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 26709/56026 [01:17<00:13, 2099.26 examples/s]Running tokenizer on dataset (num_proc=128):  48%|████▊     | 27146/56026 [01:17<00:12, 2234.91 examples/s]Running tokenizer on dataset (num_proc=128):  49%|████▉     | 27584/56026 [01:18<00:20, 1386.77 examples/s]Running tokenizer on dataset (num_proc=128):  51%|█████     | 28460/56026 [01:18<00:12, 2122.07 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 28898/56026 [01:18<00:11, 2289.91 examples/s]Running tokenizer on dataset (num_proc=128):  52%|█████▏    | 29336/56026 [01:18<00:14, 1835.83 examples/s]Running tokenizer on dataset (num_proc=128):  53%|█████▎    | 29773/56026 [01:20<00:32, 810.38 examples/s] Running tokenizer on dataset (num_proc=128):  54%|█████▍    | 30211/56026 [01:20<00:26, 968.29 examples/s]Running tokenizer on dataset (num_proc=128):  55%|█████▌    | 31086/56026 [01:20<00:16, 1552.64 examples/s]Running tokenizer on dataset (num_proc=128):  56%|█████▋    | 31524/56026 [01:21<00:17, 1366.15 examples/s]Running tokenizer on dataset (num_proc=128):  58%|█████▊    | 32400/56026 [01:21<00:14, 1597.78 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▊    | 32838/56026 [01:21<00:12, 1843.68 examples/s]Running tokenizer on dataset (num_proc=128):  59%|█████▉    | 33276/56026 [01:22<00:19, 1166.31 examples/s]Running tokenizer on dataset (num_proc=128):  60%|██████    | 33713/56026 [01:22<00:16, 1384.50 examples/s]Running tokenizer on dataset (num_proc=128):  62%|██████▏   | 34587/56026 [01:23<00:17, 1191.83 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35025/56026 [01:23<00:14, 1414.59 examples/s]Running tokenizer on dataset (num_proc=128):  63%|██████▎   | 35463/56026 [01:23<00:13, 1571.18 examples/s]Running tokenizer on dataset (num_proc=128):  64%|██████▍   | 35901/56026 [01:23<00:11, 1790.46 examples/s]Running tokenizer on dataset (num_proc=128):  65%|██████▍   | 36339/56026 [01:24<00:13, 1455.74 examples/s]Running tokenizer on dataset (num_proc=128):  66%|██████▌   | 36777/56026 [01:24<00:12, 1601.12 examples/s]Running tokenizer on dataset (num_proc=128):  68%|██████▊   | 38090/56026 [01:25<00:08, 2027.59 examples/s]Running tokenizer on dataset (num_proc=128):  70%|██████▉   | 38966/56026 [01:25<00:08, 2107.92 examples/s]Running tokenizer on dataset (num_proc=128):  71%|███████   | 39842/56026 [01:25<00:07, 2123.15 examples/s]Running tokenizer on dataset (num_proc=128):  73%|███████▎  | 40718/56026 [01:26<00:05, 2631.46 examples/s]Running tokenizer on dataset (num_proc=128):  74%|███████▍  | 41593/56026 [01:26<00:07, 1812.06 examples/s]Running tokenizer on dataset (num_proc=128):  76%|███████▌  | 42467/56026 [01:27<00:07, 1908.81 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 42905/56026 [01:27<00:06, 1954.38 examples/s]Running tokenizer on dataset (num_proc=128):  77%|███████▋  | 43342/56026 [01:27<00:07, 1646.90 examples/s]Running tokenizer on dataset (num_proc=128):  78%|███████▊  | 43779/56026 [01:28<00:08, 1407.44 examples/s]Running tokenizer on dataset (num_proc=128):  79%|███████▉  | 44216/56026 [01:28<00:09, 1302.85 examples/s]Running tokenizer on dataset (num_proc=128):  81%|████████▏ | 45528/56026 [01:28<00:04, 2371.63 examples/s]Running tokenizer on dataset (num_proc=128):  82%|████████▏ | 45966/56026 [01:29<00:03, 2531.41 examples/s]Running tokenizer on dataset (num_proc=128):  83%|████████▎ | 46404/56026 [01:29<00:04, 2259.03 examples/s]Running tokenizer on dataset (num_proc=128):  85%|████████▌ | 47717/56026 [01:29<00:02, 3713.99 examples/s]Running tokenizer on dataset (num_proc=128):  87%|████████▋ | 48592/56026 [01:29<00:02, 3117.33 examples/s]Running tokenizer on dataset (num_proc=128):  88%|████████▊ | 49468/56026 [01:30<00:02, 2790.90 examples/s]Running tokenizer on dataset (num_proc=128):  90%|████████▉ | 50343/56026 [01:30<00:01, 3080.48 examples/s]Running tokenizer on dataset (num_proc=128):  91%|█████████ | 50780/56026 [01:30<00:01, 2681.52 examples/s]Running tokenizer on dataset (num_proc=128):  93%|█████████▎| 52093/56026 [01:30<00:01, 3220.11 examples/s]Running tokenizer on dataset (num_proc=128):  95%|█████████▍| 52967/56026 [01:31<00:00, 3753.17 examples/s]Running tokenizer on dataset (num_proc=128):  96%|█████████▌| 53841/56026 [01:31<00:00, 4386.79 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 54715/56026 [01:31<00:00, 2812.17 examples/s]Running tokenizer on dataset (num_proc=128):  98%|█████████▊| 55152/56026 [01:31<00:00, 2669.05 examples/s]Running tokenizer on dataset (num_proc=128):  99%|█████████▉| 55589/56026 [01:32<00:00, 2043.19 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56026/56026 [01:32<00:00, 1982.70 examples/s]Running tokenizer on dataset (num_proc=128): 100%|██████████| 56026/56026 [01:32<00:00, 604.18 examples/s] 
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
Running tokenizer on dataset (num_proc=83):   0%|          | 0/83 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=83):   1%|          | 1/83 [00:00<00:34,  2.38 examples/s]Running tokenizer on dataset (num_proc=83):   4%|▎         | 3/83 [00:00<00:12,  6.51 examples/s]Running tokenizer on dataset (num_proc=83):   6%|▌         | 5/83 [00:00<00:08,  9.53 examples/s]Running tokenizer on dataset (num_proc=83):   8%|▊         | 7/83 [00:00<00:06, 11.57 examples/s]Running tokenizer on dataset (num_proc=83):  11%|█         | 9/83 [00:00<00:05, 12.88 examples/s]Running tokenizer on dataset (num_proc=83):  13%|█▎        | 11/83 [00:01<00:05, 14.28 examples/s]Running tokenizer on dataset (num_proc=83):  16%|█▌        | 13/83 [00:01<00:04, 14.84 examples/s]Running tokenizer on dataset (num_proc=83):  18%|█▊        | 15/83 [00:01<00:04, 14.99 examples/s]Running tokenizer on dataset (num_proc=83):  22%|██▏       | 18/83 [00:01<00:04, 16.05 examples/s]Running tokenizer on dataset (num_proc=83):  24%|██▍       | 20/83 [00:01<00:03, 15.76 examples/s]Running tokenizer on dataset (num_proc=83):  28%|██▊       | 23/83 [00:01<00:03, 16.00 examples/s]Running tokenizer on dataset (num_proc=83):  31%|███▏      | 26/83 [00:01<00:03, 16.65 examples/s]Running tokenizer on dataset (num_proc=83):  34%|███▎      | 28/83 [00:02<00:03, 16.78 examples/s]Running tokenizer on dataset (num_proc=83):  36%|███▌      | 30/83 [00:02<00:03, 16.64 examples/s]Running tokenizer on dataset (num_proc=83):  39%|███▊      | 32/83 [00:02<00:03, 16.94 examples/s]Running tokenizer on dataset (num_proc=83):  41%|████      | 34/83 [00:02<00:02, 16.95 examples/s]Running tokenizer on dataset (num_proc=83):  43%|████▎     | 36/83 [00:02<00:02, 16.31 examples/s]Running tokenizer on dataset (num_proc=83):  46%|████▌     | 38/83 [00:02<00:02, 16.65 examples/s]Running tokenizer on dataset (num_proc=83):  49%|████▉     | 41/83 [00:02<00:02, 18.66 examples/s]Running tokenizer on dataset (num_proc=83):  52%|█████▏    | 43/83 [00:02<00:02, 16.36 examples/s]Running tokenizer on dataset (num_proc=83):  54%|█████▍    | 45/83 [00:03<00:02, 16.63 examples/s]Running tokenizer on dataset (num_proc=83):  57%|█████▋    | 47/83 [00:03<00:02, 16.33 examples/s]Running tokenizer on dataset (num_proc=83):  60%|██████    | 50/83 [00:03<00:01, 18.48 examples/s]Running tokenizer on dataset (num_proc=83):  63%|██████▎   | 52/83 [00:03<00:01, 16.16 examples/s]Running tokenizer on dataset (num_proc=83):  65%|██████▌   | 54/83 [00:03<00:01, 16.72 examples/s]Running tokenizer on dataset (num_proc=83):  67%|██████▋   | 56/83 [00:03<00:01, 16.37 examples/s]Running tokenizer on dataset (num_proc=83):  70%|██████▉   | 58/83 [00:03<00:01, 16.49 examples/s]Running tokenizer on dataset (num_proc=83):  73%|███████▎  | 61/83 [00:04<00:01, 16.81 examples/s]Running tokenizer on dataset (num_proc=83):  76%|███████▌  | 63/83 [00:04<00:01, 16.72 examples/s]Running tokenizer on dataset (num_proc=83):  78%|███████▊  | 65/83 [00:04<00:01, 16.94 examples/s]Running tokenizer on dataset (num_proc=83):  81%|████████  | 67/83 [00:04<00:00, 16.78 examples/s]Running tokenizer on dataset (num_proc=83):  83%|████████▎ | 69/83 [00:04<00:00, 16.55 examples/s]Running tokenizer on dataset (num_proc=83):  86%|████████▌ | 71/83 [00:04<00:00, 16.56 examples/s]Running tokenizer on dataset (num_proc=83):  89%|████████▉ | 74/83 [00:04<00:00, 18.64 examples/s]Running tokenizer on dataset (num_proc=83):  92%|█████████▏| 76/83 [00:04<00:00, 16.15 examples/s]Running tokenizer on dataset (num_proc=83):  95%|█████████▌| 79/83 [00:05<00:00, 17.21 examples/s]Running tokenizer on dataset (num_proc=83):  98%|█████████▊| 81/83 [00:05<00:00, 17.78 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:05<00:00, 17.96 examples/s]Running tokenizer on dataset (num_proc=83): 100%|██████████| 83/83 [00:05<00:00, 15.51 examples/s]
[INFO|configuration_utils.py:765] 2025-10-21 16:32:41,417 >> loading configuration file config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/config.json
[INFO|configuration_utils.py:839] 2025-10-21 16:32:41,420 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 2560,
    "initializer_range": 0.02,
    "intermediate_size": 9728,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "tie_word_embeddings": true,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      5,
      11,
      17
    ],
    "depth": 24,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1024,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 2560,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.
[WARNING|logging.py:328] 2025-10-21 16:32:41,573 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-10-21 16:32:41,574 >> loading weights file model.safetensors from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-10-21 16:32:41,575 >> Instantiating Qwen3VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-10-21 16:32:41,588 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-10-21 16:32:41,592 >> Instantiating Qwen3VLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-10-21 16:32:41,603 >> Instantiating Qwen3VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.49s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.25s/it]
[INFO|configuration_utils.py:941] 2025-10-21 16:33:06,739 >> loading configuration file generation_config.json from cache at /home/hk-project-sustainebot/bm3844/.cache/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/generation_config.json
[INFO|configuration_utils.py:986] 2025-10-21 16:33:06,739 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|dynamic_module_utils.py:423] 2025-10-21 16:33:06,854 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-4B-Instruct.
[WARNING|trainer.py:906] 2025-10-21 16:33:06,875 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-10-21 16:33:06,879 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:982] 2025-10-21 16:33:07,156 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:1335] 2025-10-21 16:33:13,877 >> skipped Embedding(2304, 1024): 2.25M params
[INFO|trainer.py:1335] 2025-10-21 16:33:13,878 >> skipped Embedding(151936, 2560): 373.1875M params
[INFO|trainer.py:1338] 2025-10-21 16:33:13,878 >> skipped: 373.1875M params
[INFO|trainer.py:2519] 2025-10-21 16:33:14,156 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-10-21 16:33:14,156 >>   Num examples = 56,026
[INFO|trainer.py:2521] 2025-10-21 16:33:14,156 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2025-10-21 16:33:14,156 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:2525] 2025-10-21 16:33:14,156 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-10-21 16:33:14,156 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2527] 2025-10-21 16:33:14,156 >>   Total optimization steps = 2,628
[INFO|trainer.py:2528] 2025-10-21 16:33:14,158 >>   Number of trainable parameters = 4,106,660,864
[INFO|integration_utils.py:867] 2025-10-21 16:33:14,159 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: niblank to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /hkfs/home/project/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/wandb/run-20251021_163314-cl74ecu6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ${model_name_or_path}_${dataset}_${stage}
wandb: ⭐️ View project at https://wandb.ai/niblank/llamafactory
wandb: 🚀 View run at https://wandb.ai/niblank/llamafactory/runs/cl74ecu6
  0%|          | 0/2628 [00:00<?, ?it/s]/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:610: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/hk-project-sustainebot/bm3844/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1487.)
  total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
  0%|          | 1/2628 [00:05<4:09:33,  5.70s/it]  0%|          | 2/2628 [00:07<2:32:39,  3.49s/it]  0%|          | 3/2628 [00:09<1:59:24,  2.73s/it]  0%|          | 4/2628 [00:11<1:43:52,  2.38s/it]  0%|          | 5/2628 [00:13<1:34:46,  2.17s/it]  0%|          | 6/2628 [00:14<1:29:35,  2.05s/it]  0%|          | 7/2628 [00:16<1:26:01,  1.97s/it]  0%|          | 8/2628 [00:18<1:23:47,  1.92s/it]  0%|          | 9/2628 [00:20<1:22:08,  1.88s/it]  0%|          | 10/2628 [00:22<1:20:55,  1.85s/it]                                                     0%|          | 10/2628 [00:22<1:20:55,  1.85s/it]  0%|          | 11/2628 [00:23<1:20:28,  1.85s/it]  0%|          | 12/2628 [00:25<1:19:59,  1.83s/it]  0%|          | 13/2628 [00:27<1:19:41,  1.83s/it]  1%|          | 14/2628 [00:29<1:19:18,  1.82s/it]  1%|          | 15/2628 [00:31<1:19:00,  1.81s/it]  1%|          | 16/2628 [00:32<1:18:45,  1.81s/it]  1%|          | 17/2628 [00:34<1:18:38,  1.81s/it]  1%|          | 18/2628 [00:36<1:18:27,  1.80s/it]  1%|          | 19/2628 [00:38<1:18:32,  1.81s/it]  1%|          | 20/2628 [00:40<1:18:22,  1.80s/it]                                                     1%|          | 20/2628 [00:40<1:18:22,  1.80s/it]  1%|          | 21/2628 [00:41<1:18:21,  1.80s/it]  1%|          | 22/2628 [00:43<1:18:12,  1.80s/it]  1%|          | 23/2628 [00:45<1:18:10,  1.80s/it]  1%|          | 24/2628 [00:47<1:18:16,  1.80s/it]  1%|          | 25/2628 [00:49<1:18:22,  1.81s/it]  1%|          | 26/2628 [00:51<1:18:20,  1.81s/it]  1%|          | 27/2628 [00:52<1:18:20,  1.81s/it]  1%|          | 28/2628 [00:54<1:18:28,  1.81s/it]  1%|          | 29/2628 [00:56<1:18:26,  1.81s/it]  1%|          | 30/2628 [00:58<1:18:24,  1.81s/it]                                                     1%|          | 30/2628 [00:58<1:18:24,  1.81s/it]  1%|          | 31/2628 [01:00<1:18:37,  1.82s/it]  1%|          | 32/2628 [01:01<1:18:27,  1.81s/it]  1%|▏         | 33/2628 [01:03<1:18:22,  1.81s/it]  1%|▏         | 34/2628 [01:05<1:18:14,  1.81s/it]  1%|▏         | 35/2628 [01:07<1:18:26,  1.81s/it]  1%|▏         | 36/2628 [01:09<1:18:23,  1.81s/it]  1%|▏         | 37/2628 [01:10<1:18:14,  1.81s/it]  1%|▏         | 38/2628 [01:12<1:18:05,  1.81s/it]  1%|▏         | 39/2628 [01:14<1:18:06,  1.81s/it]  2%|▏         | 40/2628 [01:16<1:18:02,  1.81s/it]                                                     2%|▏         | 40/2628 [01:16<1:18:02,  1.81s/it]  2%|▏         | 41/2628 [01:18<1:17:54,  1.81s/it]  2%|▏         | 42/2628 [01:19<1:17:52,  1.81s/it]  2%|▏         | 43/2628 [01:21<1:17:50,  1.81s/it]  2%|▏         | 44/2628 [01:23<1:17:41,  1.80s/it]  2%|▏         | 45/2628 [01:25<1:17:46,  1.81s/it]  2%|▏         | 46/2628 [01:27<1:17:34,  1.80s/it]  2%|▏         | 47/2628 [01:29<1:17:34,  1.80s/it]  2%|▏         | 48/2628 [01:30<1:17:38,  1.81s/it]  2%|▏         | 49/2628 [01:32<1:17:44,  1.81s/it]  2%|▏         | 50/2628 [01:34<1:17:34,  1.81s/it]                                                     2%|▏         | 50/2628 [01:34<1:17:34,  1.81s/it]  2%|▏         | 51/2628 [01:36<1:17:48,  1.81s/it]  2%|▏         | 52/2628 [01:38<1:17:35,  1.81s/it]  2%|▏         | 53/2628 [01:39<1:17:24,  1.80s/it]  2%|▏         | 54/2628 [01:41<1:17:15,  1.80s/it]  2%|▏         | 55/2628 [01:43<1:17:24,  1.80s/it]  2%|▏         | 56/2628 [01:45<1:17:17,  1.80s/it]  2%|▏         | 57/2628 [01:47<1:17:12,  1.80s/it]  2%|▏         | 58/2628 [01:48<1:17:07,  1.80s/it]  2%|▏         | 59/2628 [01:50<1:17:09,  1.80s/it]  2%|▏         | 60/2628 [01:52<1:17:06,  1.80s/it]                                                     2%|▏         | 60/2628 [01:52<1:17:06,  1.80s/it]  2%|▏         | 61/2628 [01:54<1:17:06,  1.80s/it]  2%|▏         | 62/2628 [01:56<1:16:59,  1.80s/it]  2%|▏         | 63/2628 [01:57<1:16:59,  1.80s/it]  2%|▏         | 64/2628 [01:59<1:17:07,  1.80s/it]  2%|▏         | 65/2628 [02:01<1:17:00,  1.80s/it]  3%|▎         | 66/2628 [02:03<1:16:55,  1.80s/it]  3%|▎         | 67/2628 [02:05<1:16:57,  1.80s/it]  3%|▎         | 68/2628 [02:06<1:16:54,  1.80s/it]  3%|▎         | 69/2628 [02:08<1:16:53,  1.80s/it]  3%|▎         | 70/2628 [02:10<1:16:46,  1.80s/it]                                                     3%|▎         | 70/2628 [02:10<1:16:46,  1.80s/it]  3%|▎         | 71/2628 [02:12<1:16:45,  1.80s/it]  3%|▎         | 72/2628 [02:14<1:16:43,  1.80s/it]  3%|▎         | 73/2628 [02:15<1:16:40,  1.80s/it]  3%|▎         | 74/2628 [02:17<1:16:50,  1.81s/it]  3%|▎         | 75/2628 [02:19<1:16:41,  1.80s/it]  3%|▎         | 76/2628 [02:21<1:16:37,  1.80s/it]  3%|▎         | 77/2628 [02:23<1:16:45,  1.81s/it]  3%|▎         | 78/2628 [02:24<1:16:36,  1.80s/it]  3%|▎         | 79/2628 [02:26<1:16:30,  1.80s/it]  3%|▎         | 80/2628 [02:28<1:16:26,  1.80s/it]                                                     3%|▎         | 80/2628 [02:28<1:16:26,  1.80s/it]  3%|▎         | 81/2628 [02:30<1:16:39,  1.81s/it]  3%|▎         | 82/2628 [02:32<1:16:33,  1.80s/it]  3%|▎         | 83/2628 [02:33<1:16:30,  1.80s/it]  3%|▎         | 84/2628 [02:35<1:16:25,  1.80s/it]  3%|▎         | 85/2628 [02:37<1:16:27,  1.80s/it]  3%|▎         | 86/2628 [02:39<1:16:19,  1.80s/it]  3%|▎         | 87/2628 [02:41<1:16:15,  1.80s/it]  3%|▎         | 88/2628 [02:42<1:16:11,  1.80s/it]  3%|▎         | 89/2628 [02:44<1:16:14,  1.80s/it]  3%|▎         | 90/2628 [02:46<1:16:19,  1.80s/it]                                                     3%|▎         | 90/2628 [02:46<1:16:19,  1.80s/it]  3%|▎         | 91/2628 [02:48<1:16:21,  1.81s/it]  4%|▎         | 92/2628 [02:50<1:16:19,  1.81s/it]  4%|▎         | 93/2628 [02:51<1:16:15,  1.80s/it]  4%|▎         | 94/2628 [02:53<1:16:13,  1.80s/it]  4%|▎         | 95/2628 [02:55<1:16:18,  1.81s/it]  4%|▎         | 96/2628 [02:57<1:16:11,  1.81s/it]  4%|▎         | 97/2628 [02:59<1:16:08,  1.80s/it]  4%|▎         | 98/2628 [03:01<1:16:23,  1.81s/it]  4%|▍         | 99/2628 [03:02<1:16:25,  1.81s/it]  4%|▍         | 100/2628 [03:04<1:16:13,  1.81s/it]                                                      4%|▍         | 100/2628 [03:04<1:16:13,  1.81s/it][INFO|trainer.py:4643] 2025-10-21 16:36:20,620 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-10-21 16:36:20,620 >>   Num examples = 83
[INFO|trainer.py:4648] 2025-10-21 16:36:20,620 >>   Batch size = 8
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[WARNING|utils.py:2443] 2025-10-21 16:36:21,152 >> A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.

  0%|          | 0/3 [00:00<?, ?it/s][AA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[WARNING|utils.py:2443] 2025-10-21 16:36:24,288 >> A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.

 67%|██████▋   | 2/3 [00:57<00:28, 28.85s/it][AA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[WARNING|utils.py:2443] 2025-10-21 16:37:22,054 >> A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.

100%|██████████| 3/3 [01:54<00:00, 40.43s/it][ABuilding prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Dumping model to file cache /scratch/slurm_tmpdir/job_3589270/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589270/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589270/jieba.cache
Dumping model to file cache /scratch/slurm_tmpdir/job_3589270/jieba.cache
Loading model cost 0.391 seconds.
Prefix dict has been built successfully.
Loading model cost 0.393 seconds.
Prefix dict has been built successfully.
Loading model cost 0.395 seconds.
Prefix dict has been built successfully.
Loading model cost 0.399 seconds.
Prefix dict has been built successfully.
                                                    
                                             [A  4%|▍         | 100/2628 [05:03<1:16:13,  1.81s/it]
100%|██████████| 3/3 [01:54<00:00, 40.43s/it][A
                                             [A[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 112, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 74, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank2]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank2]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank2]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4519, in evaluate
[rank2]:     self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
[rank2]:     return self.call_event("on_evaluate", args, state, control, metrics=metrics)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 556, in call_event
[rank2]:     result = getattr(callback, event)(
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 119, in on_evaluate
[rank2]:     result = super().on_evaluate(args, state, control, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 49, in on_evaluate
[rank2]:     if self.trainer.latest_predictions is not None:
[rank2]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: AttributeError: 'CustomSeq2SeqTrainer' object has no attribute 'latest_predictions'. Did you mean: 'save_predictions'?
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 112, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 74, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank3]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank3]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank3]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4519, in evaluate
[rank3]:     self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
[rank3]:     return self.call_event("on_evaluate", args, state, control, metrics=metrics)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 556, in call_event
[rank3]:     result = getattr(callback, event)(
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 119, in on_evaluate
[rank3]:     result = super().on_evaluate(args, state, control, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 49, in on_evaluate
[rank3]:     if self.trainer.latest_predictions is not None:
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: AttributeError: 'CustomSeq2SeqTrainer' object has no attribute 'latest_predictions'. Did you mean: 'save_predictions'?
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
    run_exp()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 112, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 74, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4519, in evaluate
    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
    return self.call_event("on_evaluate", args, state, control, metrics=metrics)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 119, in on_evaluate
    result = super().on_evaluate(args, state, control, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 49, in on_evaluate
    if self.trainer.latest_predictions is not None:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CustomSeq2SeqTrainer' object has no attribute 'latest_predictions'. Did you mean: 'save_predictions'?
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 112, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 74, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank1]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank1]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank1]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4519, in evaluate
[rank1]:     self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
[rank1]:     return self.call_event("on_evaluate", args, state, control, metrics=metrics)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 556, in call_event
[rank1]:     result = getattr(callback, event)(
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 119, in on_evaluate
[rank1]:     result = super().on_evaluate(args, state, control, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 49, in on_evaluate
[rank1]:     if self.trainer.latest_predictions is not None:
[rank1]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: AttributeError: 'CustomSeq2SeqTrainer' object has no attribute 'latest_predictions'. Did you mean: 'save_predictions'?
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 112, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/tuner.py", line 74, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args,custom_args, callbacks)
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/train/sft/workflow.py", line 138, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 2756, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
[rank0]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 3170, in _evaluate
[rank0]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank0]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer.py", line 4519, in evaluate
[rank0]:     self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
[rank0]:     return self.call_event("on_evaluate", args, state, control, metrics=metrics)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/transformers/trainer_callback.py", line 556, in call_event
[rank0]:     result = getattr(callback, event)(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 119, in on_evaluate
[rank0]:     result = super().on_evaluate(args, state, control, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/eval/callback_adapters.py", line 49, in on_evaluate
[rank0]:     if self.trainer.latest_predictions is not None:
[rank0]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: AttributeError: 'CustomSeq2SeqTrainer' object has no attribute 'latest_predictions'. Did you mean: 'save_predictions'?
W1021 16:38:23.808000 3667300 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3667302 closing signal SIGTERM
W1021 16:38:23.811000 3667300 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3667305 closing signal SIGTERM
E1021 16:38:24.627000 3667300 /hkfs/home/project/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 3667303) of binary: /home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/python3.12
Traceback (most recent call last):
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-21_16:38:23
  host      : hkn0922.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3667304)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-21_16:38:23
  host      : hkn0922.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3667303)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/hk-project-sustainebot/bm3844/miniconda3/envs/roboG_train/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '48199', '/home/hk-project-sustainebot/bm3844/code/LLaMA-Factory-RoboG-v2/src/llamafactory/launcher.py', 'examples/train_full/qwen3vl/qwen3vl_roboG_poc_two_frames.yaml']' returned non-zero exit status 1.
srun: error: hkn0922: task 0: Exited with exit code 1



_target_: models.Qwen3VLModel  # or OpenAIModel, custom model, etc.
name: Qwen3VL-4B
model_name_or_path: Qwen/Qwen3-VL-4B-Instruct  # Or HuggingFace model ID


use_vllm: true  # IMPORTANT: tells run_eval.py to use vLLM engine


image_max_pixels: 65536 #262144
video_max_pixels: 16384


# Model paths (for vLLM inference)
adapter_name_or_path: null  # Optional: path to LoRA adapter
template: qwen3_vl_nothink  # Chat template (default, llama3, qwen2_5_vl, etc.)

# vLLM engine configuration
cutoff_len: 2048
max_new_tokens: 16384  # V-STaR needs a lot of tokens, can be set to 1024 for some other benchmarks
pipeline_parallel_size: 1  # Number of pipeline stages (<= num GPUs)
gpu_memory_utilization: 0.6  # Percent of reserved GPU memory, not set to 0.9 since some benchmarks also have to load models for scoring
vllm_config: {}  # Additional vLLM engine args (dict)

# Generation parameters
temperature: 0.95  # 0.0 for greedy decoding
top_p: 0.7
top_k: 50
repetition_penalty: 1.0
seed: null  # Set for reproducibility

# Multimodal settings (for vision/video/audio models)
video_fps: 2.0
video_maxlen: 128

# Processing settings
skip_special_tokens: true

# Note: batch_size is set in main config.yaml, not here

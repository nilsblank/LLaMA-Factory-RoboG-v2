#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3_vl.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections.abc import Callable
from dataclasses import dataclass
from typing import Any, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

from ... import initialization as init
from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func
from ...masking_utils import create_causal_mask
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutputWithPast, BaseModelOutputWithPooling, ModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, torch_compilable_check
from ...utils.generic import check_model_inputs, is_flash_attention_requested, maybe_autocast
from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig


# =============================================================================
# TEXT-CONDITIONED Q-FORMER IMPLEMENTATION
# =============================================================================

@dataclass
class QFormerOutput(ModelOutput):
    """
    Output type for Q-Former.
    
    Args:
        query_output: The output embeddings from query tokens [B, num_queries, hidden_size]
        text_output: The output embeddings from text tokens (if provided) [B, text_len, hidden_size]
        pooled_output: Pooled representation (first query token) [B, hidden_size]
        hidden_states: All hidden states if requested
        attentions: All attention weights if requested
        cross_attentions: Cross-attention weights if requested
    """
    query_output: torch.FloatTensor = None
    text_output: torch.FloatTensor | None = None
    pooled_output: torch.FloatTensor | None = None
    hidden_states: tuple[torch.FloatTensor] | None = None
    attentions: tuple[torch.FloatTensor] | None = None
    cross_attentions: tuple[torch.FloatTensor] | None = None


class QFormerCrossAttention(nn.Module):
    """
    Cross-attention module for Q-Former.
    Queries attend to visual encoder hidden states.
    """
    def __init__(
        self,
        hidden_size: int,
        encoder_hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.0,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scaling = self.head_dim ** -0.5
        
        # Query projection from Q-Former hidden states
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        # Key/Value projections from encoder (vision) hidden states
        self.k_proj = nn.Linear(encoder_hidden_size, hidden_size)
        self.v_proj = nn.Linear(encoder_hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        """
        Args:
            hidden_states: [B, query_len, hidden_size] - queries from Q-Former
            encoder_hidden_states: [B, encoder_len, encoder_hidden_size] - visual features
            encoder_attention_mask: [B, encoder_len] - mask for visual features
        """
        batch_size, query_len, _ = hidden_states.shape
        encoder_len = encoder_hidden_states.shape[1]
        
        # Project queries, keys, values
        queries = self.q_proj(hidden_states)
        keys = self.k_proj(encoder_hidden_states)
        values = self.v_proj(encoder_hidden_states)
        
        # Reshape for multi-head attention
        queries = queries.view(batch_size, query_len, self.num_heads, self.head_dim).transpose(1, 2)
        keys = keys.view(batch_size, encoder_len, self.num_heads, self.head_dim).transpose(1, 2)
        values = values.view(batch_size, encoder_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Compute attention scores
        attn_weights = torch.matmul(queries, keys.transpose(-2, -1)) * self.scaling
        
        # Apply encoder attention mask if provided
        if encoder_attention_mask is not None:
            # encoder_attention_mask: [B, encoder_len] -> [B, 1, 1, encoder_len]
            attn_mask = encoder_attention_mask[:, None, None, :]
            attn_mask = (1.0 - attn_mask.to(attn_weights.dtype)) * torch.finfo(attn_weights.dtype).min
            attn_weights = attn_weights + attn_mask
        
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(queries.dtype)
        attn_weights = self.dropout(attn_weights)
        
        # Compute output
        attn_output = torch.matmul(attn_weights, values)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.hidden_size)
        attn_output = self.out_proj(attn_output)
        
        return attn_output, attn_weights if output_attentions else None


class QFormerSelfAttention(nn.Module):
    """
    Self-attention module for Q-Former.
    Both query tokens and text tokens can attend to each other.
    """
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.0,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scaling = self.head_dim ** -0.5
        
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        batch_size, seq_len, _ = hidden_states.shape
        
        queries = self.q_proj(hidden_states)
        keys = self.k_proj(hidden_states)
        values = self.v_proj(hidden_states)
        
        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        attn_weights = torch.matmul(queries, keys.transpose(-2, -1)) * self.scaling
        
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(queries.dtype)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, values)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        attn_output = self.out_proj(attn_output)
        
        return attn_output, attn_weights if output_attentions else None


class QFormerMLP(nn.Module):
    """Feed-forward network for Q-Former."""
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int | None = None,
        dropout: float = 0.0,
    ):
        super().__init__()
        intermediate_size = intermediate_size or hidden_size * 4
        self.fc1 = nn.Linear(hidden_size, intermediate_size)
        self.fc2 = nn.Linear(intermediate_size, hidden_size)
        self.act = nn.GELU()
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.fc1(hidden_states)
        hidden_states = self.act(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.fc2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states


class QFormerLayer(nn.Module):
    """
    Single Q-Former layer with:
    - Self-attention (queries + text attend to each other)
    - Cross-attention (only queries attend to visual features) - optional based on layer index
    - Separate FFNs for query and text tokens
    """
    def __init__(
        self,
        hidden_size: int,
        encoder_hidden_size: int,
        num_heads: int = 8,
        intermediate_size: int | None = None,
        dropout: float = 0.0,
        has_cross_attention: bool = True,
    ):
        super().__init__()
        self.has_cross_attention = has_cross_attention
        
        # Self-attention
        self.self_attn = QFormerSelfAttention(hidden_size, num_heads, dropout)
        self.self_attn_norm = nn.LayerNorm(hidden_size)
        
        # Cross-attention (only on certain layers)
        if has_cross_attention:
            self.cross_attn = QFormerCrossAttention(hidden_size, encoder_hidden_size, num_heads, dropout)
            self.cross_attn_norm = nn.LayerNorm(hidden_size)
        
        # Separate FFNs for query tokens and text tokens
        self.ffn_query = QFormerMLP(hidden_size, intermediate_size, dropout)
        self.ffn_query_norm = nn.LayerNorm(hidden_size)
        
        self.ffn_text = QFormerMLP(hidden_size, intermediate_size, dropout)
        self.ffn_text_norm = nn.LayerNorm(hidden_size)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor | None = None,
        attention_mask: torch.Tensor | None = None,
        encoder_attention_mask: torch.Tensor | None = None,
        query_length: int = 0,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, ...]:
        """
        Args:
            hidden_states: [B, query_len + text_len, hidden_size]
            encoder_hidden_states: [B, visual_len, encoder_hidden_size]
            attention_mask: Self-attention mask
            encoder_attention_mask: Cross-attention mask for visual features
            query_length: Number of query tokens (to separate from text)
        """
        all_attentions = ()
        
        # Self-attention
        residual = hidden_states
        hidden_states = self.self_attn_norm(hidden_states)
        attn_output, self_attn_weights = self.self_attn(
            hidden_states, attention_mask, output_attentions
        )
        hidden_states = residual + attn_output
        
        if output_attentions:
            all_attentions = all_attentions + (self_attn_weights,)
        
        # Cross-attention (only for query tokens)
        cross_attn_weights = None
        if self.has_cross_attention and encoder_hidden_states is not None and query_length > 0:
            # Extract query portion
            query_states = hidden_states[:, :query_length, :]
            
            residual = query_states
            query_states = self.cross_attn_norm(query_states)
            cross_output, cross_attn_weights = self.cross_attn(
                query_states, encoder_hidden_states, encoder_attention_mask, output_attentions
            )
            query_states = residual + cross_output
            
            # Put back
            hidden_states = torch.cat([query_states, hidden_states[:, query_length:, :]], dim=1)
            
            if output_attentions:
                all_attentions = all_attentions + (cross_attn_weights,)
        
        # FFN - separate paths for query and text
        if query_length > 0 and hidden_states.shape[1] > query_length:
            # Process query tokens
            query_states = hidden_states[:, :query_length, :]
            residual = query_states
            query_states = self.ffn_query_norm(query_states)
            query_states = residual + self.ffn_query(query_states)
            
            # Process text tokens
            text_states = hidden_states[:, query_length:, :]
            residual = text_states
            text_states = self.ffn_text_norm(text_states)
            text_states = residual + self.ffn_text(text_states)
            
            hidden_states = torch.cat([query_states, text_states], dim=1)
        else:
            # Only query tokens (or only text)
            residual = hidden_states
            hidden_states = self.ffn_query_norm(hidden_states)
            hidden_states = residual + self.ffn_query(hidden_states)
        
        return (hidden_states,) + all_attentions


class TextConditionedQFormer(nn.Module):
    """
    Text-Conditioned Q-Former using BERT-style architecture.
    
    This is a clean implementation that:
    1. Uses learnable query tokens that cross-attend to visual features
    2. Optionally accepts text input that guides query extraction via self-attention
    3. Has cross-attention layers inserted at regular intervals (cross_attention_freq)
    
    Why BERT was originally used:
    - Pre-trained language understanding for text conditioning
    - Bidirectional attention allows queries and text to mutually inform each other
    - The architecture naturally supports the query + text concatenation pattern
    """
    def __init__(
        self,
        hidden_size: int = 768,
        encoder_hidden_size: int = 1024,
        num_queries: int = 32,
        num_layers: int = 12,
        num_heads: int = 12,
        intermediate_size: int | None = None,
        dropout: float = 0.0,
        cross_attention_freq: int = 2,
        max_position_embeddings: int = 512,
        vocab_size: int = 30522,  # BERT vocab size
        pad_token_id: int = 0,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_queries = num_queries
        self.cross_attention_freq = cross_attention_freq
        
        # Learnable query tokens
        self.query_tokens = nn.Parameter(
            torch.zeros(1, num_queries, hidden_size)
        )
        nn.init.normal_(self.query_tokens, std=0.02)
        
        # Text embeddings (word + position)
        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)
        self.embed_norm = nn.LayerNorm(hidden_size)
        self.embed_dropout = nn.Dropout(dropout)
        
        # Transformer layers
        self.layers = nn.ModuleList([
            QFormerLayer(
                hidden_size=hidden_size,
                encoder_hidden_size=encoder_hidden_size,
                num_heads=num_heads,
                intermediate_size=intermediate_size,
                dropout=dropout,
                # Cross-attention every cross_attention_freq layers
                has_cross_attention=(i % cross_attention_freq == 0),
            )
            for i in range(num_layers)
        ])
        
        self.final_norm = nn.LayerNorm(hidden_size)
        
        # Output projection to match LLM hidden size if needed
        self.output_proj = None
        
    def set_output_projection(self, output_dim: int):
        """Set projection to match target LLM dimension."""
        if output_dim != self.hidden_size:
            self.output_proj = nn.Linear(self.hidden_size, output_dim)
    
    def get_extended_attention_mask(
        self,
        attention_mask: torch.Tensor,
        query_length: int,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Create attention mask that allows:
        - Queries to attend to everything (queries + text)
        - Text to attend to everything (queries + text)
        """
        batch_size, seq_length = attention_mask.shape
        
        # Create full attention mask [B, 1, seq_len, seq_len]
        extended_mask = attention_mask[:, None, None, :]
        extended_mask = extended_mask.expand(batch_size, 1, seq_length, seq_length)
        
        # Convert to additive mask
        extended_mask = (1.0 - extended_mask.to(dtype)) * torch.finfo(dtype).min
        
        return extended_mask
        
    def forward(
        self,
        encoder_hidden_states: torch.Tensor,
        input_ids: torch.LongTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        encoder_attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
        output_hidden_states: bool = False,
    ) -> QFormerOutput:
        """
        Args:
            encoder_hidden_states: Visual features [B, visual_len, encoder_hidden_size]
            input_ids: Optional text token IDs [B, text_len]
            attention_mask: Mask for text tokens [B, text_len]
            encoder_attention_mask: Mask for visual features [B, visual_len]
            
        Returns:
            QFormerOutput with query embeddings and optional text embeddings
        """
        batch_size = encoder_hidden_states.shape[0]
        device = encoder_hidden_states.device
        dtype = encoder_hidden_states.dtype
        
        # Expand query tokens for batch
        query_embeds = self.query_tokens.expand(batch_size, -1, -1).to(dtype)
        query_length = self.num_queries
        
        # Process text input if provided
        if input_ids is not None:
            text_length = input_ids.shape[1]
            position_ids = torch.arange(text_length, device=device).unsqueeze(0)
            
            text_embeds = self.word_embeddings(input_ids)
            text_embeds = text_embeds + self.position_embeddings(position_ids)
            text_embeds = self.embed_norm(text_embeds)
            text_embeds = self.embed_dropout(text_embeds)
            
            # Concatenate [queries, text]
            hidden_states = torch.cat([query_embeds, text_embeds], dim=1)
            
            # Create attention mask
            if attention_mask is None:
                attention_mask = torch.ones(batch_size, text_length, device=device)
            # Prepend ones for query tokens
            query_mask = torch.ones(batch_size, query_length, device=device)
            full_attention_mask = torch.cat([query_mask, attention_mask], dim=1)
        else:
            hidden_states = query_embeds
            text_length = 0
            full_attention_mask = torch.ones(batch_size, query_length, device=device)
        
        # Get extended attention mask
        extended_attention_mask = self.get_extended_attention_mask(
            full_attention_mask, query_length, dtype
        )
        
        # Process through layers
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions else None
        
        for layer in self.layers:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            
            layer_outputs = layer(
                hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                attention_mask=extended_attention_mask,
                encoder_attention_mask=encoder_attention_mask,
                query_length=query_length,
                output_attentions=output_attentions,
            )
            
            hidden_states = layer_outputs[0]
            
            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1] if len(layer_outputs) > 1 else None,)
                if layer.has_cross_attention and len(layer_outputs) > 2:
                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)
        
        hidden_states = self.final_norm(hidden_states)
        
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        
        # Split outputs
        query_output = hidden_states[:, :query_length, :]
        text_output = hidden_states[:, query_length:, :] if text_length > 0 else None
        
        # Apply output projection if set
        if self.output_proj is not None:
            query_output = self.output_proj(query_output)
            if text_output is not None:
                text_output = self.output_proj(text_output)
        
        # Pool (use first query token)
        pooled_output = query_output[:, 0, :]
        
        return QFormerOutput(
            query_output=query_output,
            text_output=text_output,
            pooled_output=pooled_output,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
            cross_attentions=all_cross_attentions,
        )


# =============================================================================
# QWEN3VL-BASED Q-FORMER (SAME EMBEDDING SPACE)
# =============================================================================

class Qwen3VLQFormerCrossAttention(nn.Module):
    """
    Cross-attention module that uses Qwen3VL-style attention with RMSNorm.
    """
    def __init__(
        self,
        hidden_size: int,
        encoder_hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.0,
        rms_norm_eps: float = 1e-6,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scaling = self.head_dim ** -0.5
        
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(encoder_hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(encoder_hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # QK normalization like Qwen3VL
        self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=rms_norm_eps)
        self.k_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=rms_norm_eps)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        batch_size, query_len, _ = hidden_states.shape
        encoder_len = encoder_hidden_states.shape[1]
        
        # Project and reshape
        queries = self.q_proj(hidden_states).view(batch_size, query_len, self.num_heads, self.head_dim)
        keys = self.k_proj(encoder_hidden_states).view(batch_size, encoder_len, self.num_heads, self.head_dim)
        values = self.v_proj(encoder_hidden_states).view(batch_size, encoder_len, self.num_heads, self.head_dim)
        
        # Apply QK normalization
        queries = self.q_norm(queries)
        keys = self.k_norm(keys)
        
        # Transpose for attention
        queries = queries.transpose(1, 2)  # [B, num_heads, query_len, head_dim]
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)
        
        # Compute attention
        attn_weights = torch.matmul(queries, keys.transpose(-2, -1)) * self.scaling
        
        if encoder_attention_mask is not None:
            attn_mask = encoder_attention_mask[:, None, None, :]
            attn_mask = (1.0 - attn_mask.to(attn_weights.dtype)) * torch.finfo(attn_weights.dtype).min
            attn_weights = attn_weights + attn_mask
        
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(queries.dtype)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, values)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        
        return attn_output, attn_weights if output_attentions else None


class Qwen3VLQFormerSelfAttention(nn.Module):
    """
    Self-attention using Qwen3VL-style attention.
    """
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.0,
        rms_norm_eps: float = 1e-6,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scaling = self.head_dim ** -0.5
        
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=rms_norm_eps)
        self.k_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=rms_norm_eps)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        batch_size, seq_len, _ = hidden_states.shape
        
        queries = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        keys = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        values = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        queries = self.q_norm(queries)
        keys = self.k_norm(keys)
        
        queries = queries.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)
        
        attn_weights = torch.matmul(queries, keys.transpose(-2, -1)) * self.scaling
        
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(queries.dtype)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, values)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        
        return attn_output, attn_weights if output_attentions else None


class Qwen3VLQFormerMLP(nn.Module):
    """Qwen3VL-style MLP with SwiGLU activation."""
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int | None = None,
    ):
        super().__init__()
        intermediate_size = intermediate_size or int(hidden_size * 8 / 3)
        # Round to multiple of 256 for efficiency
        intermediate_size = ((intermediate_size + 255) // 256) * 256
        
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = nn.SiLU()
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.down_proj(self.act_fn(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))


class Qwen3VLQFormerLayer(nn.Module):
    """
    Q-Former layer using Qwen3VL-style components.
    """
    def __init__(
        self,
        hidden_size: int,
        encoder_hidden_size: int,
        num_heads: int = 8,
        intermediate_size: int | None = None,
        dropout: float = 0.0,
        rms_norm_eps: float = 1e-6,
        has_cross_attention: bool = True,
    ):
        super().__init__()
        self.has_cross_attention = has_cross_attention
        
        # Self-attention
        self.input_layernorm = Qwen3VLTextRMSNorm(hidden_size, eps=rms_norm_eps)
        self.self_attn = Qwen3VLQFormerSelfAttention(hidden_size, num_heads, dropout, rms_norm_eps)
        
        # Cross-attention
        if has_cross_attention:
            self.cross_attn_norm = Qwen3VLTextRMSNorm(hidden_size, eps=rms_norm_eps)
            self.cross_attn = Qwen3VLQFormerCrossAttention(
                hidden_size, encoder_hidden_size, num_heads, dropout, rms_norm_eps
            )
        
        # FFN for query tokens
        self.post_attention_layernorm_query = Qwen3VLTextRMSNorm(hidden_size, eps=rms_norm_eps)
        self.mlp_query = Qwen3VLQFormerMLP(hidden_size, intermediate_size)
        
        # FFN for text tokens
        self.post_attention_layernorm_text = Qwen3VLTextRMSNorm(hidden_size, eps=rms_norm_eps)
        self.mlp_text = Qwen3VLQFormerMLP(hidden_size, intermediate_size)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor | None = None,
        attention_mask: torch.Tensor | None = None,
        encoder_attention_mask: torch.Tensor | None = None,
        query_length: int = 0,
        output_attentions: bool = False,
    ) -> tuple[torch.Tensor, ...]:
        all_attentions = ()
        
        # Self-attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        attn_output, self_attn_weights = self.self_attn(
            hidden_states, attention_mask, output_attentions
        )
        hidden_states = residual + attn_output
        
        if output_attentions:
            all_attentions = all_attentions + (self_attn_weights,)
        
        # Cross-attention (only for query tokens)
        if self.has_cross_attention and encoder_hidden_states is not None and query_length > 0:
            query_states = hidden_states[:, :query_length, :]
            
            residual = query_states
            query_states = self.cross_attn_norm(query_states)
            cross_output, cross_attn_weights = self.cross_attn(
                query_states, encoder_hidden_states, encoder_attention_mask, output_attentions
            )
            query_states = residual + cross_output
            
            hidden_states = torch.cat([query_states, hidden_states[:, query_length:, :]], dim=1)
            
            if output_attentions:
                all_attentions = all_attentions + (cross_attn_weights,)
        
        # FFN - separate paths
        if query_length > 0 and hidden_states.shape[1] > query_length:
            # Query FFN
            query_states = hidden_states[:, :query_length, :]
            residual = query_states
            query_states = self.post_attention_layernorm_query(query_states)
            query_states = residual + self.mlp_query(query_states)
            
            # Text FFN
            text_states = hidden_states[:, query_length:, :]
            residual = text_states
            text_states = self.post_attention_layernorm_text(text_states)
            text_states = residual + self.mlp_text(text_states)
            
            hidden_states = torch.cat([query_states, text_states], dim=1)
        else:
            residual = hidden_states
            hidden_states = self.post_attention_layernorm_query(hidden_states)
            hidden_states = residual + self.mlp_query(hidden_states)
        
        return (hidden_states,) + all_attentions


class Qwen3VLTextConditionedQFormer(nn.Module):
    """
    Text-Conditioned Q-Former using Qwen3VL architecture as backbone.
    
    Key advantages over BERT-based Q-Former:
    1. Same embedding space as Qwen3VL LLM - no alignment issues
    2. Can directly use Qwen3VL's tokenizer and embeddings
    3. RMSNorm + SwiGLU activation for better training dynamics
    4. QK normalization for stable attention
    
    This allows the Q-Former queries to be in the same semantic space
    as the LLM, potentially leading to better visual-language alignment.
    """
    def __init__(
        self,
        hidden_size: int = 3584,  # Qwen3VL-7B hidden size
        encoder_hidden_size: int = 1280,  # Vision encoder hidden size
        num_queries: int = 32,
        num_layers: int = 6,  # Fewer layers than full LLM
        num_heads: int = 28,  # Match Qwen3VL
        intermediate_size: int | None = None,
        dropout: float = 0.0,
        cross_attention_freq: int = 2,
        rms_norm_eps: float = 1e-6,
        # For text conditioning - can share with main LLM
        vocab_size: int = 151936,  # Qwen3VL vocab size
        max_position_embeddings: int = 512,
        pad_token_id: int = 151643,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_queries = num_queries
        self.cross_attention_freq = cross_attention_freq
        
        # Learnable query tokens
        self.query_tokens = nn.Parameter(
            torch.zeros(1, num_queries, hidden_size)
        )
        nn.init.normal_(self.query_tokens, std=0.02)
        
        # Text embeddings - can be initialized from/shared with Qwen3VL
        self.embed_tokens = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        
        # Transformer layers
        self.layers = nn.ModuleList([
            Qwen3VLQFormerLayer(
                hidden_size=hidden_size,
                encoder_hidden_size=encoder_hidden_size,
                num_heads=num_heads,
                intermediate_size=intermediate_size,
                dropout=dropout,
                rms_norm_eps=rms_norm_eps,
                has_cross_attention=(i % cross_attention_freq == 0),
            )
            for i in range(num_layers)
        ])
        
        self.norm = Qwen3VLTextRMSNorm(hidden_size, eps=rms_norm_eps)
        
        # Output projection if needed
        self.output_proj = None
        
    def set_output_projection(self, output_dim: int):
        """Set projection to match target dimension."""
        if output_dim != self.hidden_size:
            self.output_proj = nn.Linear(self.hidden_size, output_dim, bias=False)
    
    def tie_embeddings(self, embed_tokens: nn.Embedding):
        """Tie embeddings with the main Qwen3VL model."""
        self.embed_tokens = embed_tokens
        
    def get_extended_attention_mask(
        self,
        attention_mask: torch.Tensor,
        query_length: int,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        batch_size, seq_length = attention_mask.shape
        extended_mask = attention_mask[:, None, None, :]
        extended_mask = extended_mask.expand(batch_size, 1, seq_length, seq_length)
        extended_mask = (1.0 - extended_mask.to(dtype)) * torch.finfo(dtype).min
        return extended_mask
        
    def forward(
        self,
        encoder_hidden_states: torch.Tensor,
        input_ids: torch.LongTensor | None = None,
        inputs_embeds: torch.FloatTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        encoder_attention_mask: torch.Tensor | None = None,
        output_attentions: bool = False,
        output_hidden_states: bool = False,
    ) -> QFormerOutput:
        """
        Args:
            encoder_hidden_states: Visual features from vision encoder
            input_ids: Optional text token IDs for conditioning
            inputs_embeds: Optional pre-computed text embeddings (alternative to input_ids)
            attention_mask: Mask for text tokens
            encoder_attention_mask: Mask for visual features
            
        Returns:
            QFormerOutput with query embeddings (aligned with Qwen3VL space)
        """
        batch_size = encoder_hidden_states.shape[0]
        device = encoder_hidden_states.device
        dtype = encoder_hidden_states.dtype
        
        # Expand query tokens
        query_embeds = self.query_tokens.expand(batch_size, -1, -1).to(dtype)
        query_length = self.num_queries
        
        # Process text input
        if input_ids is not None or inputs_embeds is not None:
            if inputs_embeds is None:
                inputs_embeds = self.embed_tokens(input_ids)
            
            text_length = inputs_embeds.shape[1]
            hidden_states = torch.cat([query_embeds, inputs_embeds], dim=1)
            
            if attention_mask is None:
                attention_mask = torch.ones(batch_size, text_length, device=device)
            query_mask = torch.ones(batch_size, query_length, device=device)
            full_attention_mask = torch.cat([query_mask, attention_mask], dim=1)
        else:
            hidden_states = query_embeds
            text_length = 0
            full_attention_mask = torch.ones(batch_size, query_length, device=device)
        
        # Extended attention mask
        extended_attention_mask = self.get_extended_attention_mask(
            full_attention_mask, query_length, dtype
        )
        
        # Process through layers
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions else None
        
        for layer in self.layers:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            
            layer_outputs = layer(
                hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                attention_mask=extended_attention_mask,
                encoder_attention_mask=encoder_attention_mask,
                query_length=query_length,
                output_attentions=output_attentions,
            )
            
            hidden_states = layer_outputs[0]
            
            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1] if len(layer_outputs) > 1 else None,)
                if layer.has_cross_attention and len(layer_outputs) > 2:
                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)
        
        hidden_states = self.norm(hidden_states)
        
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        
        # Split outputs
        query_output = hidden_states[:, :query_length, :]
        text_output = hidden_states[:, query_length:, :] if text_length > 0 else None
        
        # Apply output projection if set
        if self.output_proj is not None:
            query_output = self.output_proj(query_output)
            if text_output is not None:
                text_output = self.output_proj(text_output)
        
        pooled_output = query_output[:, 0, :]
        
        return QFormerOutput(
            query_output=query_output,
            text_output=text_output,
            pooled_output=pooled_output,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
            cross_attentions=all_cross_attentions,
        )



#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®
#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3_vl.py file directly. One of our CI enforces this.
#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
import math
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F


from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import BaseModelOutputWithPast, ModelOutput
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling
from transformers.utils.deprecation import deprecate_kwarg
from transformers.utils.generic import check_model_inputs
from transformers import Qwen3VLConfig, Qwen3VLTextConfig
from transformers.models.qwen3_vl.configuration_qwen3_vl import Qwen3VLVisionConfig
from transformers.modeling_outputs import BaseModelOutputWithPooling
from transformers.models.qwen3_vl.video_processing_qwen3_vl import Qwen3VLVideoProcessor
from transformers.models.qwen2_vl.image_processing_qwen2_vl_fast import Qwen2VLImageProcessorFast
from transformers.utils.generic import maybe_autocast

from llamafactory.custom_models.temporal_resamplers.hybrid_slot_query.slot_constrast_pred import TransformerEncoder

@dataclass
@auto_docstring
class BaseModelOutputWithDeepstackFeatures(BaseModelOutputWithPooling):
    r"""
    deepstack_features (`List[torch.FloatTensor]`, *optional*):
        List of hidden-states (feature maps) from deepstack layers.
    """

    deepstack_features: list[torch.FloatTensor] | None = None


class RandomInit(nn.Module):
    """Sampled random initialization for all slots."""

    def __init__(self, n_slots: int, dim: int, initial_std: Optional[float] = None):
        #TODO - add INIT Option for mean per slot and one std
        super().__init__()
        self.n_slots = n_slots
        self.dim = dim
        self.mean = nn.Parameter(torch.zeros(1, 1, dim))
        if initial_std is None:
            self.initial_std = dim**-0.5
        else:
            self.initial_std = initial_std
        self.log_std = nn.Parameter(torch.log(torch.ones(1, 1, dim) * self.initial_std))

    @torch.no_grad()
    def reset_parameters(self):
        # Only run when not meta
        if self.mean.is_meta or self.log_std.is_meta:
            return
        self.mean.zero_()
        self.log_std.fill_(math.log(self.initial_std))


    def forward(self, batch_size: int, device: torch.device, dtype: torch.dtype):
        noise = torch.randn(batch_size, self.n_slots, self.dim, device=device, dtype=dtype)
        mean = self.mean.to(device=device, dtype=dtype)
        std = self.log_std.exp().to(device=device, dtype=dtype)
        return mean + noise * std
    



class FixedLearnedInit(nn.Module):
    """Learned initialization with a fixed number of slots."""

    def __init__(
        self, n_slots: int, dim: int, initial_std: Optional[float] = None, frozen: bool = False
    ):
        super().__init__()
        self.n_slots = n_slots
        self.dim = dim
        if initial_std is None:
            initial_std = dim**-0.5
        self.slots = nn.Parameter(torch.randn(1, n_slots, dim) * initial_std)
        if frozen:
            self.slots.requires_grad_(False)

    def reset_parameters(self):
        # Only run when not meta and not frozen
        if self.slots.is_meta or not self.slots.requires_grad:
            return
        nn.init.normal_(self.slots, mean=0.0, std=self.slots.size(-1) ** -0.5)

    def forward(self, batch_size: int, device: torch.device, dtype: torch.dtype):
        return self.slots.to(device=device, dtype=dtype).expand(batch_size, -1, -1)


class MotionAwareSlotAttention(nn.Module):
    """
    Slot attention biased toward moving regions with temporal carry-over.

    Key design:
      - Use more INTERNAL slots than you finally expose.
      - Track by carrying slots across frames (init frame 0 from mu/sigma, others from previous).
      - Return motion_score per slot for downstream role pooling.
    """

    def __init__(
        self,
        input_dim: int,
        slot_dim: int,
        num_slots_internal: int = 8,
        num_iterations: int = 3,
        motion_weight: float = 0.5,
        num_heads_temporal: int = 8,
        eps: float = 1e-8,
        init_type: str = "fixed",  # "random" or "fixed"
        init_frozen: bool = False,  # Only for fixed init
    ):
        super().__init__()
        self.slot_dim = slot_dim
        self.num_slots_internal = num_slots_internal
        self.num_iterations = num_iterations
        self.motion_weight = motion_weight
        self.scale = slot_dim ** -0.5
        self.eps = eps
        self.init_type = init_type

        self.use_motion_encoder = False

        # Slot initialization: random or fixed learned
        #init_type = "fixed"


        if init_type == "random":
            self.slots = RandomInit(num_slots_internal, slot_dim)
            s = 1
        elif init_type == "fixed":
            self.slots = FixedLearnedInit(num_slots_internal, slot_dim, frozen=init_frozen)
        else:
            raise ValueError(f"Unknown init_type: {init_type}. Must be 'random' or 'fixed'.")

        # Input projection & norm
        self.input_proj = nn.Linear(input_dim, slot_dim)
        self.input_norm = nn.LayerNorm(slot_dim)

        # Motion encoding (in input space -> slot space)
        self.motion_proj = nn.Sequential(
            nn.Linear(input_dim, slot_dim),
            nn.ReLU(),
            nn.Linear(slot_dim, slot_dim),
        )

        # Slot attention projections
        self.to_q = nn.Linear(slot_dim, slot_dim, bias=False)
        self.to_k = nn.Linear(slot_dim, slot_dim, bias=False)
        self.to_v = nn.Linear(slot_dim, slot_dim, bias=False)


        self.predictor = TransformerEncoder(
            dim = slot_dim,
            n_blocks=1,
            n_heads=4,)

        #self.predictor = None


        self.slot_norm = nn.LayerNorm(slot_dim)
        self.gru = nn.GRUCell(slot_dim, slot_dim)
        self.mlp = nn.Sequential(
            nn.LayerNorm(slot_dim),
            nn.Linear(slot_dim, slot_dim * 2),
            nn.GELU(),
            nn.Linear(slot_dim * 2, slot_dim),
        )

        # Optional temporal smoothing/aggregation of per-frame slots
        self.temporal_attn = nn.MultiheadAttention(slot_dim, num_heads_temporal, batch_first=True)
        self.temporal_norm = nn.LayerNorm(slot_dim)

        # Pretraining heads (initialized explicitly via initialize_pretrain_modules)
        self.patch_pos_proj: Optional[nn.Linear] = None
        self.patch_decoder: Optional[nn.Sequential] = None

    def initialize_pretrain_modules(
        self,
        patch_pos_dim: int,
        vision_dim: int,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> None:
        """Initialize reconstruction modules once so their params are optimizer-visible."""
        if self.patch_pos_proj is None:
            self.patch_pos_proj = nn.Linear(patch_pos_dim, self.slot_dim)
        if self.patch_decoder is None:
            self.patch_decoder = nn.Sequential(
                nn.Linear(self.slot_dim, vision_dim),
                nn.ReLU(),
                nn.Linear(vision_dim, vision_dim),
                nn.ReLU(),
                nn.Linear(vision_dim, vision_dim),
                nn.ReLU(),
                nn.Linear(vision_dim, vision_dim + 1),
            )

        if device is not None or dtype is not None:
            if self.patch_pos_proj is not None:
                self.patch_pos_proj = self.patch_pos_proj.to(device=device, dtype=dtype)
            if self.patch_decoder is not None:
                self.patch_decoder = self.patch_decoder.to(device=device, dtype=dtype)

    def compute_motion(self, features: torch.Tensor) -> torch.Tensor:
        """Compute motion signal between consecutive frames."""
        # features: (B, T, N, D)
        motion = features[:, 1:] - features[:, :-1]        # (B, T-1, N, D)
        motion = F.pad(motion, (0, 0, 0, 0, 0, 1))         # (B, T, N, D)
        return motion

    def init_slots(self, B: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:
        slots = self.slots(B, device=device, dtype=dtype)
        
        # Safety check for NaN values
        if torch.isnan(slots).any() or torch.isinf(slots).any():
            print(f"Warning: NaN/Inf detected in initialized slots ({self.init_type}), using fallback")
            slots = torch.randn(B, self.num_slots_internal, self.slot_dim, device=device, dtype=dtype) * 0.02
        
        return slots

    def forward(
        self,
        visual_features: torch.Tensor,                 # (B, T, N, input_dim)
        attention_mask: Optional[torch.Tensor] = None  # (B, T, N) True=real, False=pad
    ) -> Dict[str, torch.Tensor]:
        B, T, N, Din = visual_features.shape
        device = visual_features.device
        dtype = visual_features.dtype

        # Project inputs into slot space
        feats = self.input_norm(self.input_proj(visual_features))      # (B, T, N, H)

        if self.use_motion_encoder:
            # Motion bias
            motion_raw = self.compute_motion(visual_features)              # (B, T, N, Din)
            motion_feats = self.motion_proj(motion_raw)                    # (B, T, N, H)
            feats = feats + self.motion_weight * motion_feats

        # Initialize slots only once (t=0), then carry forward
        slots = self.init_slots(B, device=device, dtype=dtype)         # (B, K, H)

        all_slots = []
        all_attn = []

        for t in range(T):
            x = feats[:, t]  # (B, N, H)

            # key padding mask for this frame: True means IGNORE
            key_padding_mask = None
            if attention_mask is not None:
                # attention_mask True=real -> key_padding_mask True=pad
                key_padding_mask = ~attention_mask[:, t]  # (B, N)

            # Skip fully masked frames to avoid NaN
            if key_padding_mask is not None and key_padding_mask.all():
                # All tokens are padding, keep slots unchanged
                all_slots.append(slots)
                all_attn.append(torch.zeros(B, self.num_slots_internal, N, device=device, dtype=dtype))
                continue

            frame_slots = slots
            attn = None

            for _ in range(self.num_iterations):
                s = self.slot_norm(frame_slots)
                q = self.to_q(s)          # (B, K, H)
                k = self.to_k(x)          # (B, N, H)
                v = self.to_v(x)          # (B, N, H)

                logits = torch.einsum("bkh,bnh->bkn", q, k) * self.scale  # (B, K, N)

                if key_padding_mask is not None:
                    # mask padded tokens as -inf before softmax
                    logits = logits.masked_fill(key_padding_mask.unsqueeze(1), float("-inf"))
                    # Replace -inf rows with large negative value to avoid NaN in softmax
                    # This handles per-batch cases where some samples have all padding
                    all_masked = key_padding_mask.all(dim=1)  # (B,)
                    if all_masked.any():
                        # For samples with all padding, use uniform attention
                        logits[all_masked] = 0.0

                # competition over slots (for each token)
                attn = F.softmax(logits, dim=1)  # (B, K, N)

                # normalize per-slot over tokens for weighted mean
                denom = attn.sum(dim=-1, keepdim=True).clamp_min(self.eps)  # (B, K, 1)
                attn_norm = attn / denom

                updates = torch.einsum("bkn,bnh->bkh", attn_norm, v)  # (B, K, H)

                frame_slots = self.gru(
                    updates.reshape(B * self.num_slots_internal, self.slot_dim),
                    frame_slots.reshape(B * self.num_slots_internal, self.slot_dim),
                ).view(B, self.num_slots_internal, self.slot_dim)

                frame_slots = frame_slots + self.mlp(frame_slots)
                #frame_slots = frame_slots
                #frame_slots = self.mlp(frame_slots)
            
            all_slots.append(frame_slots)
            all_attn.append(attn)

            # temporal carry: use current frame slots as next init
            slots = frame_slots

            if self.predictor is not None:
                slots = self.predictor(slots)

        slots_per_frame = torch.stack(all_slots, dim=1)  # (B, T, K, H)
        attn_per_frame = torch.stack(all_attn, dim=1)    # (B, T, K, N)

        # Compute motion score per slot (helps select manipulated objects later)
        # Use motion magnitude in slot space, weighted by slot attention over tokens.
        if self.use_motion_encoder:
            motion_mag = motion_feats.norm(dim=-1)  # (B, T, N)

            if attention_mask is not None:
                motion_mag = motion_mag * attention_mask.float()  # zero out padded

            # attn_per_frame: (B,T,K,N)
            num = (attn_per_frame * motion_mag.unsqueeze(2)).sum(dim=(1, 3))          # (B,K)
            den = attn_per_frame.sum(dim=(1, 3)).clamp_min(self.eps)                  # (B,K)
            motion_score = num / den                                                  # (B,K)

        else:
            motion_score = torch.zeros(B, self.num_slots_internal, device=slots.device)

        # Temporal aggregation: smooth slot tracks then pool
        slots_flat = slots_per_frame.permute(0, 2, 1, 3).reshape(B * self.num_slots_internal, T, self.slot_dim)
        slots_temporal, _ = self.temporal_attn(slots_flat, slots_flat, slots_flat)
        slots_temporal = self.temporal_norm(slots_temporal + slots_flat)

        # Mean pool for now (you can swap to attention pooling later)
        slots_agg = slots_temporal.mean(dim=1).view(B, self.num_slots_internal, self.slot_dim)

        return {
            "slots_internal": slots_agg,            # (B, K, H)
            "slots_internal_per_frame": slots_per_frame,  # (B, T, K, H)
            "attention": attn_per_frame,            # (B, T, K, N)
            "motion_score": motion_score,           # (B, K)
        }

    def compute_batch_losses(
        self,
        visual_features: torch.Tensor,
        attention_mask: torch.Tensor,
        patch_pos_qwen: torch.Tensor,
        supervised_loss: Optional[torch.Tensor] = None,
        patch_recon_weight: float = 1.0,
        patch_recon_mask_ratio: float = 0.0,
        hybrid_output: Optional[Dict[str, torch.Tensor]] = None,
        return_visuals: bool = False,
        video_grid_thw: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.Tensor] = None,
    ) -> Dict[str, Optional[torch.Tensor]]:
        if self.patch_pos_proj is None or self.patch_decoder is None:
            raise RuntimeError(
                "Pretrain modules are not initialized. Call initialize_pretrain_modules(...) before training."
            )

        if hybrid_output is None:
            hybrid_output = self.forward(visual_features, attention_mask=attention_mask)

        if supervised_loss is None:
            supervised_loss = visual_features.new_zeros(())

        slot_states = hybrid_output["slots_internal_per_frame"]  # (B,T,K,H)
        patch_pos = self.patch_pos_proj(patch_pos_qwen)  # (B,T,N,H)

        bsz, t_steps, num_slots, hidden_dim = slot_states.shape
        num_patches = patch_pos.shape[2]
        slots_plus_pos = (
            slot_states.unsqueeze(3).expand(bsz, t_steps, num_slots, num_patches, hidden_dim)
            + patch_pos.unsqueeze(2).expand(bsz, t_steps, num_slots, num_patches, hidden_dim)
        )

        decoded = self.patch_decoder(slots_plus_pos)
        vision_dim = visual_features.shape[-1]
        recons, alpha = decoded.split((vision_dim, 1), dim=-1)

        invalid = (~attention_mask).unsqueeze(2).unsqueeze(-1)
        alpha = alpha.masked_fill(invalid, -1e4)
        masks = torch.softmax(alpha, dim=2)
        pred_patches = (recons * masks).sum(dim=2)

        valid_mask = attention_mask
        if patch_recon_mask_ratio > 0:
            rand_mask = torch.rand_like(valid_mask.float())
            recon_mask = valid_mask & (rand_mask < patch_recon_mask_ratio)
            if recon_mask.sum() == 0:
                recon_mask = valid_mask
        else:
            recon_mask = valid_mask

        pred = F.layer_norm(pred_patches[recon_mask], normalized_shape=(vision_dim,))
        target = F.layer_norm(visual_features[recon_mask], normalized_shape=(vision_dim,))

        recon_loss = F.mse_loss(pred, target)
        total_loss = supervised_loss + patch_recon_weight * recon_loss

        vis_payload = None
        if return_visuals:
            vis_payload = {
                "masks": masks.detach().float().cpu(),
                "video_grid_thw": video_grid_thw.detach().cpu() if video_grid_thw is not None else None,
                "pixel_values_videos": pixel_values_videos.detach().float().cpu() if pixel_values_videos is not None else None,
            }

        return {
            "total_loss": total_loss,
            "recon_loss": recon_loss,
            "vis_payload": vis_payload,
        }
    

class LearnableQuery(nn.Module):
    """A single learnable query token."""
    def __init__(self, hidden_dim: int):
        super().__init__()
        self.query = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)

    def forward(self, batch_size: int, device: torch.device) -> torch.Tensor:
        return self.query.expand(batch_size, -1, -1).to(device)
    

# -----------------------------
# Slot Role Pooling (differentiable compression)
# -----------------------------
class SlotRolePooler(nn.Module):
    """
    Compress many INTERNAL slots -> a small set of role tokens:
      - robot token (1)
      - object tokens (M)
      - background token (optional)

    This avoids argmax and supports multiple manipulated objects.
    """

    def __init__(self, hidden_dim: int, num_object_tokens: int = 2, use_background_token: bool = False):
        super().__init__()
        self.num_object_tokens = num_object_tokens
        self.use_background_token = use_background_token

        # Learned selectors to pick multiple object tracks from internal slots
        self.object_selectors = nn.Parameter(torch.randn(1, num_object_tokens, hidden_dim) * 0.02)

        # Optional background selector
        if use_background_token:
            self.bg_selector = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)
        else:
            self.bg_selector = None

    def _attn_pool(self, selectors: torch.Tensor, slots: torch.Tensor, slot_bias: Optional[torch.Tensor] = None):
        """
        selectors: (B, M, H)
        slots: (B, K, H)
        slot_bias: (B, K) additive bias (e.g. log objectness)
        returns:
          pooled: (B, M, H)
          weights: (B, M, K)
        """
        # dot product
        logits = torch.einsum("bmh,bkh->bmk", selectors, slots) / math.sqrt(slots.size(-1))
        if slot_bias is not None:
            logits = logits + slot_bias.unsqueeze(1)
        weights = torch.softmax(logits, dim=-1)  # over K
        #cast to slot dtype
        weights = weights.to(slots.dtype)
        pooled = torch.einsum("bmk,bkh->bmh", weights, slots)
        return pooled, weights

    def forward(
        self,
        slots: torch.Tensor,               # (B, K, H)
        slot_class_logits: torch.Tensor,   # (B, K, 3) -> [robot, object, background]
        motion_score: torch.Tensor,        # (B, K)
        motion_temp: float = 1.0,
    ) -> Dict[str, torch.Tensor]:
        B, K, H = slots.shape
        probs = slot_class_logits.softmax(dim=-1)
        robot_p = probs[..., 0]  # (B,K)
        obj_p = probs[..., 1]    # (B,K)
        bg_p = probs[..., 2]     # (B,K)

        # Robot token: weighted average by robot probability
        robot_w = robot_p / (robot_p.sum(dim=-1, keepdim=True).clamp_min(1e-8))
        robot_token = torch.einsum("bk,bkh->bh", robot_w, slots).unsqueeze(1)  # (B,1,H)

        # Object tokens: selectors attend to slots with bias toward objectness + motion
        # Use log-bias to encourage selecting object-like moving slots.
        bias = torch.log(obj_p.clamp_min(1e-6)) + (motion_score / motion_temp)
        obj_selectors = self.object_selectors.expand(B, -1, -1)  # (B,M,H)
        object_tokens, object_weights = self._attn_pool(obj_selectors, slots, slot_bias=bias)

        out = {
            "robot_token": robot_token,               # (B,1,H)
            "object_tokens": object_tokens,           # (B,M,H)
            "object_slot_weights": object_weights,    # (B,M,K)
            "robot_slot_weights": robot_w,            # (B,K)
            "obj_prob": obj_p,
            "robot_prob": robot_p,
            "bg_prob": bg_p,
        }

        if self.use_background_token:
            bg_bias = torch.log(bg_p.clamp_min(1e-6))
            bg_selectors = self.bg_selector.expand(B, -1, -1)
            bg_token, bg_weights = self._attn_pool(bg_selectors, slots, slot_bias=bg_bias)
            out["background_token"] = bg_token
            out["background_slot_weights"] = bg_weights

        return out


# -----------------------------
# Stage 2: Relation Query Encoder (queries attend to memory)
# -----------------------------
class RelationQueryLayer(nn.Module):
    """Single layer for relation query processing with optional slot + context coordination."""

    def __init__(self, hidden_dim: int, visual_dim: int, num_heads: int):
        super().__init__()

        # Joint self-attn between queries + context + slots
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.self_norm = nn.LayerNorm(hidden_dim)

        # Cross-attn to memory (visual tokens and/or per-frame slots)
        self.mem_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.mem_norm = nn.LayerNorm(hidden_dim)

        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim),
        )
        self.ffn_norm = nn.LayerNorm(hidden_dim)

        # Project memory tokens to hidden_dim
        self.mem_proj = nn.Linear(visual_dim, hidden_dim) if visual_dim != hidden_dim else nn.Identity()

    def forward(
        self,
        queries: torch.Tensor,               # (B, Q, H)
        slots: torch.Tensor,                 # (B, K, H)
        context: Optional[torch.Tensor],     # (B, C, H)  e.g. robot/object tokens
        memory: torch.Tensor,                # (B, M, D_mem)
        memory_key_padding_mask: Optional[torch.Tensor] = None,  # (B, M) True=pad
    ) -> torch.Tensor:
        # 1) self-attn between queries + context + slots (coordination)
        if context is None:
            combined = torch.cat([queries, slots], dim=1)
            q_len = queries.size(1)
        else:
            combined = torch.cat([queries, context, slots], dim=1)
            q_len = queries.size(1)

        x = self.self_norm(combined)
        self_out, _ = self.self_attn(x, x, x)
        combined = combined + self_out

        # split back
        queries = combined[:, :q_len, :]

        # 2) cross-attn to memory
        mem = self.mem_proj(memory)
        qn = self.mem_norm(queries)
        attn_out, _ = self.mem_attn(qn, mem, mem, key_padding_mask=memory_key_padding_mask)
        queries = queries + attn_out

        # 3) ffn
        queries = queries + self.ffn(self.ffn_norm(queries))
        return queries


class RelationQueryEncoder(nn.Module):
    """
    Encodes relation queries by attending to memory built from:
      - visual tokens (flattened T*N) and/or
      - per-frame slots (flattened T*K)
    """

    def __init__(
        self,
        hidden_dim: int,
        visual_dim: int,
        slot_dim: int,
        num_heads: int = 8,
        num_layers: int = 2,
        temporal_pooling: str = "concat",     # 'concat' or 'mean'
        memory_mode: str = "tokens+slots",    # 'tokens', 'slots', 'tokens+slots'
    ):
        super().__init__()
        assert memory_mode in {"tokens", "slots", "tokens+slots"}
        self.temporal_pooling = temporal_pooling
        self.memory_mode = memory_mode

        self.layers = nn.ModuleList([
            RelationQueryLayer(hidden_dim, visual_dim, num_heads)
            for _ in range(num_layers)
        ])

        # If we include slots as memory too, they are already in hidden_dim.
        # We'll concatenate them as additional "memory tokens" in hidden_dim space,
        # so memory_dim becomes visual_dim for the visual part and hidden_dim for slots part.
        # To keep the layer simple, we'll project everything to visual_dim before mem_proj OR
        # just unify memory to visual_dim using a projector. We'll do unify-to-visual_dim.
        self.slot_mem_proj = nn.Linear(slot_dim, visual_dim) if slot_dim != visual_dim else nn.Identity()
        self.slot_proj = nn.Linear(slot_dim, hidden_dim) if slot_dim != hidden_dim else nn.Identity()

    def build_memory(
        self,
        visual_features: torch.Tensor,                    # (B, T, N, Dv)
        slots_per_frame: Optional[torch.Tensor],          # (B, T, K, H)
        attention_mask: Optional[torch.Tensor] = None,    # (B, T, N) True=real
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        B, T, N, Dv = visual_features.shape

        memories = []
        pad_masks = []

        # --- visual tokens memory ---
        if self.memory_mode in {"tokens", "tokens+slots"}:
            if self.temporal_pooling == "concat":
                vis = visual_features.reshape(B, T * N, Dv)
                memories.append(vis)
                if attention_mask is not None:
                    # key_padding_mask True=pad
                    vis_pad = ~attention_mask.reshape(B, T * N)
                    pad_masks.append(vis_pad)
                else:
                    pad_masks.append(None)
            else:
                # mean over time with masking
                if attention_mask is not None:
                    m = attention_mask.float()  # (B,T,N)
                    vis_sum = (visual_features * m.unsqueeze(-1)).sum(dim=1)  # (B,N,Dv)
                    denom = m.sum(dim=1, keepdim=False).unsqueeze(-1).clamp_min(1.0)  # (B,N,1)
                    vis = vis_sum / denom
                    memories.append(vis)
                    vis_pad = ~attention_mask.any(dim=1)  # (B,N)
                    pad_masks.append(vis_pad)
                else:
                    memories.append(visual_features.mean(dim=1))  # (B,N,Dv)
                    pad_masks.append(None)

        # --- slots memory ---
        if self.memory_mode in {"slots", "tokens+slots"}:
            assert slots_per_frame is not None, "slots_per_frame required for slots memory"
            Bs, Ts, K, H = slots_per_frame.shape
            assert Bs == B and Ts == T
            slot_mem = slots_per_frame.reshape(B, T * K, H)  # (B, T*K, H)
            slot_mem = self.slot_mem_proj(slot_mem)          # (B, T*K, Dv)
            memories.append(slot_mem)
            # slots are never padded
            pad_masks.append(None)

        # concat memories
        memory = torch.cat(memories, dim=1)  # (B, M, Dv)

        # concat pad masks (if any exist)
        if all(m is None for m in pad_masks):
            return memory, None

        masks = []
        for m in pad_masks:
            if m is None:
                # no padding -> all False
                masks.append(torch.zeros(B, memories[masks.__len__()].shape[1], device=memory.device, dtype=torch.bool))
            else:
                masks.append(m.to(device=memory.device))

        memory_key_padding_mask = torch.cat(masks, dim=1)  # (B, M)
        return memory, memory_key_padding_mask

    def forward(
        self,
        queries: torch.Tensor,                  # (B, Q, H)
        visual_features: torch.Tensor,          # (B, T, N, Dv)
        slots: torch.Tensor,                   # (B, K, H) (internal slots for coordination)
        slots_per_frame: Optional[torch.Tensor] = None,  # (B, T, K, H)
        context_tokens: Optional[torch.Tensor] = None,   # (B, C, H) e.g. robot/object tokens
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        memory, mem_kpm = self.build_memory(visual_features, slots_per_frame, attention_mask)


        #project slots
        slots_proj = self.slot_proj(slots)

        for layer in self.layers:
            queries = layer(
                queries=queries,
                slots=slots_proj,
                context=context_tokens,
                memory=memory,
                memory_key_padding_mask=mem_kpm,
            )
        return queries


# -----------------------------
# Full Hybrid Model
# -----------------------------
class HybridSlotQueryModel(nn.Module):
    """
    Revised design:
      - Stage 1: INTERNAL slots track objects/parts across frames (K_internal)
      - Stage 1b: Role pooling compresses -> robot token + M object tokens (small, stable)
      - Stage 2: Typed queries run PER object token (multi-object ready), attend to tokens+slots memory
      - Stage 3: LLM integration (kept frozen)
    """

    def __init__(
        self,
        vision_dim: int = 1280,
        vision_hidden_dim: int = 1024,
        num_slots_internal: int = 8,     # robust decomposition
        num_object_tokens: int = 2,      # how many manipulated objects you want to represent
        num_frames: int = 16,
        slot_dim = 128,
        num_heads: int = 8,
        qwen_model: str = "Qwen/Qwen2-0.5B",
        memory_mode: str = "tokens+slots",   # 'tokens', 'slots', 'tokens+slots'
        slot_init_type: str = "fixed",      # 'random' or 'fixed'
        slot_init_frozen: bool = False,      # Only for fixed init
        use_per_frame_query: bool = False,
    ):
        super().__init__()

        hidden_dim = vision_hidden_dim
        self.hidden_dim = hidden_dim
        self.num_slots_internal = num_slots_internal
        self.num_object_tokens = num_object_tokens
        self.num_frames = num_frames
        self.use_per_frame_query = use_per_frame_query

        self.slot_dim = slot_dim
        # --------------------------
        # Stage 1: Slot discovery (internal slots)
        # --------------------------
        self.slot_discovery = MotionAwareSlotAttention(
            input_dim=vision_dim,
            slot_dim=slot_dim,
            num_slots_internal=num_slots_internal,
            num_iterations=3,
            init_type=slot_init_type,
            init_frozen=slot_init_frozen,
        )

        # Slot classification over INTERNAL slots: robot, object, background
        self.slot_classifier = nn.Sequential(
            nn.Linear(slot_dim, slot_dim // 2),
            nn.ReLU(),
            nn.Linear(slot_dim // 2, 3),
        )

        # Role pooling: internal slots -> robot token + M object tokens
        self.role_pooler = SlotRolePooler(
            hidden_dim=slot_dim,
            num_object_tokens=num_object_tokens,
            use_background_token=False,
        )

        # --------------------------
        # Stage 2: Typed relation queries
        # --------------------------
        self.relation_queries = nn.ModuleDict({
            "start_location": LearnableQuery(hidden_dim),
            "end_location": LearnableQuery(hidden_dim),
            "target_container": LearnableQuery(hidden_dim),
            "robot_gripper": LearnableQuery(hidden_dim),
        })

        self.relation_encoder = RelationQueryEncoder(
            hidden_dim=hidden_dim,
            visual_dim=vision_dim,               # visual_features are still in vision_dim
            slot_dim=slot_dim,
            num_heads=num_heads,
            num_layers=2,
            temporal_pooling="concat",
            memory_mode=memory_mode,
        )

        # Frame index one-hot token projector for per-frame query mode.
        # A projected one-hot frame token is appended to memory and cross-attended by relation queries.
        self.frame_index_proj = nn.Linear(num_frames, vision_dim)

        # --------------------------
        # Stage 3: LLM Integration (frozen)
        # --------------------------
        from transformers import Qwen2ForCausalLM, AutoTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(qwen_model)
        # self.llm = Qwen2ForCausalLM.from_pretrained(qwen_model, torch_dtype=torch.bfloat16)

        # for p in self.llm.parameters():
        #     p.requires_grad = False

        llm_dim = 2048
        self.slot_to_llm = nn.Linear(hidden_dim, llm_dim)
        self.query_to_llm = nn.Linear(hidden_dim, llm_dim)

        # --------------------------
        # Heads (optional)
        # --------------------------
        # Internal slot boxes per frame (if you have supervision/pseudo-labels)
        self.slot_box_head = nn.Sequential(
            nn.Linear(slot_dim, slot_dim),
            nn.ReLU(),
            nn.Linear(slot_dim, 4),
            nn.Sigmoid(),
        )

        # Query boxes (per object token, per query)
        self.query_box_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 4),
            nn.Sigmoid(),
        )

        self.temporal_head = nn.Sequential(
            nn.Linear(slot_dim, slot_dim // 2),
            nn.ReLU(),
            nn.Linear(slot_dim // 2, 2),
            nn.Sigmoid(),
        )

        #self.setup_special_tokens()

    # def setup_special_tokens(self):
    #     special_tokens = ["<obj>", "</obj>", "<loc>", "</loc>", "<box>", "</box>"]
    #     self.tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
    #     self.llm.resize_token_embeddings(len(self.tokenizer))



    def reset_parameters(self):
        # recursively init all submodules
        #self.apply(self._init_weights)
        for m in self.modules():  # includes self; harmless if handled
            if m is self:
                continue
            if hasattr(m, "reset_parameters"):
                m.reset_parameters()
            #else:
            #    s = 1

        
        # init *this one specific parameter* once
        self.slot_discovery.slots.reset_parameters()

    # @staticmethod
    # def _init_weights(module):
    #     if isinstance(module, nn.Linear):
    #         nn.init.xavier_uniform_(module.weight)
    #         if module.bias is not None:
    #             nn.init.zeros_(module.bias)
        
        #check if is meta and train, then reset parameters of meta modules (for all types
 
        #module.reset_parameters()



    def get_n_video_tokens(self):
        # For an LLM interface you likely want: 1 robot + M objects + Q relation tokens (maybe per object)
        # Here we return the minimal count if you choose to pass:
        #   robot token + M object tokens + Q global query tokens (not per object)
        # If you pass per-object query tokens, multiply Q by M.
        return 1 + self.num_object_tokens + len(self.relation_queries)

    def forward(
        self,
        visual_features: torch.Tensor,                 # (B, T, N, vision_dim)
        attention_mask: Optional[torch.Tensor] = None, # (B, T, N) True=real, False=pad
    ) -> Dict[str, torch.Tensor]:
        B, T, N, D = visual_features.shape

        # --------------------------
        # Stage 1: Internal slot tracking
        # --------------------------
        slot_out = self.slot_discovery(visual_features, attention_mask=attention_mask)
        slots_int = slot_out["slots_internal"]                 # (B, K, H)
        slots_int_pf = slot_out["slots_internal_per_frame"]    # (B, T, K, H)
        slot_attn = slot_out["attention"]                      # (B, T, K, N)
        motion_score = slot_out["motion_score"]                # (B, K)

        slot_class_logits = self.slot_classifier(slots_int)    # (B, K, 3)

        # per-frame boxes for internal slots (optional)
        slot_boxes_pf = self.slot_box_head(slots_int_pf)       # (B, T, K, 4)
        slot_boxes_pf = slot_boxes_pf.permute(0, 2, 1, 3)      # (B, K, T, 4)

        temporal_bounds = self.temporal_head(slots_int)        # (B, K, 2)

        # --------------------------
        # Stage 1b: Role pooling (differentiable, multi-object)
        # --------------------------
        role = self.role_pooler(
            slots=slots_int,
            slot_class_logits=slot_class_logits,
            motion_score=motion_score,
        )
        robot_token = role["robot_token"]          # (B,1,H)
        object_tokens = role["object_tokens"]      # (B,M,H)
        object_slot_weights = role["object_slot_weights"]  # (B,M,K)

        # --------------------------
        # Stage 2: Relation queries PER object token
        # --------------------------
        query_names = list(self.relation_queries.keys())
        base_queries = torch.stack(
            [self.relation_queries[name](B, visual_features.device).squeeze(1) for name in query_names],
            dim=1,
        )  # (B, Q, H)

        if self.use_per_frame_query:
            # Run relation queries per frame in a batched way over (B*T):
            # each frame's queries attend to that frame's visual tokens and that frame's slots.
            BT = B * T
            Q = base_queries.shape[1]

            per_frame_queries = base_queries.unsqueeze(1).expand(B, T, Q, self.hidden_dim).reshape(BT, Q, self.hidden_dim)
            per_frame_visual = visual_features.reshape(BT, 1, N, D)
            per_frame_slots = slots_int_pf.reshape(BT, self.num_slots_internal, self.slot_dim)
            per_frame_slots_pf = slots_int_pf.reshape(BT, 1, self.num_slots_internal, self.slot_dim)

            per_frame_attn_mask = None
            if attention_mask is not None:
                per_frame_attn_mask = attention_mask.reshape(BT, 1, N)

            frame_ids = torch.arange(T, device=visual_features.device, dtype=torch.long).unsqueeze(0).expand(B, T)
            frame_ids = frame_ids.clamp(max=self.num_frames - 1).reshape(BT)
            frame_one_hot = F.one_hot(frame_ids, num_classes=self.num_frames).to(dtype=visual_features.dtype)
            frame_token = self.frame_index_proj(frame_one_hot).unsqueeze(1).unsqueeze(1)  # (BT, 1, 1, D)

            # Append frame-index token into memory sequence for cross-attention.
            per_frame_visual = torch.cat([per_frame_visual, frame_token], dim=2)  # (BT, 1, N+1, D)
            if per_frame_attn_mask is not None:
                extra_mask = torch.ones(BT, 1, 1, device=per_frame_attn_mask.device, dtype=per_frame_attn_mask.dtype)
                per_frame_attn_mask = torch.cat([per_frame_attn_mask, extra_mask], dim=2)

            per_frame_query_feats = self.relation_encoder(
                queries=per_frame_queries,
                visual_features=per_frame_visual,
                slots=per_frame_slots,
                slots_per_frame=per_frame_slots_pf,
                context_tokens=None,
                attention_mask=per_frame_attn_mask,
            ).reshape(B, T, Q, self.hidden_dim)  # (B, T, Q, H)

            per_frame_query_boxes = self.query_box_head(per_frame_query_feats)  # (B, T, Q, 4)

            # Keep global dict outputs for compatibility by temporal mean pooling.
            global_queries = per_frame_query_feats.mean(dim=1)  # (B, Q, H)
            global_query_boxes = self.query_box_head(global_queries)  # (B, Q, 4)

            return {
                # Stage 1 outputs (internal)
                "slots_internal": slots_int,                  # (B,K,H)
                "slots_internal_per_frame": slots_int_pf,     # (B,T,K,H)
                "slot_attention": slot_attn,                  # (B,T,K,N)
                "slot_class_logits": slot_class_logits,       # (B,K,3)
                "motion_score": motion_score,                 # (B,K)
                "slot_boxes_per_frame": slot_boxes_pf,        # (B,K,T,4)
                "temporal_bounds": temporal_bounds,           # (B,K,2)

                # Role tokens (compressed, stable interface)
                "robot_token": robot_token,                   # (B,1,H)
                "object_tokens": object_tokens,               # (B,M,H)
                "object_slot_weights": object_slot_weights,   # (B,M,K)

                # Stage 2 outputs (per frame)
                "query_features_per_frame": per_frame_query_feats,  # (B,T,Q,H)
                "query_boxes_per_frame": per_frame_query_boxes,     # (B,T,Q,4)

                # Compatibility global outputs
                "query_features": {name: global_queries[:, i, :] for i, name in enumerate(query_names)},
                "query_boxes": {name: global_query_boxes[:, i, :] for i, name in enumerate(query_names)},
            }

        # For each object token, run the typed queries conditioned on:
        #   context_tokens = [robot_token, object_token]
        # This yields Q relation tokens per object.
        per_object_query_feats = []
        per_object_query_boxes = []

        for m in range(self.num_object_tokens):
            obj_tok = object_tokens[:, m:m+1, :]               # (B,1,H)
            context = torch.cat([robot_token, obj_tok], dim=1) # (B,2,H)

            q_feats = self.relation_encoder(
                queries=base_queries,
                visual_features=visual_features,
                slots=slots_int,                 # internal slots coordinate with queries
                slots_per_frame=slots_int_pf,     # used if memory_mode includes slots
                context_tokens=context,
                attention_mask=attention_mask,
            )  # (B, Q, H)

            q_boxes = self.query_box_head(q_feats)  # (B, Q, 4)

            per_object_query_feats.append(q_feats)
            per_object_query_boxes.append(q_boxes)

        per_object_query_feats = torch.stack(per_object_query_feats, dim=1)  # (B, M, Q, H)
        per_object_query_boxes = torch.stack(per_object_query_boxes, dim=1)  # (B, M, Q, 4)

        # Optional: if you want a single "global" query set, you can also run with context=[robot, mean(objects)]
        global_obj = object_tokens.mean(dim=1, keepdim=True)
        global_context = torch.cat([robot_token, global_obj], dim=1)
        global_queries = self.relation_encoder(
            queries=base_queries,
            visual_features=visual_features,
            slots=slots_int,
            slots_per_frame=slots_int_pf,
            context_tokens=global_context,
            attention_mask=attention_mask,
        )  # (B, Q, H)

        global_query_boxes = self.query_box_head(global_queries)  # (B, Q, 4)

        return {
            # Stage 1 outputs (internal)
            "slots_internal": slots_int,                  # (B,K,H)
            "slots_internal_per_frame": slots_int_pf,     # (B,T,K,H)
            "slot_attention": slot_attn,                  # (B,T,K,N)
            "slot_class_logits": slot_class_logits,       # (B,K,3)
            "motion_score": motion_score,                 # (B,K)
            "slot_boxes_per_frame": slot_boxes_pf,        # (B,K,T,4)
            "temporal_bounds": temporal_bounds,           # (B,K,2)

            # Role tokens (compressed, stable interface)
            "robot_token": robot_token,                   # (B,1,H)
            "object_tokens": object_tokens,               # (B,M,H)
            "object_slot_weights": object_slot_weights,   # (B,M,K)

            # Stage 2 outputs (per object)
            "per_object_query_features": per_object_query_feats,  # (B,M,Q,H)
            "per_object_query_boxes": per_object_query_boxes,     # (B,M,Q,4)

            # Stage 2 outputs (global)
            "query_features": {name: global_queries[:, i, :] for i, name in enumerate(query_names)},  # dict of (B,H)
            "query_boxes": {name: global_query_boxes[:, i, :] for i, name in enumerate(query_names)}, # dict of (B,4)
        }



class QueryResampler(nn.Module):
    """
    Q-Former style resampler with self-attention + periodic cross-attention.
    """
    def __init__(
        self,
        dim: int,
        num_queries: int = 32,
        num_layers: int = 12,
        num_heads: int = 8,
        mlp_ratio: float = 4.0,
        dropout: float = 0.0,
        cross_attention_frequency: int = 2,  # Cross-attn every N layers
    ):
        super().__init__()
        self.num_queries = num_queries
        self.cross_attention_frequency = cross_attention_frequency

        # Learnable queries
        self.queries = nn.Parameter(torch.randn(num_queries, dim) * 0.02)

        # Per-layer components
        self.layers = nn.ModuleList([
            QFormerLayer(
                dim=dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                dropout=dropout,
                has_cross_attention=(i % cross_attention_frequency == 0)
            )
            for i in range(num_layers)
        ])

    def forward(
        self, 
        x: torch.Tensor,  # [B, L, D] visual tokens
        text_embeds: Optional[torch.Tensor] = None,  # [B, T, D] text tokens
        x_key_padding_mask: Optional[torch.Tensor] = None
    ):
        """
        Returns: q [B, Q, D] - compressed query tokens
        """
        B = x.shape[0]
        q = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, Q, D]
        
        # Concatenate queries with text (if provided)
        if text_embeds is not None:
            hidden_states = torch.cat([q, text_embeds], dim=1)  # [B, Q+T, D]
        else:
            hidden_states = q
        
        # Process through layers
        for layer in self.layers:
            hidden_states = layer(
                hidden_states=hidden_states,
                encoder_hidden_states=x,  # Visual tokens for cross-attn
                encoder_attention_mask=x_key_padding_mask,
                query_length=self.num_queries
            )
        
        # Extract only the query outputs
        q_out = hidden_states[:, :self.num_queries, :]
        return q_out


class QFormerLayer(nn.Module):
    """Single Q-Former layer with optional cross-attention."""
    
    def __init__(self, dim, num_heads, mlp_ratio, dropout, has_cross_attention):
        super().__init__()
        self.has_cross_attention = has_cross_attention
        
        # Self-attention (always present)
        self.self_attn = nn.MultiheadAttention(
            embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(dim)
        
        # Cross-attention (conditional)
        if has_cross_attention:
            self.cross_attn = nn.MultiheadAttention(
                embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True
            )
            self.norm_cross = nn.LayerNorm(dim)
        
        # FFN
        self.norm2 = nn.LayerNorm(dim)
        self.ffn = nn.Sequential(
            nn.Linear(dim, int(dim * mlp_ratio)),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(int(dim * mlp_ratio), dim),
            nn.Dropout(dropout),
        )
    
    def forward(
        self, 
        hidden_states,  # [B, Q+T, D]
        encoder_hidden_states,  # [B, L, D] visual tokens
        encoder_attention_mask,
        query_length
    ):
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 1. SELF-ATTENTION (all tokens attend to each other)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        normed = self.norm1(hidden_states)
        attn_out, _ = self.self_attn(normed, normed, normed)
        hidden_states = hidden_states + attn_out
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 2. CROSS-ATTENTION (queries ‚Üí visual, conditional)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if self.has_cross_attention and query_length > 0:
            # Only queries participate in cross-attention
            query_states = hidden_states[:, :query_length, :]
            
            normed_q = self.norm_cross(query_states)
            cross_attn_out, _ = self.cross_attn(
                query=normed_q,
                key=encoder_hidden_states,
                value=encoder_hidden_states,
                key_padding_mask=encoder_attention_mask
            )
            
            # Update only the query portion
            hidden_states[:, :query_length, :] = query_states + cross_attn_out
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêÔøΩÔøΩÔøΩ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 3. FFN
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        hidden_states = hidden_states + self.ffn(self.norm2(hidden_states))
        
        return hidden_states

class Qwen3VLVisionMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.linear_fc1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)
        self.linear_fc2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=True)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_state):
        return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))




class Qwen3VLVisionPatchEmbed(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.patch_size = config.patch_size
        self.temporal_patch_size = config.temporal_patch_size
        self.in_channels = config.in_channels
        self.embed_dim = config.hidden_size

        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]
        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=True)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype
        hidden_states = hidden_states.view(
            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
        )
        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        return hidden_states


class Qwen3VLVisionRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        self.dim = dim
        self.theta = theta
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class Qwen3VLVisionPatchMerger(nn.Module):
    def __init__(self, config: Qwen3VLVisionConfig, use_postshuffle_norm=False) -> None:
        super().__init__()
        self.hidden_size = config.hidden_size * (config.spatial_merge_size**2)
        self.use_postshuffle_norm = use_postshuffle_norm
        self.norm = nn.LayerNorm(self.hidden_size if use_postshuffle_norm else config.hidden_size, eps=1e-6)
        self.linear_fc1 = nn.Linear(self.hidden_size, self.hidden_size)
        self.act_fn = nn.GELU()
        self.linear_fc2 = nn.Linear(self.hidden_size, config.out_hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)
        x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))
        return x


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3VLVisionAttention(nn.Module):
    def __init__(self, config: Qwen3VLVisionConfig) -> None:
        super().__init__()
        self.dim = config.hidden_size
        self.num_heads = config.num_heads
        self.head_dim = self.dim // self.num_heads
        self.num_key_value_groups = 1  # needed for eager attention
        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)
        self.proj = nn.Linear(self.dim, self.dim)
        self.scaling = self.head_dim**-0.5
        self.config = config
        self.attention_dropout = 0.0
        self.is_causal = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,
        **kwargs,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        query_states, key_states, value_states = (
            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        )
        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

        query_states = query_states.transpose(0, 1).unsqueeze(0)
        key_states = key_states.transpose(0, 1).unsqueeze(0)
        value_states = value_states.transpose(0, 1).unsqueeze(0)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if self.config._attn_implementation == "flash_attention_2":
            # Flash Attention: Use cu_seqlens for variable length attention
            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
            attn_output, _ = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask=None,
                scaling=self.scaling,
                dropout=0.0 if not self.training else self.attention_dropout,
                cu_seq_lens_q=cu_seqlens,
                cu_seq_lens_k=cu_seqlens,
                max_length_q=max_seqlen,
                max_length_k=max_seqlen,
                is_causal=False,
                **kwargs,
            )
        else:
            # Other implementations: Process each chunk separately
            lengths = cu_seqlens[1:] - cu_seqlens[:-1]
            splits = [
                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
            ]

            attn_outputs = [
                attention_interface(
                    self,
                    q,
                    k,
                    v,
                    attention_mask=None,
                    scaling=self.scaling,
                    dropout=0.0 if not self.training else self.attention_dropout,
                    is_causal=False,
                    **kwargs,
                )[0]
                for q, k, v in zip(*splits)
            ]
            attn_output = torch.cat(attn_outputs, dim=1)

        attn_output = attn_output.reshape(seq_length, -1).contiguous()
        attn_output = self.proj(attn_output)
        return attn_output


class Qwen3VLVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config, attn_implementation: str = "sdpa") -> None:
        super().__init__()
        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)
        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)
        self.attn = Qwen3VLVisionAttention(config=config)
        self.mlp = Qwen3VLVisionMLP(config=config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,
        **kwargs,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


class Qwen3VLTextRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3VLTextConfig, device=None):
        super().__init__()
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config

        self.rope_type = self.config.rope_parameters["rope_type"]
        rope_init_fn: Callable = self.compute_default_rope_parameters
        if self.rope_type != "default":
            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]
        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)

        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.register_buffer("original_inv_freq", inv_freq.clone(), persistent=False)

        self.mrope_section = config.rope_parameters.get("mrope_section", [24, 20, 20])

    @staticmethod
    def compute_default_rope_parameters(
        config: Qwen3VLTextConfig | None = None,
        device: Optional["torch.device"] = None,
        seq_len: int | None = None,
    ) -> tuple["torch.Tensor", float]:
        """
        Computes the inverse frequencies according to the original RoPE implementation
        Args:
            config ([`~transformers.PreTrainedConfig`]):
                The model configuration.
            device (`torch.device`):
                The device to use for initialization of the inverse frequencies.
            seq_len (`int`, *optional*):
                The current sequence length. Unused for this type of RoPE.
        Returns:
            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the
            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).
        """
        base = config.rope_parameters["rope_theta"]
        dim = getattr(config, "head_dim", None) or config.hidden_size // config.num_attention_heads

        attention_factor = 1.0  # Unused in this type of RoPE

        # Compute the inverse frequencies
        inv_freq = 1.0 / (
            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)
        )
        return inv_freq, attention_factor

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        # In contrast to other models, Qwen3VL has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        if position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)

    def apply_interleaved_mrope(self, freqs, mrope_section):
        """Apply interleaved MRoPE to 3D rotary embeddings.
        Reorganizes frequency layout from chunked [TTT...HHH...WWW] to
        interleaved [THWTHWTHW...TT], preserving frequency continuity.
        args:
            x: (3, bs, seq_len, head_dim // 2)
            mrope_section: (3,)
        returns:
            x_t: (bs, seq_len, head_dim // 2)
        """
        freqs_t = freqs[0]  # just overwrite the first dimension T
        for dim, offset in enumerate((1, 2), start=1):  # H, W
            length = mrope_section[dim] * 3
            idx = slice(offset, length, 3)
            freqs_t[..., idx] = freqs[dim, ..., idx]
        return freqs_t


@use_kernel_forward_from_hub("RMSNorm")
class Qwen3VLTextRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3VLTextRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class Qwen3VLTextAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):
        super().__init__()
        self.layer_type = config.layer_types[layer_idx] if hasattr(config, "layer_types") else None
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3VLTextRMSNorm(
            self.head_dim, eps=config.rms_norm_eps
        )  # thus post q_norm does not need reshape

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3VLTextMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Qwen3VLTextDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3VLTextAttention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3VLTextMLP(config)
        self.input_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Llava outputs, with hidden states and attentions.
    """
)
class Qwen3VLModelOutputWithPast(ModelOutput):
    r"""
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


@auto_docstring
class Qwen3VLPreTrainedModel(PreTrainedModel):
    config: Qwen3VLConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3VLTextDecoderLayer", "Qwen3VLVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3VLTextDecoderLayer,
        "attentions": Qwen3VLTextAttention,
    }


class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):
    config: Qwen3VLVisionConfig
    _no_split_modules = ["Qwen3VLVisionBlock"]

    def __init__(self, config, *inputs, **kwargs) -> None:
        super().__init__(config, *inputs, **kwargs)
        self.spatial_merge_size = config.spatial_merge_size
        self.patch_size = config.patch_size
        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size

        self.patch_embed = Qwen3VLVisionPatchEmbed(
            config=config,
        )

        self.pos_embed = nn.Embedding(config.num_position_embeddings, config.hidden_size)
        self.num_grid_per_side = int(config.num_position_embeddings**0.5)

        head_dim = config.hidden_size // config.num_heads
        self.rotary_pos_emb = Qwen3VLVisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList([Qwen3VLVisionBlock(config) for _ in range(config.depth)])
        self.merger = Qwen3VLVisionPatchMerger(
            config=config,
            use_postshuffle_norm=False,
        )

        self.deepstack_visual_indexes = config.deepstack_visual_indexes
        self.deepstack_merger_list = nn.ModuleList(
            [
                Qwen3VLVisionPatchMerger(
                    config=config,
                    use_postshuffle_norm=True,
                )
                for _ in range(len(config.deepstack_visual_indexes))
            ]
        )

        self.gradient_checkpointing = False

        self.post_init()

        #self.post_init()

        #init slots 

    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
        merge_size = self.spatial_merge_size

        max_hw = int(grid_thw[:, 1:].max().item())
        freq_table = self.rotary_pos_emb(max_hw)  # (max_hw, dim // 2)
        device = freq_table.device

        total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
        pos_ids = torch.empty((total_tokens, 2), dtype=torch.long, device=device)

        offset = 0
        for num_frames, height, width in grid_thw:
            merged_h, merged_w = height // merge_size, width // merge_size

            block_rows = torch.arange(merged_h, device=device)  # block row indices
            block_cols = torch.arange(merged_w, device=device)  # block col indices
            intra_row = torch.arange(merge_size, device=device)  # intra-block row offsets
            intra_col = torch.arange(merge_size, device=device)  # intra-block col offsets

            # Compute full-resolution positions
            row_idx = block_rows[:, None, None, None] * merge_size + intra_row[None, None, :, None]
            col_idx = block_cols[None, :, None, None] * merge_size + intra_col[None, None, None, :]

            row_idx = row_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)
            col_idx = col_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)

            coords = torch.stack((row_idx, col_idx), dim=-1)

            if num_frames > 1:
                coords = coords.repeat(num_frames, 1)

            num_tokens = coords.shape[0]
            pos_ids[offset : offset + num_tokens] = coords
            offset += num_tokens

        embeddings = freq_table[pos_ids]  # lookup rotary embeddings
        embeddings = embeddings.flatten(1)
        return embeddings

    def fast_pos_embed_interpolate(self, grid_thw):
        grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]
        device = self.pos_embed.weight.device

        idx_list = [[] for _ in range(4)]
        weight_list = [[] for _ in range(4)]

        for t, h, w in zip(grid_ts, grid_hs, grid_ws):
            h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)
            w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)

            h_idxs_floor = h_idxs.int()
            w_idxs_floor = w_idxs.int()
            h_idxs_ceil = (h_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)
            w_idxs_ceil = (w_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)

            dh = h_idxs - h_idxs_floor
            dw = w_idxs - w_idxs_floor

            base_h = h_idxs_floor * self.num_grid_per_side
            base_h_ceil = h_idxs_ceil * self.num_grid_per_side

            indices = [
                (base_h[None].T + w_idxs_floor[None]).flatten(),
                (base_h[None].T + w_idxs_ceil[None]).flatten(),
                (base_h_ceil[None].T + w_idxs_floor[None]).flatten(),
                (base_h_ceil[None].T + w_idxs_ceil[None]).flatten(),
            ]

            weights = [
                ((1 - dh)[None].T * (1 - dw)[None]).flatten(),
                ((1 - dh)[None].T * dw[None]).flatten(),
                (dh[None].T * (1 - dw)[None]).flatten(),
                (dh[None].T * dw[None]).flatten(),
            ]

            for i in range(4):
                idx_list[i].extend(indices[i].tolist())
                weight_list[i].extend(weights[i].tolist())

        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)
        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)
        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]
        patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]

        patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])

        patch_pos_embeds_permute = []
        merge_size = self.config.spatial_merge_size
        for pos_embed, t, h, w in zip(patch_pos_embeds, grid_ts, grid_hs, grid_ws):
            pos_embed = pos_embed.repeat(t, 1)
            pos_embed = (
                pos_embed.view(t, h // merge_size, merge_size, w // merge_size, merge_size, -1)
                .permute(0, 1, 3, 2, 4, 5)
                .flatten(0, 4)
            )
            patch_pos_embeds_permute.append(pos_embed)
        patch_pos_embeds = torch.cat(patch_pos_embeds_permute)
        return patch_pos_embeds

    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Args:
            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
                The final hidden states of the model.
            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
                The temporal, height and width of feature shape of each image in LLM.

        Returns:
            `torch.Tensor`: hidden_states.
        """
        hidden_states = self.patch_embed(hidden_states)

        pos_embeds = self.fast_pos_embed_interpolate(grid_thw)
        hidden_states = hidden_states + pos_embeds

        rotary_pos_emb = self.rot_pos_emb(grid_thw)

        seq_len, _ = hidden_states.size()
        hidden_states = hidden_states.reshape(seq_len, -1)
        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
            dim=0,
            # Select dtype based on the following factors:
            #  - FA2 requires that cu_seqlens_q must have dtype int32
            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
            # See https://github.com/huggingface/transformers/pull/34852 for more information
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

        deepstack_feature_lists = []
        for layer_num, blk in enumerate(self.blocks):
            hidden_states = blk(
                hidden_states,
                cu_seqlens=cu_seqlens,
                position_embeddings=position_embeddings,
                **kwargs,
            )
            if layer_num in self.deepstack_visual_indexes:
                deepstack_feature = self.deepstack_merger_list[self.deepstack_visual_indexes.index(layer_num)](
                    hidden_states
                )
                deepstack_feature_lists.append(deepstack_feature)

        merged_hidden_states = self.merger(hidden_states)

        return BaseModelOutputWithDeepstackFeatures(
            last_hidden_state=hidden_states,
            pooler_output=merged_hidden_states,
            deepstack_features=deepstack_feature_lists,
        )


@auto_docstring(
    custom_intro=(
        "Text part of Qwen3VL, "
        "not a pure text-only model, as DeepStack integrates visual features into the early hidden states."
    )
)
class Qwen3VLTextModel(Qwen3VLPreTrainedModel):
    config: Qwen3VLTextConfig
    _no_split_modules = ["Qwen3VLTextDecoderLayer"]

    def __init__(self, config: Qwen3VLTextConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3VLTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3VLTextRotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()

    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        # args for deepstack
        visual_pos_masks: Optional[torch.Tensor] = None,
        deepstack_visual_embeds: Optional[list[torch.Tensor]] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]:
        r"""
        visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):
            The mask of the visual positions.
        deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):
            The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).
            The feature is extracted from the different visual encoder layers, and fed to the decoder
            hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).
        """
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        # torch.jit.trace() doesn't support cache objects in the output
        if use_cache and past_key_values is None and not torch.jit.is_tracing():
            past_key_values = DynamicCache(config=self.config)

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # the hard coded `3` is for temporal, height and width.
        if position_ids is None:
            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
        elif position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)

        if position_ids.ndim == 3 and position_ids.shape[0] == 4:
            text_position_ids = position_ids[0]
            position_ids = position_ids[1:]
        else:
            text_position_ids = position_ids[0]

        attention_mask = create_causal_mask(
            config=self.config,
            input_embeds=inputs_embeds,
            attention_mask=attention_mask,
            cache_position=cache_position,
            past_key_values=past_key_values,
            position_ids=text_position_ids,
        )

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        for layer_idx, decoder_layer in enumerate(self.layers):
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=text_position_ids,
                past_key_values=past_key_values,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )
            hidden_states = layer_outputs

            # add visual features to the hidden states of first several layers
            if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)):
                hidden_states = self._deepstack_process(
                    hidden_states,
                    visual_pos_masks,
                    deepstack_visual_embeds[layer_idx],
                )

        hidden_states = self.norm(hidden_states)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
        )

    def _deepstack_process(
        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor
    ):
        visual_pos_masks = visual_pos_masks.to(hidden_states.device)
        visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)
        hidden_states = hidden_states.clone()
        local_this = hidden_states[visual_pos_masks, :] + visual_embeds
        hidden_states[visual_pos_masks, :] = local_this
        return hidden_states


@auto_docstring
class Qwen3VLModel(Qwen3VLPreTrainedModel):
    base_model_prefix = "model"
    _checkpoint_conversion_mapping = {}
    # Reference: fix gemma3 grad acc #37208
    accepts_loss_kwargs = False
    config: Qwen3VLConfig
    _no_split_modules = ["Qwen3VLTextDecoderLayer", "Qwen3VLVisionBlock"]

    def __init__(self, config, **kwargs):
        super().__init__(config)
        self.visual = Qwen3VLVisionModel._from_config(config.vision_config)
        self.language_model = Qwen3VLTextModel._from_config(config.text_config)
        self.rope_deltas = None  # cache rope_deltas here

        # ---------------------------------------------------------------------
        # Hybrid Slot Query Model for motion-aware object discovery and tracking
        # Uses slot attention to discover moving objects and semantic queries for relations
        # ---------------------------------------------------------------------
        self.use_slot_query = kwargs.get("use_slot_query", True)
        self.slot_query_max_frames = kwargs.get("slot_query_max_frames", 128)
        self.slot_query_num_heads = kwargs.get("slot_query_num_heads", 8)
        self.slot_query_use_pooled_output = kwargs.get("slot_query_use_pooled_output", False)
        self.slot_dim = kwargs.get("slot_dim", 512)  # dimension of each slot query

        if self.slot_query_use_pooled_output:
            vision_dim = config.vision_config.out_hidden_size
        else:            
            vision_dim = config.vision_config.hidden_size   
        
        text_dim = config.text_config.hidden_size

        self.vision_dim = vision_dim

        # Hybrid Slot Query Model parameters
        self.hybrid_slot_num_slots = kwargs.get("hybrid_slot_num_slots", 2)
        self.hybrid_slot_hidden_dim = kwargs.get("hybrid_slot_hidden_dim", 512)
        self.hybrid_n_query_groups = kwargs.get("hybrid_n_query_groups", 4)


        self.use_per_frame_query = kwargs.get("per_frame_query", False)
        self.append_query_at_end = kwargs.get("append_query", False)
        # Choose HSQ input source from vision encoder:
        # - True: use `vision_output.pooler_output` (merged grid, default)
        # - False: use `vision_output.last_hidden_state` (token-level grid)

        
        self.slot_frame_pos_emb = nn.Embedding(self.slot_query_max_frames, vision_dim)

        # If token-level source is enabled, input dim may differ from HSQ expected dim.
        vision_hidden_size = config.vision_config.hidden_size
        # self.slot_query_input_proj = (
        #     nn.Identity()
        #     if (self.slot_query_use_pooled_output or vision_hidden_size == vision_dim)
        #     else nn.Linear(vision_hidden_size, vision_dim, bias=False)
        # )
        self.slot_query_input_proj = (
           nn.Linear(vision_hidden_size, vision_dim, bias=False)
        )
        
        # Use HybridSlotQueryModel for frame-level processing
        self.hybrid_slot_query_model = HybridSlotQueryModel(
            vision_dim=vision_dim,
            #hidden_dim=self.hybrid_slot_hidden_dim,
            num_slots_internal=self.hybrid_slot_num_slots,
            num_frames=self.slot_query_max_frames,
            slot_dim= self.slot_dim,
            num_heads=self.slot_query_num_heads,
            vision_hidden_dim=vision_dim,
            use_per_frame_query=self.use_per_frame_query,
        )


        self.tokens_per_frame = self.hybrid_slot_num_slots +  self.hybrid_n_query_groups


        self.n_tokens_per_video = self.hybrid_slot_query_model.get_n_video_tokens()
        
        # Project hybrid slot query output to text_dim for LLM
        # Note: hybrid slot model outputs vision_dim, not hybrid_slot_hidden_dim
        self.slot_to_llm = nn.Identity() if vision_dim == text_dim else nn.Linear(vision_dim, text_dim, bias=False)

        # Initialize weights and apply final processing
        self.post_init()


    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def _init_weights(self, module):
        super()._init_weights(module)

        if isinstance(module, HybridSlotQueryModel):
            module.reset_parameters()



    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids."""

        # Since we use timestamps to separate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split
        if video_grid_thw is not None:
            video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)
            video_grid_thw[:, 0] = 1

        spatial_merge_size = self.config.vision_config.spatial_merge_size
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id
        mrope_position_deltas = []
        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            attention_mask = attention_mask.to(total_input_ids.device)
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images > 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos > 0:
                        ed_video = input_tokens.index(video_token_id, st)

                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image < ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image

                    else:
                        try:
                            t, h, w = (
                                video_grid_thw[video_index][0],
                                video_grid_thw[video_index][1],
                                video_grid_thw[video_index][2],
                            )
                            video_index += 1
                            remain_videos -= 1
                        except Exception as e:
                            s = 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                    # t_index is always 0 because llm_grid_t is always 1 (we use timestamps to encode the temporal information for videos)
                    #custom rope index for slot/query. 
                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, self.tokens_per_frame).flatten()
                    #frame level
                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(self.hybrid_slot_num_slots, -1, self.hybrid_n_query_groups).flatten()
                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
                    llm_pos_ids_list.append(torch.stack([t_index, t_index, t_index]) + text_len + st_idx)

                    st = ed + self.tokens_per_frame

                if st < len(input_tokens):
                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas
    

    def _unwrap_q(self,res):
        return res[0] if isinstance(res, (tuple, list)) else res

    def _get_hsq_per_frame_tokens(self, height: int, width: int) -> int:
        if self.slot_query_use_pooled_output:
            return (height * width) // (self.visual.spatial_merge_size**2)
        return height * width
    

    def compress_videos_frame_stage_batched(
    self,
    video_tokens_list: List[torch.Tensor],   # each [T_i*per_frame_i, D]
    video_grid_thw: torch.Tensor,            # [B,3] = (T,H,W)
    timestamps_input_ids: Optional[torch.Tensor] = None,
    timestamps_attention_mask: Optional[torch.Tensor] = None,
    frame_offsets: Optional[torch.Tensor] = None,
):
        """
        Returns:
        q_time_per_video: list of length B, each [1, T_i*Qf, Dq]
        (this corresponds to your q_time before video-level resampler)
        """
        B = len(video_tokens_list)
        assert video_grid_thw.shape[0] == B

        # ---- (1) reshape each video into [T_i, per_frame_i, D] ----
        per_frame_list, T_list = [], []
        tokens_tf_list = []
        D_in = video_tokens_list[0].shape[-1]

        for vid_idx in range(B):
            thw = video_grid_thw[vid_idx]
            T = int(thw[0].item())
            H = int(thw[1].item())
            W = int(thw[2].item())
            per_frame = self._get_hsq_per_frame_tokens(H, W)

            vt = video_tokens_list[vid_idx]
            if vt.shape[-1] != D_in:
                raise ValueError("All videos must have same token dim D to batch like this.")
            if vt.shape[0] != T * per_frame:
                raise ValueError(f"len mismatch vid={vid_idx}: got {vt.shape[0]}, expected {T*per_frame}")

            tokens_tf = vt.view(T, per_frame, D_in)  # [T, per_frame, D]
            tokens_tf_list.append(tokens_tf)
            per_frame_list.append(per_frame)
            T_list.append(T)

        # ---- (2) Batch videos properly: [B, max_T, Lmax, D] ----
        max_T = max(T_list)
        Lmax = max(per_frame_list)

        device = video_tokens_list[0].device
        dtype = video_tokens_list[0].dtype

        x_batched = torch.zeros((B, max_T, Lmax, D_in), device=device, dtype=dtype)
        pad_mask_batched = torch.ones((B, max_T, Lmax), device=device, dtype=torch.bool)  # True=PAD
        frame_idx_batched = torch.zeros((B, max_T), device=device, dtype=torch.long)

        for vid_idx, (tokens_tf, T, L) in enumerate(zip(tokens_tf_list, T_list, per_frame_list)):
            # tokens_tf: [T, L, D]
            x_batched[vid_idx, :T, :L, :] = tokens_tf
            pad_mask_batched[vid_idx, :T, :L] = False  # not pad
            frame_idx_batched[vid_idx, :T] = torch.arange(T, device=device, dtype=torch.long)

        # ---- (3) Language conditioning removed - pure visual slot query ----

        # ---- (4) run Hybrid Slot Query Model ONCE for all videos ----
        # x_batched is already [B, max_T, Lmax, D]
        attn_mask = ~pad_mask_batched  # [B, max_T, Lmax] - True=real, False=pad
        hybrid_output = self.hybrid_slot_query_model(x_batched, attention_mask=attn_mask)  # returns dict
        
        # Extract slot features [B, max_T, num_slots, H]
        slot_features = hybrid_output['slots_internal_per_frame']  # [B, max_T, num_slots, H]
        
        
        # Query features:
        # - per-frame mode: [B, max_T, Q, H]
        # - fallback mode: global dict -> [B, Q, H] then broadcast to all frames
        if self.use_per_frame_query and 'query_features_per_frame' in hybrid_output:
            query_features_concat = hybrid_output['query_features_per_frame']
        else:
            query_features = hybrid_output['query_features']  # dict of key:tensor (B,H)
            query_features_concat = torch.stack(list(query_features.values()), dim=1).unsqueeze(1)  # [B,1,Q,H]
            query_features_concat = query_features_concat.expand(-1, max_T, -1, -1)  # [B,max_T,Q,H]
        # ---- (5) add frame position embedding (batched) ----


        query_features_global = hybrid_output['query_features']  # dict of key:tensor (B,H)
        query_features_global_concat = torch.stack(list(query_features_global.values()), dim=1)  # [B,Q,H]
        query_features_global_projected = self.slot_to_llm(query_features_global_concat)  # [B,Q,text_dim]


        pos = frame_idx_batched.clamp(max=self.slot_query_max_frames - 1)  # [B, max_T]
        pos_emb = self.slot_frame_pos_emb(pos).unsqueeze(2)  # [B, max_T, 1, vision_dim]
        
        # Project slots to text_dim and add position  
        q_all = self.slot_to_llm(slot_features + pos_emb[:, :, :, :slot_features.shape[-1]] if pos_emb.shape[-1] >= slot_features.shape[-1] else slot_features)  # [B, max_T, num_slots, text_dim]

        # Project query features to text_dim
        query_feats_projected = self.slot_to_llm(query_features_concat)  # [B, max_T, Q, text_dim]
        # ---- (6) regroup back per video into q_time: [1, T*Qf, Dq] ----
        q_time_per_video = [None] * B
        for vid_idx, T in enumerate(T_list):
            q_vid = q_all[vid_idx, :T]  # [T, Qf, Dq]
            q_time_per_video[vid_idx] = q_vid.reshape(1, T * q_vid.shape[1], q_vid.shape[2])
            
        
        # Cat query features to slot features (along sequence dimension)
        q_time_per_video_with_query = [None] * B
        for vid_idx, T in enumerate(T_list):
            slot_vid = q_all[vid_idx, :T]                # [T, K, Dq]
            query_vid = query_feats_projected[vid_idx, :T]  # [T, Q, Dq]

            vid_repr = slot_vid
            if self.use_per_frame_query:
                if slot_vid.shape[0] != query_vid.shape[0]:
                    raise ValueError(f"Frame count mismatch for video {vid_idx}: slot_vid has {slot_vid.shape[0]} frames, query_vid has {query_vid.shape[0]} frames.")
                #cat along sequence dimesion (K/Q)
                concat_slot_with_query = torch.cat([slot_vid, query_vid], dim=1)  # [T, K+Q, Dq] 
                vid_repr = concat_slot_with_query



            #flatten along time and sequence dimension
            vid_repr = vid_repr.reshape(T * vid_repr.shape[1], vid_repr.shape[2])  # [T*(K+Q), Dq] or [T*K, Dq] if not per-frame query

            if self.append_query_at_end:
                #Append the global query at the end of the sequence dimension (after flattening)

                vid_repr = torch.cat([vid_repr, query_features_global_projected[vid_idx]], dim=0)  # [T*(K+Q)+Q, Dq] or [T*K+Q, Dq]
            

            q_time_per_video_with_query[vid_idx] = vid_repr
        

        



        return q_time_per_video_with_query
    




    def compress_deepstack_varT_samePerFrame(
        self,
        deepstack_tokens_split: List,              # len S; each entry is list length B of [T_i*per_frame, D]
        video_grid_thw: torch.Tensor,              # [B,3]
        timestamps_input_ids: Optional[torch.Tensor],
        timestamps_attention_mask: Optional[torch.Tensor],
        frame_offsets: Optional[torch.Tensor],
    ):
        S = len(deepstack_tokens_split)
        B = video_grid_thw.shape[0]
        spatial_merge = self.visual.spatial_merge_size

        # --- compute per_frame (assumed constant), and T_i per video ---
        T_list = []
        per_frame_list = []
        for vid_idx in range(B):
            T = int(video_grid_thw[vid_idx, 0].item())
            H = int(video_grid_thw[vid_idx, 1].item())
            W = int(video_grid_thw[vid_idx, 2].item())
            per_frame = (H * W) // (spatial_merge**2)
            if per_frame <= 0:
                raise ValueError(f"Invalid per-frame token count: H={H}, W={W}, merge={spatial_merge}")
            T_list.append(T)
            per_frame_list.append(per_frame)

        if len(set(per_frame_list)) != 1:
            raise ValueError("per_frame varies; use the padded+mask batching path instead.")
        per_frame = per_frame_list[0]

        # --- build per-video timestamp slices once (same logic as your code) ---
        ts_ids_list = [None] * B
        ts_mask_list = [None] * B
        if timestamps_input_ids is not None:
            for vid_idx in range(B):
                T = T_list[vid_idx]
                if timestamps_input_ids.dim() == 3:
                    ts_ids_list[vid_idx] = timestamps_input_ids[vid_idx]
                    if timestamps_attention_mask is not None:
                        ts_mask_list[vid_idx] = timestamps_attention_mask[vid_idx]
                elif timestamps_input_ids.dim() == 2 and frame_offsets is not None:
                    off = int(frame_offsets[vid_idx].item())
                    ts_ids_list[vid_idx] = timestamps_input_ids[off:off+T]
                    if timestamps_attention_mask is not None:
                        ts_mask_list[vid_idx] = timestamps_attention_mask[off:off+T]
                else:
                    raise ValueError("Unsupported timestamp layout.")

        # --- frame-stage batching: build x_all of shape [S*sum(T_i), per_frame, D] ---
        # We pack per-video blocks in order: vid0 (S*T0 frames), vid1 (S*T1 frames), ...
        ds0_vid0 = deepstack_tokens_split[0][0]
        device, dtype = ds0_vid0.device, ds0_vid0.dtype
        D = ds0_vid0.shape[-1]

        x_blocks = []
        pos_blocks = []
        ids_blocks = []
        mask_blocks = []

        # Language conditioning removed - pure visual slot query
        use_ts = False

        for vid_idx in range(B):
            T = T_list[vid_idx]

            # stack deepstack levels for this video: [S, T*per_frame, D] -> [S, T, per_frame, D] -> [S*T, per_frame, D]
            levels = []
            for s in range(S):
                vt = deepstack_tokens_split[s][vid_idx]  # [T*per_frame, D]
                if vt.shape[0] != T * per_frame:
                    raise ValueError(f"deepstack[{s}][{vid_idx}] len mismatch: got {vt.shape[0]}, expected {T*per_frame}")
                levels.append(vt.view(T, per_frame, D))
            x_vid = torch.stack(levels, dim=0).reshape(S * T, per_frame, D)
            x_blocks.append(x_vid)

            # frame position ids: [0..T-1] repeated S times
            pos_vid = torch.arange(T, device=device, dtype=torch.long).repeat(S)
            pos_blocks.append(pos_vid)

            # timestamps: not used in slot query approach
        #xblocks is list length B of [S*T, per_frame, D]
        x_all = torch.cat(x_blocks, dim=0)              # [S*sumT (B*S*T), per_frame, D]
        pos_all = torch.cat(pos_blocks, dim=0)          # [S*sumT]

        # Language conditioning removed - timestamp injection not used

        # run Hybrid Slot Query Model once
        # Reshape to [1, S*sumT, per_frame, D]
        x_for_hybrid = x_all.unsqueeze(0)  # [1, S*sumT, per_frame, D]
        # All tokens are real (no padding in this path since per_frame is constant)
        attn_mask = torch.ones(1, x_all.shape[0], x_all.shape[1], dtype=torch.bool, device=device)
        hybrid_output = self.hybrid_slot_query_model(x_for_hybrid, attention_mask=attn_mask)  # returns dict
        
        # Extract slot features [1, S*sumT, num_slots, H]
        slot_features = hybrid_output['slots_internal_per_frame'].squeeze(0)  # [S*sumT, num_slots, H]
        
        # add frame pos emb
        pos_all = pos_all.clamp(max=self.slot_query_max_frames - 1)
        pos_emb = self.slot_frame_pos_emb(pos_all).unsqueeze(1)  # [S*sumT, 1, vision_dim]
        
        # Project to text_dim
        q_frames = self.slot_to_llm(slot_features + pos_emb[:, :, :slot_features.shape[-1]] if pos_emb.shape[-1] >= slot_features.shape[-1] else slot_features)  # [S*sumT, num_slots, text_dim]

        # --- regroup back: for each ds_level produce torch.cat(per_video, dim=0) like your original ---
        Qf = q_frames.shape[1]
        Dq = q_frames.shape[2]

        self.query_aggr_all_deepstack_layers = False

        if self.query_aggr_all_deepstack_layers:
            # If True, we concatenate all S levels together before video resampling, and run ONE video resampler over the whole thing.

            #get per video features by concatenating all deepstack levels for that video
            per_video_deepstack_query_features = []
            #Aggregate across all deepstack levels
            #split into videos first, then concatenate levels per video
            frame_start_idx = 0
            for vid_index, _ in enumerate(x_blocks):
                n_frames = x_blocks[vid_index].shape[0]
                per_video_deepstack_query_features.append(q_frames[frame_start_idx:frame_start_idx+n_frames].reshape(n_frames*Qf, Dq))
                frame_start_idx += n_frames
            
            # Batch all videos (pad to max length)
            max_len = max(q.shape[0] for q in per_video_deepstack_query_features)
            q_batched = torch.zeros((B, max_len, Dq), device=device, dtype=dtype)
            pad_mask = torch.ones((B, max_len), device=device, dtype=torch.bool)
            
            for i, q in enumerate(per_video_deepstack_query_features):
                l = q.shape[0]
                q_batched[i, :l] = q
                pad_mask[i, :l] = False
            
            # Run video resampler once on all videos
            out_batched = self._call_video_resampler(q_batched, pad_mask)  # [B, Qv, D]
            out_batched = self.timechat_mm_projector(out_batched)  # [B, Qv, Dproj]
            
            # Return as list, one per video
            deepstack_compressed_concat = [out_batched[i] for i in range(B)]
            return deepstack_compressed_concat



        # Batch across both S (deepstack layers) and B (videos): effective batch size = S*B
        # Extract all frame queries for all (s, vid_idx) pairs
        q_all_sb = []
        sb_to_sv = []  # Track (s, vid_idx) for each position
        
        vid_cursor = 0
        for vid_idx in range(B):
            T = T_list[vid_idx]
            block = q_frames[vid_cursor : vid_cursor + S * T]  # [S*T, Qf, Dq]
            vid_cursor += S * T

            for s in range(S):
                q_vid_s = block[s * T : (s + 1) * T]                # [T, Qf, Dq]
                q_time_flat = q_vid_s.reshape(T * Qf, Dq)           # [T*Qf, Dq]
                q_all_sb.append(q_time_flat)
                sb_to_sv.append((s, vid_idx))
        
        # Batch across S*B: pad to max length [S*B, max_len, Dq]
        max_len = max(q.shape[0] for q in q_all_sb)
        q_batched_sb = torch.zeros((S * B, max_len, Dq), device=device, dtype=dtype)
        pad_mask_sb = torch.ones((S * B, max_len), device=device, dtype=torch.bool)
        
        for i, q in enumerate(q_all_sb):
            l = q.shape[0]
            q_batched_sb[i, :l] = q
            pad_mask_sb[i, :l] = False
        
        # Run video resampler once on all S*B items
        #out_batched_sb = self._call_video_resampler(q_batched_sb, pad_mask_sb)  # [S*B, Qv, D]
        #out_batched_sb = self.timechat_mm_projector(out_batched_sb)              # [S*B, Qv, Dproj]
        
        out_batched_sb = q_batched_sb
        
        # Reshape back: organize by deepstack layer, keeping videos separate [B, Qv, Dproj]
        deepstack_compressed_concat = []
        for s in range(S):
            per_video_out = []
            for vid_idx in range(B):
                # Find the position in sb_to_sv for this (s, vid_idx)
                sb_idx = next(i for i, (s_, v_) in enumerate(sb_to_sv) if s_ == s and v_ == vid_idx)
                per_video_out.append(out_batched_sb[sb_idx])
            deepstack_compressed_concat.append(torch.stack(per_video_out, dim=0))  # [B, Qv, Dproj]

        return deepstack_compressed_concat


    @auto_docstring
    def get_video_features(
        self,
        pixel_values_videos: torch.FloatTensor,
        video_grid_thw: Optional[torch.LongTensor] = None,
        timestamps_input_ids: Optional[torch.LongTensor] = None,
        timestamps_attention_mask: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, BaseModelOutputWithDeepstackFeatures]:
        r"""
        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
            The tensors corresponding to the input videos.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        timestamps_input_ids (`torch.LongTensor`, *optional*):
            Optional tokenized timestamps per frame for TimeChat-style language conditioning. Accepted shapes:
              - `(num_videos, T, L)`  (per video, per frame)
              - `(sum(T), L)`        (all frames flattened)
        timestamps_attention_mask (`torch.LongTensor`, *optional*):
            Attention mask for `timestamps_input_ids` (same shape without the vocab dim).
        """
        # If we don't have grid info or slot query is disabled, fall back to the default behavior.
        if video_grid_thw is None or not getattr(self, "use_slot_query", False):
            return self.get_image_features(pixel_values_videos, video_grid_thw, **kwargs)

        #flatten video grid thw to process each frame independently through the vision encoder, then we will regroup them for the hybrid slot query model
        #total_frames = video_grid_thw[:, 0].sum().item()
        video_grid_thw_reshaped = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)
        video_grid_thw_reshaped[:, 0] = 1  # set T
        # Forward all frames through the vision encoder in one pass.
        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)
        vision_output: BaseModelOutputWithDeepstackFeatures = self.visual(
            pixel_values_videos, grid_thw=video_grid_thw, **kwargs
        )

        spatial_merge = self.visual.spatial_merge_size
        if self.slot_query_use_pooled_output:
            split_sizes = (video_grid_thw.prod(-1) // (spatial_merge**2)).tolist()
            token_source = vision_output.pooler_output
        else:
            split_sizes = video_grid_thw.prod(-1).tolist()
            token_source = vision_output.last_hidden_state
            token_source = self.slot_query_input_proj(token_source)

        video_tokens_list = torch.split(token_source, split_sizes)

        deepstack_tokens = vision_output.deepstack_features
        deepstack_tokens_split = None
        if deepstack_tokens is not None:
            deepstack_tokens_split = [torch.split(ds, split_sizes) for ds in deepstack_tokens]

        # If timestamps are provided as a flat (sum_frames, L) tensor, compute per-video offsets.
        frame_offsets = None
        if timestamps_input_ids is not None and timestamps_input_ids.dim() == 2:
            frame_offsets = torch.cat(
                [
                    torch.zeros(1, device=video_grid_thw.device, dtype=video_grid_thw.dtype),
                    torch.cumsum(video_grid_thw[:, 0], dim=0)[:-1],
                ],
                dim=0,
            )

        def _compress_one_video(
            tokens_1d: torch.Tensor,
            thw: torch.Tensor,
            ts_ids: Optional[torch.Tensor],
            ts_mask: Optional[torch.Tensor],
        ) -> torch.Tensor:
            # tokens_1d: [L, D] where L = T * (H*W/merge^2)
            T = int(thw[0].item())
            H = int(thw[1].item())
            W = int(thw[2].item())
            per_frame = (H * W) // (spatial_merge**2)
            if per_frame <= 0:
                raise ValueError(f"Invalid per-frame token count: H={H}, W={W}, merge={spatial_merge}")
            if tokens_1d.shape[0] != T * per_frame:
                raise ValueError(
                    f"Video token length mismatch: got {tokens_1d.shape[0]} but expected {T * per_frame} "
                    f"(T={T}, per_frame={per_frame})."
                )

            D = tokens_1d.shape[-1]
            tokens_tf = tokens_1d.view(T, per_frame, D)  # [T, per_frame, D]

            x = tokens_tf  # [T, per_frame, D] treat T as batch

            # Language conditioning (batched): inject pooled timestamp embedding into visual tokens.
            if self.timechat_qformer_text_input and ts_ids is not None:
                # Normalize ts_ids/ts_mask to shape [T, L] (or broadcast if [L])
                if ts_ids.dim() == 1:
                    ids = ts_ids.unsqueeze(0).expand(T, -1)  # [T, L]
                elif ts_ids.dim() == 2:
                    if ts_ids.shape[0] != T:
                        raise ValueError(f"ts_ids has shape {tuple(ts_ids.shape)} but expected first dim T={T}")
                    ids = ts_ids  # [T, L]
                else:
                    raise ValueError(f"ts_ids must be 1D or 2D, got dim={ts_ids.dim()}")

                mask = None
                if ts_mask is not None:
                    if ts_mask.dim() == 1:
                        mask = ts_mask.unsqueeze(0).expand(T, -1)  # [T, L]
                    elif ts_mask.dim() == 2:
                        if ts_mask.shape[0] != T:
                            raise ValueError(f"ts_mask has shape {tuple(ts_mask.shape)} but expected first dim T={T}")
                        mask = ts_mask
                    else:
                        raise ValueError(f"ts_mask must be 1D or 2D, got dim={ts_mask.dim()}")

                emb = self.language_model.embed_tokens(ids.to(x.device))  # [T, L, text_dim]
                if emb.shape[-1] == D:
                    if mask is None:
                        pooled = emb.mean(dim=1)  # [T, D]
                    else:
                        m = mask.to(emb.device).float()  # [T, L]
                        denom = m.sum(dim=1, keepdim=True).clamp(min=1.0)  # [T, 1]
                        pooled = (emb * m.unsqueeze(-1)).sum(dim=1) / denom  # [T, D]

                    x = x + pooled.unsqueeze(1)  # [T, per_frame, D]

            # Hybrid Slot Query Model (batched)
            # Reshape to [1, T, per_frame, D] for hybrid slot query model
            x_for_hybrid = x.unsqueeze(0)  # [1, T, per_frame, D]
            hybrid_output = self.hybrid_slot_query_model(x_for_hybrid)  # returns dict
            
            # Extract slot features [1, T, num_slots, H]
            slot_features = hybrid_output['slot_per_frame']  # [1, T, num_slots, H]
            slot_features = slot_features.squeeze(0)  # [T, num_slots, H]
            
            # Frame position embedding
            pos_ids = torch.arange(T, device=slot_features.device, dtype=torch.long)
            pos_ids = pos_ids.clamp(max=self.slot_query_max_frames - 1)  # [T]
            pos_emb = self.slot_frame_pos_emb(pos_ids).unsqueeze(1)  # [T, 1, vision_dim]
            
            # Project slots to text_dim and add position embedding
            # First project to vision_dim to match pos_emb, then to text_dim
            slots_vision = torch.nn.functional.linear(
                slot_features, 
                self.slot_to_llm.weight if hasattr(self.slot_to_llm, 'weight') else torch.eye(slot_features.shape[-1], device=slot_features.device)
            )  # [T, num_slots, vision_dim or text_dim]
            
            # If projection exists, add positional embedding before final projection
            if hasattr(self.slot_to_llm, 'weight'):
                # Expand pos_emb to match num_slots
                pos_emb_expanded = pos_emb.expand(-1, slot_features.shape[1], -1)  # [T, num_slots, vision_dim]
                slots_with_pos = slot_features + pos_emb_expanded[:, :, :slot_features.shape[-1]] if pos_emb_expanded.shape[-1] >= slot_features.shape[-1] else slot_features
                out = self.slot_to_llm(slots_with_pos)  # [T, num_slots, text_dim]
            else:
                out = slot_features
            
            # Flatten to [T*num_slots, text_dim]
            return out.reshape(-1, out.shape[-1])


        # Compress pooler_output per video.

        compressed_per_video = self.compress_videos_frame_stage_batched(
            video_tokens_list=video_tokens_list,
            video_grid_thw=video_grid_thw,
            timestamps_input_ids=timestamps_input_ids,
            timestamps_attention_mask=timestamps_attention_mask,
            frame_offsets=frame_offsets,
        )
        # compressed_per_video = []
        # for vid_idx, (vid_tokens, thw) in enumerate(zip(video_tokens_list, video_grid_thw)):
        #     ts_ids = None
        #     ts_mask = None
        #     if timestamps_input_ids is not None:
        #         if timestamps_input_ids.dim() == 3:
        #             ts_ids = timestamps_input_ids[vid_idx]
        #             ts_mask = timestamps_attention_mask[vid_idx] if timestamps_attention_mask is not None else None
        #         elif timestamps_input_ids.dim() == 2 and frame_offsets is not None:
        #             off = int(frame_offsets[vid_idx].item())
        #             T = int(thw[0].item())
        #             ts_ids = timestamps_input_ids[off : off + T]
        #             ts_mask = timestamps_attention_mask[off : off + T] if timestamps_attention_mask is not None else None

        #     compressed_per_video.append(_compress_one_video(vid_tokens, thw, ts_ids, ts_mask))

        # Compress deepstack features (if present) so they stay aligned with the new visual token stream, batched


        process_deepstack = False

        if process_deepstack:
            deepstack_compressed_concat = self.compress_deepstack_varT_samePerFrame(
                deepstack_tokens_split=deepstack_tokens_split,
                video_grid_thw=video_grid_thw,
                timestamps_input_ids=timestamps_input_ids,
                timestamps_attention_mask=timestamps_attention_mask,
                frame_offsets=frame_offsets,
            )
        else:
            deepstack_compressed_concat = None

        # deepstack_compressed_concat = None
        # if deepstack_tokens_split is not None:
        #     deepstack_compressed_concat = []
        #     for ds_split in deepstack_tokens_split:
        #         per_video = []
        #         for vid_idx, (ds_tokens, thw) in enumerate(zip(ds_split, video_grid_thw)):
        #             ts_ids = None
        #             ts_mask = None
        #             if timestamps_input_ids is not None:
        #                 if timestamps_input_ids.dim() == 3:
        #                     ts_ids = timestamps_input_ids[vid_idx]
        #                     ts_mask = timestamps_attention_mask[vid_idx] if timestamps_attention_mask is not None else None
        #                 elif timestamps_input_ids.dim() == 2 and frame_offsets is not None:
        #                     off = int(frame_offsets[vid_idx].item())
        #                     T = int(thw[0].item())
        #                     ts_ids = timestamps_input_ids[off : off + T]
        #                     ts_mask = timestamps_attention_mask[off : off + T] if timestamps_attention_mask is not None else None
        #             per_video.append(_compress_one_video(ds_tokens, thw, ts_ids, ts_mask))
        #         deepstack_compressed_concat.append(torch.cat(per_video, dim=0))

        vision_output.pooler_output = tuple(compressed_per_video)
        vision_output.deepstack_features = deepstack_compressed_concat
        return vision_output

    
    @auto_docstring
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        image_grid_thw: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, BaseModelOutputWithDeepstackFeatures]:
        r"""
        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
            The tensors corresponding to the input images.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        """
        pixel_values = pixel_values.type(self.visual.dtype)
        vision_output: BaseModelOutputWithDeepstackFeatures = self.visual(
            pixel_values, grid_thw=image_grid_thw, **kwargs
        )
        image_embeds = vision_output.pooler_output
        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
        image_embeds = torch.split(image_embeds, split_sizes)
        vision_output.pooler_output = image_embeds

        return vision_output

    def get_placeholder_mask(
        self,
        input_ids: torch.LongTensor,
        inputs_embeds: torch.FloatTensor,
        image_features: Optional[torch.FloatTensor] = None,
        video_features: Optional[torch.FloatTensor] = None,
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
            special_video_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_video_mask = special_video_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id
            special_video_mask = input_ids == self.config.video_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}"
            )

        n_video_tokens = special_video_mask.sum()
        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():
            raise ValueError(
                f"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}"
            )

        return special_image_mask, special_video_mask

    @auto_docstring
    @check_model_inputs
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen3VLModelOutputWithPast]:
        r"""
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        """
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")


        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(input_ids)

        image_mask = None
        video_mask = None




        if pixel_values is not None:
            image_outputs: BaseModelOutputWithDeepstackFeatures = self.get_image_features(
                pixel_values, image_grid_thw, **kwargs
            )
            image_embeds = image_outputs.pooler_output
            deepstack_image_embeds = image_outputs.deepstack_features
            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)

        if pixel_values_videos is not None:
            video_outputs: BaseModelOutputWithDeepstackFeatures = self.get_video_features(
                pixel_values_videos, video_grid_thw, **kwargs
            )
            video_embeds = video_outputs.pooler_output
            deepstack_video_embeds = video_outputs.deepstack_features
            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)

            _, video_mask = self.get_placeholder_mask(
                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds
            )

        visual_pos_masks = None
        deepstack_visual_embeds = None


        video_mask = None
        image_mask = None

        #image_mask = special_image_mask
        #video_mask = special_video_mask_expanded

        if image_mask is not None and video_mask is not None:
            # aggregate visual_pos_masks and deepstack_visual_embeds
            image_mask = image_mask[..., 0]
            video_mask = video_mask[..., 0]
            visual_pos_masks = image_mask | video_mask
            deepstack_visual_embeds = []
            image_mask_joint = image_mask[visual_pos_masks]
            video_mask_joint = video_mask[visual_pos_masks]
            for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):
                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)
                embed_joint[image_mask_joint, :] = img_embed
                embed_joint[video_mask_joint, :] = vid_embed
                deepstack_visual_embeds.append(embed_joint)
        elif image_mask is not None:
            image_mask = image_mask[..., 0]
            visual_pos_masks = image_mask
            deepstack_visual_embeds = deepstack_image_embeds
        elif video_mask is not None:
            video_mask = video_mask[..., 0]
            visual_pos_masks = video_mask
            deepstack_visual_embeds = deepstack_video_embeds


        # position_ids = None
        # if position_ids is None:
        #     past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()
        #     if self.rope_deltas is None or past_key_values_length == 0:
        #         position_ids, rope_deltas = self.get_rope_index(
        #             input_ids,
        #             image_grid_thw,
        #             video_grid_thw_for_rope,
        #             attention_mask=attention_mask,
        #         )
        #         self.rope_deltas = rope_deltas
        #     # then use the prev pre-calculated rope-deltas to get the correct position ids
        #     else:
        #         batch_size, seq_length, _ = inputs_embeds.shape
        #         delta = (past_key_values_length + self.rope_deltas).to(inputs_embeds.device)
        #         position_ids = torch.arange(seq_length, device=inputs_embeds.device)
        #         position_ids = position_ids.view(1, -1).expand(batch_size, -1)
        #         if cache_position is not None:  # otherwise `deltas` is an int `0`
        #             delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
        #         position_ids = position_ids.add(delta)
        #         position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        outputs = self.language_model(
            input_ids=None,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            visual_pos_masks=visual_pos_masks,
            deepstack_visual_embeds=deepstack_visual_embeds,
            **kwargs,
        )

        return Qwen3VLModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values,
            rope_deltas=self.rope_deltas,
        )


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Qwen3VL causal language model (or autoregressive) outputs.
    """
)
class Qwen3VLCausalLMOutputWithPast(ModelOutput):
    r"""
    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
        Language modeling loss (for next-token prediction).
    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


class Qwen3VLForConditionalGenerationSlot(Qwen3VLPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {}
    _tied_weights_keys = {"lm_head.weight": "model.language_model.embed_tokens.weight"}
    # Reference: fix gemma3 grad acc #37208
    accepts_loss_kwargs = False
    config: Qwen3VLConfig

    def __init__(self, config, **kwargs):
        super().__init__(config)
        self.model = Qwen3VLModel(config, **kwargs)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)

        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    @auto_docstring
    def get_video_features(
        self,
        pixel_values_videos: torch.FloatTensor,
        video_grid_thw: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, BaseModelOutputWithDeepstackFeatures]:
        r"""
        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
            The tensors corresponding to the input videos.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        """
        return self.model.get_video_features(
            pixel_values_videos=pixel_values_videos, video_grid_thw=video_grid_thw, **kwargs
        )

    @auto_docstring
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        image_grid_thw: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, BaseModelOutputWithDeepstackFeatures]:
        r"""
        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
            The tensors corresponding to the input images.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        """
        return self.model.get_image_features(pixel_values=pixel_values, image_grid_thw=image_grid_thw, **kwargs)

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen3VLCausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.

        Example:

        ```python
        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration

        >>> model = Qwen3VLForConditionalGeneration.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")
        >>> processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")

        >>> messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg",
                    },
                    {"type": "text", "text": "Describe the image."},
                ],
            }
        ]

        >>> inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )

        >>> # Generate
        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)
        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        >>> print(output_text)
        ```
        """

        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs[0]

        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])



        
        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)

        return Qwen3VLCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=outputs.rope_deltas,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        is_first_iteration=False,
        **kwargs,
    ):
        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model

        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            use_cache=use_cache,
            is_first_iteration=is_first_iteration,
            **kwargs,
        )

        # Qwen3VL position_ids are prepared with rope_deltas
        if position_ids is None:
            # Calculate RoPE index once per generation in the pre-fill stage only.
            # When compiling, we can't check tensor values thus we check only input length
            # It is safe to assume that `length!=1` means we're in pre-fill because compiled
            # models currently cannot do asssisted decoding
            if model_inputs["cache_position"][0] == 0 or self.model.rope_deltas is None:
                vision_positions, rope_deltas = self.model.get_rope_index(
                    model_inputs.get("input_ids", None),
                    image_grid_thw=image_grid_thw,
                    video_grid_thw=video_grid_thw,
                    attention_mask=attention_mask,
                )
                self.model.rope_deltas = rope_deltas
            # then use the prev pre-calculated rope-deltas to get the correct position ids
            elif "position_ids" in model_inputs:
                batch_size, seq_length = model_inputs["position_ids"].shape
                device = model_inputs["position_ids"].device
                position_ids = torch.arange(seq_length, device=device)
                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)
                delta = cache_position[0] + self.model.rope_deltas
                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                vision_positions = position_ids + delta.expand_as(position_ids)

            # Concatenate "text + vision" positions into [4, bs, seq-len]
            text_positions = model_inputs["position_ids"][None, ...]
            model_inputs["position_ids"] = torch.cat([text_positions, vision_positions], dim=0)

        if not is_first_iteration and use_cache:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None

        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.
        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.

        Returns:
            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)
            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)
        """
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id

        if inputs_embeds is not None:
            vision_start_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            image_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
            video_mask = (
                inputs_embeds
                == self.get_input_embeddings()(
                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)
                )
            )[..., 0]
        else:
            vision_start_mask = input_ids == vision_start_token_id
            image_mask = input_ids == image_token_id
            video_mask = input_ids == video_token_id

        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)
        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)
        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)

        return image_nums, video_nums

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> tuple[torch.LongTensor, dict[str, Any]]:
        # Overwritten -- Qwen3VL use timestamps and remove second_per_grid_ts
        # Support for expanding tensors without a batch size dimension
        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw
        # pixel_values.shape[0] is sum(seqlen_images for samples)
        # image_grid_thw.shape[0] is sum(num_images for samples)

        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            image_nums, video_nums = self._get_image_nums_and_video_nums(
                input_ids, inputs_embeds=model_kwargs.get("inputs_embeds", None)
            )

            # video_nums: (batch_size,)
            # since video_nums is the number of videos in the input dependent on the input_ids(vision_start),
            # but qwen3vl append vision_start to each frame of each video, so we need to recover the real video_nums according to video_grid_thw
            if video_grid_thw is not None:
                cumulative_frame_counts = torch.cumsum(video_grid_thw[:, 0], dim=0)
                cumulative_token_video_counts = torch.cumsum(video_nums, dim=0)
                # Find video boundaries in cumulative_frame_counts
                video_boundary_indices = torch.searchsorted(cumulative_frame_counts, cumulative_token_video_counts)
                # example: video_boundary_indices = [3, 5] means video_nums = [4, 2]
                video_nums = torch.diff(torch.cat([-video_boundary_indices.new_ones(1), video_boundary_indices]))

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    # split images into samples
                    samples = torch.split(image_grid_thw, list(image_nums))
                    # compute the sequence length of images for each sample
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    # get the num of images for each sample
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs


__all__ = [
    "Qwen3VLVisionModel",
    "Qwen3VLForConditionalGeneration",
    "Qwen3VLModel",
    "Qwen3VLPreTrainedModel",
    "Qwen3VLTextModel",
]